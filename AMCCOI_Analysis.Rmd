---
title: "AMCCOI"
author: "BYDavis"
date: "2024-04-17"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Markdown file to analyze AMC water eDNA samples, sequenced using the BF2/BR2 COI primer

# Run set-up files
```{r COI_config, echo = FALSE}
# config file with paths and versioning
source("AMCCOI_config.R")
```
```{r COI_reload}
# File with previous outputs to make reloading easier
source("AMCCOI_reload.R")
```

# Set raw file types
```{r raw_isolate, echo = FALSE}

# Specify file name formats
fnsBF <- list.files(path_rawBF)
fastqsBF <- fnsBF[grepl('.gz$', fnsBF)]

fnsBR <- list.files(path_rawBR)
fastqsBR <- fnsBR[grepl('.gz$', fnsBR)]

# The file paths still include the desktop.ini file, so let's specify a file path to lead future code only to the fastq files in the folder

fnsBFisolate <- file.path(path_rawBF, fastqsBF)
fnsBRisolate <- file.path(path_rawBR, fastqsBR)

# Set sample names to a vector
# Remove path and the .gz extension
# Repeat file_path_sans_ext to also remove the .fastq
namesBFfast <- tools::file_path_sans_ext(basename(fastqsBF))
namesBF <- tools::file_path_sans_ext(namesBFfast)

namesBRfast <- tools::file_path_sans_ext(basename(fastqsBR))
# Repeat file_path_sans_ext to also remove the .fastq
namesBR <- tools::file_path_sans_ext(namesBRfast)
```


# Check Sequence Quality

Run 6 randomly selected quality profile plots per primer per read type (forward or reverse). Number will correspond to the file order.

```{r random_selection, echo = FALSE}
# Numbers for BF2 Forward reads:
sample(1:78, 6, replace = FALSE)
  # Results on 3/20/2024: 48 64 20 57 15 11

# Numbers for BF2 Reverse reads:
sample(1:78, 6, replace = FALSE)
  # Results on 3/20/2024: 15, 49, 56, 78, 11, 28
```

Now generate the randomly selected plots
```{r quality_profile, echo = FALSE}
# B Forward:
ggsave("BF2ForwardQuality_RNG48.png", plot = plotQualityProfile(fnsBFisolate[48]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG64.png", plot = plotQualityProfile(fnsBFisolate[64]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG20.png", plot = plotQualityProfile(fnsBFisolate[20]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG57.png", plot = plotQualityProfile(fnsBFisolate[57]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG15.png", plot = plotQualityProfile(fnsBFisolate[15]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG11.png", plot = plotQualityProfile(fnsBFisolate[11]), path = path_outputs, width = 6, height = 4, units = "in")

# Notify when the forward plots are done
beep(sound = "fanfare")

# B Reverse:
ggsave("BR2ReverseQuality_RNG15.png", plot = plotQualityProfile(fnsBRisolate[15]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG49.png", plot = plotQualityProfile(fnsBRisolate[49]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG56.png", plot = plotQualityProfile(fnsBRisolate[56]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG77.png", plot = plotQualityProfile(fnsBRisolate[77]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG11.png", plot = plotQualityProfile(fnsBRisolate[11]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG28.png", plot = plotQualityProfile(fnsBRisolate[28]), path = path_outputs, width = 6, height = 4, units = "in")

# Notify when the reverse plots are done
beep(sound = "fanfare")
```

The indexes and adapters have been removed from the sequence files, but the original primers are still attached. Need to account for their presence in the trim amounts and remove them.

BF2/BR2 COI Region: ~420 bp - this is longer than the cycle length, but should still provide 260bp of overlap

BF2 primer length: 20 -  GCHCCHGAYATRGCHTTYCC 

BR2 primer length: 20 -  TCDGGRTGNCCRAARAAYCA 


Given a desired score of 25+, trim reverse after 250 and forward after 280

# Begin filtering process
```{r filtAndTrim, echo = TRUE}
# Trimming parameters
filtoutB <- filterAndTrim(file.path(path_rawBF, fastqsBF), file.path(path_filtBF, paste0(namesBF, "filt.fastq.gz")), file.path(path_rawBR, fastqsBR), file.path(path_filtBR, paste0(namesBR, "filt.fastq.gz")), trimLeft = c(20,20), trimRight=c(20,50), maxN=0, maxEE=c(2,2), verbose=TRUE) 

# Save output
saveRDS(filtoutB, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/FiltOutput.rds")
```


# Explore filtered output 

```{r assess_filt, echo = FALSE}

# Order by filtered read amount
filtoutB[order(filtoutB[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtoutB)

# Save output
write.csv(filtoutB, file.path(path_outputs, "FiltOutput.csv"))
```


# Look at filtered trends
```{r filter_trends, echo = FALSE}
# Plot read numbers before and after trimming
filttrendB <- ggplot(as.data.frame(filtoutB)) + geom_point(aes(row.names(filtoutB), reads.in), color = "blue") + geom_point(aes(row.names(filtoutB), reads.out), color = "orange") + ggtitle("Filter Trends for AMC BF2/BR2 Reads")

# View plot
filttrendB

# Save plot
ggsave("FilterTrends.png", plot = filttrendB, path = path_outputs, width = 6, height = 4, units = "in")
```


# Error Rates and Dereplication

First we need to set a seed and make sure the paths are all still set.
```{r error_setup, echo = FALSE}
# Set seed
set.seed(0743)

# BForward
# create a list of files in the path
filtnamesBFinter <- list.files(path_filtBF, full.names = TRUE)

# Specify I only want the files with the .gz extension
filtnamesBF <- filtnamesBFinter[grepl('.gz$', filtnamesBFinter)]

# Extract just the file name, not the path, and remove the .gz extension. This leaves on the .fastq extension
fastqfiltBF <- tools::file_path_sans_ext(basename(filtnamesBF))

# Remove the .fastq extension
namesBFfilt <- tools::file_path_sans_ext(basename(fastqfiltBF))

# Repeat for reverse
filtnamesBRinter <- list.files(path_filtBR, full.names = TRUE)
filtnamesBR <- filtnamesBRinter[grepl('.gz$', filtnamesBRinter)]
fastqfiltBR <- tools::file_path_sans_ext(basename(filtnamesBR))
namesBRfilt <- tools::file_path_sans_ext(basename(fastqfiltBR))
```


## Dereplication

This step comes from the dada2 workflow (https://bioconductor.org/packages/devel/bioc/vignettes/dada2/inst/doc/dada2-intro.html#learn-the-error-rates)

```{r derep, echo = FALSE}
derepBF <- derepFastq(filtnamesBF, verbose = TRUE)
derepBR <- derepFastq(filtnamesBR, verbose = TRUE)
```

Now that the names are set up, we can run the simulations to learn and estimate the number of errors in each filtered folder. This step is in Sue's code (and is the source of the saveRDS and plotErrors lines), but using the default parameters from the dada2 workflow


## Error Rates

```{r error_rates, echo = FALSE}
# Forward error rates
errBF <- learnErrors(derepBF, multithread = FALSE, randomize = TRUE)

# Save error rates
saveRDS(errBF, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/ForwardError.rds")

# Generate and save forward error plot
ggsave("Forward_ErrorPlot.png", path = path_outputs, plotErrors(errBF, nominalQ = TRUE), width = 6, height = 4, units = "in") 

# Reverse error rates
errBR <- learnErrors(derepBR, multithread = FALSE, randomize = TRUE)

# Save error rates
saveRDS(errBR, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/ReverseError.rds")

# Generate and save reverse error plot
ggsave("Reverse_ErrorPlot.png", path = path_outputs, plotErrors(errBR, nominalQ = TRUE), width = 6, height = 4, units = "in") 

# This takes a long time. If you have to do it again, include this:
beep(sound = "fanfare")
```


# Infer sample composition
```{r sample_composition, echo = FALSE}
dadaBF <- dada(derepBF, err = errBF, multithread = FALSE)
print("dada BF finished")
beep(sound = "coin")

dadaBR <- dada(derepBR, err = errBR, multithread = FALSE)
print("dada BR finished")
beep(sound = "fanfare")

# Save all in case I need to reload the dada objects
saveRDS(dadaBF, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/BForward_SampleComp.rds")
saveRDS(dadaBR, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/BReverse_SampleComp.rds")
```


## Merge paired reads

```{r merge_reads, echo = FALSE}
mergersB <- mergePairs(dadaBF, derepBF, dadaBR, derepBR, verbose = TRUE)

# Save output
saveRDS(mergersB, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/BMergedSampleComp.rds")
```

```{r seqtab, echo = FALSE}
# Turn into sequence table
seqtabB <- makeSequenceTable(mergersB)

# Save output
saveRDS(seqtabB, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/BMerged Sequence Table.rds")
```

# Remove Chimeras 
```{r no_chim, echo = FALSE}
# Remove chimeras
seqtabB.nochim <- removeBimeraDenovo(seqtabB, method = "consensus", multithread = FALSE, verbose = TRUE)

# Save
saveRDS(seqtabB.nochim, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/BMerged Sequence No Chimera Table.rds")

# Notification
beep(sound = "coin")
```


# Workflow Verification

```{r workflow_verification, echo = FALSE}
# Function for sum
getN <- function(x) sum(getUniques(x))

# Combine read numbers across workflow stages into a table
trackB <- cbind(filtoutB, sapply(dadaBF, getN), sapply(dadaBR, getN), sapply(mergersB, getN), rowSums(seqtabB.nochim))

# Set column names
colnames(trackB) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")

# Save output
write.csv(trackB, file.path(path_outputs, "WorkflowVerification.csv"))
```


# Assign Taxonomy

```{r assign_taxonomy, echo = FALSE}
# Very slow and intensive
all.taxa <- assignTaxonomy(seqtabB.nochim, 'C:/Users/bydav/Desktop/RefDB_Dev/output/3-May19-2024/COI_REFDB.fasta', tryRC = TRUE, verbose = TRUE)
beep(sound = "fanfare")

write.csv(all.taxa, file.path(path_outputs, "COIAMC_AssignedTaxa.csv"))
```


# Prep for Phyloseq

First we need to update the row names for the seqtabB.nochim, all.taxa, and meta objects before attempting to make the phyloseq object. Since at least one sample was dropped during processing, we have to read meta in without row.names, remove the row(s) associated with the dropped sample(s), then reset the sampleID column to row.names
```{r name_check, include = FALSE}
# Update the all.taxa row names to be the actual sequences
all.taxa <- data.frame(all.taxa, row.names = 1)

# Order seqtab.nochim by row names
seqtabB.nochim <- seqtabB.nochim[order(row.names(seqtabB.nochim)), ]

# Order the meta file by sample name
meta <- meta[order(meta$sample), ]
# Remove the row(s) associated with dropped sample(s) from the meta spreadsheet - the only way I can do this is one row at a time
meta2 <- subset(meta, sample != "AMC22_AB01")
meta2 <- subset(meta2, sample != "AMC23_AB04") 
meta2 <- subset(meta2, sample != "AMC23_MB09") 

# Set the sample column as row.names
meta <- data.frame(meta2, row.names = 1)
# Order the meta file by row.names
meta <- meta[order(row.names(meta)), ]

# Make sure all samples in seqtab.nochim match and are in the same order as in meta, because the next step is a flat replacement of row.names
row.names(seqtabB.nochim) <- row.names(meta)
```


# Create the Phyloseq object
```{r ps_object, echo = FALSE}
# Create phyloseq object with all samples
EX_ps <- phyloseq(otu_table(seqtabB.nochim, taxa_are_rows=FALSE), sample_data(meta), tax_table(as.matrix(all.taxa)))

# SAVE 
saveRDS(EX_ps, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/InitialPhyloseqUD.rds")
```


# Exploratory richness plots
```{r initial_exploratory}
# I'd like to make a stacked bar plot, before any taxa-filtering, of the read abundances
# Set function to make a proportional graph
EX_ps.func <- transform_sample_counts(EX_ps, function(x) x / sum(x) )
# Plot based on Phylum
plot_bar(EX_ps.func, fill="Phylum") + ggtitle("Proportion by Phylum of Identified Reads - COI")
  # Warning: Removed 79588 rows containing missing values or values outside the scale range (`geom_bar()`). 

# Save plot
ggsave("PreDecontam_IDProportions.png", path = path_outputs, plot_bar(EX_ps.func, fill="Phylum") + ggtitle("Proportion by Phylum of Identified Reads"), width = 7, height = 4, units = "in") 
```

### NTS: May need to repeat ^ plot, sample IDs are illegible

# Step 3: Decontam - From 12S

## Inspect Library Sizes
```{r library_size, echo = FALSE}
df <- as.data.frame(sample_data(EX_ps)) # Put sample_data into a ggplot-friendly data.frame
df$LibrarySize <- sample_sums(EX_ps)
df <- df[order(df$LibrarySize),]
df$Index <- seq(nrow(df))
ggplot(data=df, aes(x=Index, y=LibrarySize, color=Type)) + geom_point()

ggsave("DecontamLibrarySize_Type.png", path = path_outputs, ggplot(data=df, aes(x=Index, y=LibrarySize, color=Type)) + geom_point() + ggtitle("COI Identified Read Numbers per Sample, by Sample Type"), width = 6, height = 4, units = "in") 

```
Controls and eDNA are intermixed

Check order by year too
```{r library_size, echo = FALSE}
df <- as.data.frame(sample_data(EX_ps)) # Put sample_data into a ggplot-friendly data.frame
df$LibrarySize <- sample_sums(EX_ps)
df <- df[order(df$LibrarySize),]
df$Index <- seq(nrow(df))
ggplot(data=df, aes(x=Index, y=LibrarySize, color=Year)) + geom_point()

ggsave("DecontamLibrarySize_Year.png", path = path_outputs, ggplot(data=df, aes(x=Index, y=LibrarySize, color=Year)) + geom_point() + ggtitle("COI Identified Read Numbers per Sample, by Collection Year"), width = 6, height = 4, units = "in") 
```
Unsure why it's making a gradient - figure out later, but 2022 seems to have fewer reads than 2023 - will see if that persists after decontam


## Remove Contaminants

The standard decontam package uses either frequency or prevalence to identify and remove contaminants, but relies on blanks being true negative controls to use presence/absence of reads between true and negative samples to identify contaminants. Since these blank samples do have a significant number of reads, I use the Ishaq decontam method instead - isolating reads that were identified in the controls and removing them from all samples, based on which samples were extracted with which blank.


```{r decontam_isolate, echo = FALSE}
# subset each DNA extraction batch
batch1 = subset_samples(EX_ps, Ebatch == "1")  
batch2 = subset_samples(EX_ps, Ebatch == "2")
batch3 = subset_samples(EX_ps, Ebatch == "3")  
batch4 = subset_samples(EX_ps, Ebatch == "4")  
batch5 = subset_samples(EX_ps, Ebatch == "5")  
batch6 = subset_samples(EX_ps, Ebatch == "6")  
batch7 = subset_samples(EX_ps, Ebatch == "7")  
batch8 = subset_samples(EX_ps, Ebatch == "8")  
batch9 = subset_samples(EX_ps, Ebatch == "9")  
batch10 = subset_samples(EX_ps, Ebatch == "10")  


# subset controls and prune to only those taxa
batch1_sub = subset_samples(batch1, Type == "NegCon")
batch2_sub = subset_samples(batch2, Type == "NegCon")
batch3_sub = subset_samples(batch3, Type == "NegCon")
batch4_sub = subset_samples(batch4, Type == "NegCon")
batch5_sub = subset_samples(batch5, Type == "NegCon")
batch6_sub = subset_samples(batch6, Type == "NegCon")
batch7_sub = subset_samples(batch7, Type == "NegCon")
batch8_sub = subset_samples(batch8, Type == "NegCon")
batch9_sub = subset_samples(batch9, Type == "NegCon")
batch10_sub = subset_samples(batch10, Type == "NegCon")


batch1_sub <- prune_taxa(taxa_sums(batch1_sub) > 0, batch1_sub)
batch2_sub <- prune_taxa(taxa_sums(batch2_sub) > 0, batch2_sub)
batch3_sub <- prune_taxa(taxa_sums(batch3_sub) > 0, batch3_sub)
batch4_sub <- prune_taxa(taxa_sums(batch4_sub) > 0, batch4_sub)
batch5_sub <- prune_taxa(taxa_sums(batch5_sub) > 0, batch5_sub)
batch6_sub <- prune_taxa(taxa_sums(batch6_sub) > 0, batch6_sub)
batch7_sub <- prune_taxa(taxa_sums(batch7_sub) > 0, batch7_sub)
batch8_sub <- prune_taxa(taxa_sums(batch8_sub) > 0, batch8_sub)
batch9_sub <- prune_taxa(taxa_sums(batch9_sub) > 0, batch9_sub)

# Batch 10's control has no reads, can keep all of batch10
#batch10_sub <- prune_taxa(taxa_sums(batch10_sub) > 0, batch10_sub)


# Make the taxa names into a vector so you can remove them, then use the keep vector for the prune taxa argument, because it wants the argument to be true (matching), and repeat for both batches
batch1_ctrl <- as.vector(taxa_names(batch1_sub)) 
batch1_vec <- as.vector(taxa_names(batch1)) 
batch1_kp <- setdiff(batch1_vec, batch1_ctrl)
batch1_clean <- prune_taxa(batch1_kp, batch1)

batch2_ctrl <- as.vector(taxa_names(batch2_sub)) 
batch2_vec <- as.vector(taxa_names(batch2)) 
batch2_kp <- setdiff(batch2_vec, batch2_ctrl)
batch2_clean <- prune_taxa(batch2_kp, batch2)

batch3_ctrl <- as.vector(taxa_names(batch3_sub)) 
batch3_vec <- as.vector(taxa_names(batch3)) 
batch3_kp <- setdiff(batch3_vec, batch3_ctrl)
batch3_clean <- prune_taxa(batch3_kp, batch3)

batch4_ctrl <- as.vector(taxa_names(batch4_sub)) 
batch4_vec <- as.vector(taxa_names(batch4)) 
batch4_kp <- setdiff(batch4_vec, batch4_ctrl)
batch4_clean <- prune_taxa(batch4_kp, batch4)

batch5_ctrl <- as.vector(taxa_names(batch5_sub)) 
batch5_vec <- as.vector(taxa_names(batch5)) 
batch5_kp <- setdiff(batch5_vec, batch5_ctrl)
batch5_clean <- prune_taxa(batch5_kp, batch5)

batch6_ctrl <- as.vector(taxa_names(batch6_sub)) 
batch6_vec <- as.vector(taxa_names(batch6)) 
batch6_kp <- setdiff(batch6_vec, batch6_ctrl)
batch6_clean <- prune_taxa(batch6_kp, batch6)

batch7_ctrl <- as.vector(taxa_names(batch7_sub)) 
batch7_vec <- as.vector(taxa_names(batch7)) 
batch7_kp <- setdiff(batch7_vec, batch7_ctrl)
batch7_clean <- prune_taxa(batch7_kp, batch7)

batch8_ctrl <- as.vector(taxa_names(batch8_sub)) 
batch8_vec <- as.vector(taxa_names(batch8)) 
batch8_kp <- setdiff(batch8_vec, batch8_ctrl)
batch8_clean <- prune_taxa(batch8_kp, batch8)

batch9_ctrl <- as.vector(taxa_names(batch9_sub)) 
batch9_vec <- as.vector(taxa_names(batch9)) 
batch9_kp <- setdiff(batch9_vec, batch9_ctrl)
batch9_clean <- prune_taxa(batch9_kp, batch9)


# Save the phyloseq object of identified contaminants
pstrimmed <- merge_phyloseq(batch1_sub, batch2_sub, batch3_sub, batch4_sub, batch5_sub, batch6_sub, batch7_sub, batch8_sub, batch9_sub) 
saveRDS(pstrimmed, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/DecontamRemoved.rds")

```

```{r}
# Merge the phyloseq objects back together, then remove any blank taxa or samples
ps_clean <- merge_phyloseq(batch1_clean, batch2_clean, batch3_clean, batch4_clean, batch5_clean, batch6_clean, batch7_clean, batch8_clean, batch9_clean, batch10) 

# Clean out taxa/SV columns that are no longer present
ps_clean <- prune_taxa(taxa_sums(ps_clean) > 0, ps_clean) 
ps_clean <- prune_samples(sample_sums(ps_clean) > 0, ps_clean)

ps_clean

# Save in case R crashes again
saveRDS(ps_clean, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/Intermediate.rds")

ps_clean.stack <- transform_sample_counts(ps_clean, function(x) x / sum(x) )
# Plot based on Phylum
plot_bar(ps_clean.stack, fill="Kingdom") + ggtitle("Proportion by Kingdom of Identified Reads - COI")
```
phyloseq-class experiment-level object
otu_table()   OTU Table:         [ 19025 taxa and 52 samples ]
sample_data() Sample Data:       [ 52 samples by 15 sample variables ]
tax_table()   Taxonomy Table:    [ 19025 taxa by 6 taxonomic ranks ]

I still want to remove bacterial and plant/algae identifications, so don't save the object yet

```{r remove_taxa, echo = FALSE}
# Remove plant and others
psCOI <- ps_clean %>% 
  subset_taxa(Kingdom != "Ascomycota") 
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Bacillariophyta")
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Basidiomycota")
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Chlorophyta")
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Oomycota")
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Pseudomonadota")
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Rhodophyta")
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Rotifera")
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Tubulinea")
psCOI <- psCOI %>% 
  subset_taxa(Kingdom != "Streptophyta")
```

```{r clean_columns, echo = FALSE}
psCOI <- prune_taxa(taxa_sums(psCOI) > 0, psCOI) 
psCOI <- prune_samples(sample_sums(psCOI) > 0, psCOI)

psCOI.stack <- transform_sample_counts(psCOI, function(x) x / sum(x) )
# Plot based on Class
plot_bar(psCOI.stack, fill="Class") + ggtitle("Proportion of Cleaned Reads by Class.png")

psCOI

ggsave("CleanedClassIDs.png", path = path_outputs, plot_bar(psCOI.stack, fill="Class") + ggtitle("Proportion of Cleaned Reads by Class.png") + ggtitle("COI Identified Clean Reads"), width = 8, height = 5, units = "in") 

saveRDS(psCOI, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/COIB/5-May03-2024/CleanPhyloseq.RDS")
```
phyloseq-class experiment-level object
otu_table()   OTU Table:         [ 140 taxa and 33 samples ]
sample_data() Sample Data:       [ 33 samples by 15 sample variables ]
tax_table()   Taxonomy Table:    [ 140 taxa by 6 taxonomic ranks ]


### All prior clean

# Step 4: Alpha Diversity Analysis

## Rarefaction
```{r rarecurve, echo = FALSE}
rc <-as.data.frame(otu_table(psCOI))
test <- rarecurve(rc, step = 10, cex=0.5, label = FALSE)

raremax <- max(rowSums(otu_table(psCOI))) # Max SV number is 35078
raremin <- min(rowSums(otu_table(psCOI))) # Min SV number is 3
rowSums(otu_table(psCOI))
```


## Chao Alpha Diversity