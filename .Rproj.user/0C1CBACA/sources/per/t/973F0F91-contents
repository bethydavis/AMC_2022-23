---
title: "AMCCOI"
author: "BYDavis"
date: "2024-04-17"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Markdown file to analyze AMC water eDNA samples, using Dr. Sue Ishaq's DNA Sequencing Analysis workflow (to start). 

Debugging statements are preceded with ###

# Set up packages and paths with config file
```{r COI_config}
source("AMCCOI_config.R")
```

Prior to this Rmd, the raw fastq files have been renamed in the repo UM_FSM_Cleaning

# Set folder paths and file types
```{r raw_paths, echo = FALSE}

# Specify file name formats
fnsBF <- list.files(path_rawBF)
fastqsBF <- fnsBF[grepl('.gz$', fnsBF)]

fnsBR <- list.files(path_rawBR)
fastqsBR <- fnsBR[grepl('.gz$', fnsBR)]

# The file paths still include the desktop.ini file, so let's specify a file path to lead future code only to the fastq files in the folder

fnsBFisolate <- file.path(path_rawBF, fastqsBF)
fnsBRisolate <- file.path(path_rawBR, fastqsBR)

# Set sample names to a vector
# Remove path and the .gz extension
# Repeat file_path_sans_ext to also remove the .fastq
namesBFfast <- tools::file_path_sans_ext(basename(fastqsBF))
namesBF <- tools::file_path_sans_ext(namesBFfast)

namesBRfast <- tools::file_path_sans_ext(basename(fastqsBR))
# Repeat file_path_sans_ext to also remove the .fastq
namesBR <- tools::file_path_sans_ext(namesBRfast)

```

The sample names are already set how I want them, so I won't use/adapt Sue's file name cleaning chunk (Clean File Names for Initial Load)

# Check Sequence Quality

Run 4 randomly selected quality profile plots per primer per read type (forward or reverse). Number will correspond to the file order. Min is set to 2 to avoid the desktop.ini file that GoogleDrive desktop creates from being chosen

```{r random_selection}
# Numbers for BF2 Forward reads:
sample(1:78, 6, replace = FALSE)
  # Results on 3/20/2024: 48 64 20 57 15 11

# Numbers for BF2 Reverse reads:
sample(1:78, 6, replace = FALSE)
  # Results on 3/20/2024: 15, 49, 56, 78, 11, 28
```

## NTS: keep working at cleaning out the 12S from here

5/03 note: run the plots again to standardize size
```{r}
# B Forward:
#ggsave("BF2ForwardQuality_RNG48.png", path = path_outputs, plot = plotQualityProfile(fnsBFisolate[48]), width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG64.png", plot = plotQualityProfile(fnsBFisolate[64]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG20.png", plot = plotQualityProfile(fnsBFisolate[20]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG57.png", plot = plotQualityProfile(fnsBFisolate[57]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG15.png", plot = plotQualityProfile(fnsBFisolate[15]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BF2ForwardQuality_RNG11.png", plot = plotQualityProfile(fnsBFisolate[11]), path = path_outputs, width = 6, height = 4, units = "in")

beep(sound = "fanfare")
```

```{r}
# B Reverse:
ggsave("BR2ReverseQuality_RNG15.png", plot = plotQualityProfile(fnsBRisolate[15]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG49.png", plot = plotQualityProfile(fnsBRisolate[49]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG56.png", plot = plotQualityProfile(fnsBRisolate[56]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG77.png", plot = plotQualityProfile(fnsBRisolate[77]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG11.png", plot = plotQualityProfile(fnsBRisolate[11]), path = path_outputs, width = 6, height = 4, units = "in")

ggsave("BR2ReverseQuality_RNG28.png", plot = plotQualityProfile(fnsBRisolate[28]), path = path_outputs, width = 6, height = 4, units = "in")

beep(sound = "fanfare")
```

The indexes and adapters have been removed from the sequence files, but the original primers are still attached. Need to account for their presence in the trim amounts and remove them.

BF2/BR2 COI Region: ~420 bp - this is longer than the cycle length, but should still provide 260bp of overlap

BF2 primer length: 20 -  GCHCCHGAYATRGCHTTYCC 

BR2 primer length: 20 -  TCDGGRTGNCCRAARAAYCA 


Quality notes:
BF2 Forward: cut after 240, potentially from 0-50 also?

BR2 Reverse: these are a hot mess. Run more randoms. Post-randoms - there's a consistent bad low quality spike around 50-60. Trim from 60-200

### NTS: 3/05/2024
Parameters used for original COIB filterAndTrim:
filtoutBF <- filterAndTrim(file.path(path_rawBF, fastqsBF), file.path(path_filtBF, paste0(namesBF, "filt.fastq.gz")), trimLeft = 20, truncLen = 270, maxEE = 2, maxN = 0, verbose = TRUE)
filtoutBR <- filterAndTrim(file.path(path_rawBR, fastqsBR), file.path(path_filtBR, paste0(namesBR, "filt.fastq.gz")), trimLeft = 20, truncLen = 240, maxEE = 2, maxN = 0, verbose = TRUE)

### NTS: 3/20/2024
Parameters used for COIB filterAndTrim:
filterAndTrim(file.path(path_rawBF, fastqsBF), file.path(path_filtBF, paste0(namesBF, "filt.fastq.gz")), trimLeft = 50, truncLen = 240, maxEE = 2, maxN = 0, verbose = TRUE)
filtoutBR <- filterAndTrim(file.path(path_rawBR, fastqsBR), file.path(path_filtBR, paste0(namesBR, "filt.fastq.gz")), trimLeft = 60, truncLen = 200, maxEE = 2, maxN = 0, verbose = TRUE)

### NTS: 3/22/2024. After merging the reads with the quality-guided filtering parameters (above), I'm worried about the large drops in read number, and wondering if the trimming was too aggressive, and sacrificed overlap for quality. Going to rerun the BF2/BR2 filtering, into a different output folder, with the following parameters:

BF2 Forward: cut left 20, minQ = 25
filtoutBF <- filterAndTrim(file.path(path_rawBF, fastqsBF), file.path(path_filtBF, paste0(namesBF, "filt.fastq.gz")), trimLeft = 20, minQ = 25, maxEE = 2, maxN = 0, verbose = TRUE)
BR2 Reverse: cut left 20, minQ = 25
filtoutBR <- filterAndTrim(file.path(path_rawBR, fastqsBR), file.path(path_filtBR, paste0(namesBR, "filt.fastq.gz")), trimLeft = 20, minQ = 25, maxEE = 2, maxN = 0, verbose = TRUE)

This should target only removing the primers and low quality reads, and leave as much of the rest as possible intact for overlap

### NTS: 5/03/2024. After talking to Andy about ^, involving minQ was the exact wrong thing to do. I need to remove the primers, so the trimLeft = 20 will stay, and would like to cut off at a quality score of 25. 


# Begin filtering process

First set directories for a new set of folders - the locations that the filtered files will go into

```{r filterlocale}


filt12F <- file.path(path_filt12F, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


Running the reads separate even when I do have pairs because the BF2/BR2 has a mismatch in number of files currently and I'd rather do them all the same. From my understanding, this may let some lower quality reads through since it'll allow individual reads pass when it wouldn't pass as a pair, but I figure the trim settings will already take care of most of the issues, and there are further cleaning steps too.


```{r filt_BF}
filtoutBF <- filterAndTrim(file.path(path_rawBF, fastqsBF), file.path(path_filtBF, paste0(namesBF, "filt.fastq.gz")), trimLeft = 20, minQ = 25, maxEE = 2, maxN = 0, verbose = TRUE)

saveRDS(filtoutBF, "C:/Users/bydav/Desktop/MathFilterTestAMC/BF_FilteredOutputs.rds")
```
AMC22_AB01_BR1 removed
AMC23_AB04_BR1 removed
AMC23_MB09_BR1 removed


```{r filt_BR}
filtoutBR <- filterAndTrim(file.path(path_rawBR, fastqsBR), file.path(path_filtBR, paste0(namesBR, "filt.fastq.gz")), trimLeft = 20, minQ = 25, maxEE = 2, maxN = 0, verbose = TRUE)

saveRDS(filtoutBR, "C:/Users/bydav/Desktop/MathFilterTestAMC/BR_FilteredOutputs.rds")
```
AMC22_AB01_BR2 removed
AMC23_AB04_BR2 removed
AMC23_MB09_BR2 removed

```{r filt12_both}
filtoutBsync <- filterAndTrim(file.path(path_rawBF, fastqsBF), file.path(path_filtBF, paste0(namesBF, "filt.fastq.gz")), file.path(path_rawBR, fastqsBR), file.path(path_filtBR, paste0(namesBR, "filt.fastq.gz")), trimLeft = c(20,20), minQ=c(25,25), maxN=0, maxEE=c(2,2), verbose=TRUE) 


saveRDS(filtoutBsync, "C:/Users/bydav/Desktop/MathFilterTestAMC/BSync_FilteredOutputs.rds)
```
AMC22_AB01_BR2 and BR1 removed
AMC23_MB09_BR2 and BR1 removed


### Load filt outputs back in if needed:
```{r}
filtout12F <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/12F_FilteredOutputs.rds")

filtout12R <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/12R_FilteredOutputs.rds")

filtoutBF <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/BF_FilteredOutputs.rds")

filtoutBR <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/BR_FilteredOutputs.rds")

filtout12sync <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/12Sync_FilteredOutputs.rds")

filtoutBsync <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/BSync_FilteredOutputs.rds)
```

# Explore filtered output 

Can look at the dimensions of each RDS output, and order by the filtered read amount, and/or look at the number of total raw reads compared to the filtered out reads

```{r assess filt_12F}
filtout12F
dim(filtout12F)

# Order by filtered read amount
filtout12F[order(filtout12F[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtout12F)
```


```{r assess filt_12R}
filtout12R
dim(filtout12R)

# Order by filtered read amount
filtout12R[order(filtout12R[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtout12R)
```


```{r assess filt_BF}
filtoutBF
dim(filtoutBF)

# Order by filtered read amount
filtoutBF[order(filtoutBF[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtoutBF)
```


```{r assess filt_BR}
filtoutBR
dim(filtoutBR)

# Order by filtered read amount
filtoutBR[order(filtoutBR[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtoutBR)
```

# Look at filtered trends
```{r filtertrends}
filttrend12F <- ggplot(as.data.frame(filtout12F)) + geom_point(aes(row.names(filtout12F), reads.in), color = "blue") + geom_point(aes(row.names(filtout12F), reads.out), color = "orange") + ggtitle("Filter Trends for AMC 12S Forward Reads")

filttrend12R <- ggplot(as.data.frame(filtout12R)) + geom_point(aes(row.names(filtout12R), reads.in), color = "blue") + geom_point(aes(row.names(filtout12R), reads.out), color = "orange") + ggtitle("Filter Trends for AMC 12S Reverse Reads")

filttrendBF <- ggplot(as.data.frame(filtoutBF)) + geom_point(aes(row.names(filtoutBF), reads.in), color = "blue") + geom_point(aes(row.names(filtoutBF), reads.out), color = "orange") + ggtitle("Filter Trends for AMC BF2/BR2 Forward Reads")

filttrendBR <- ggplot(as.data.frame(filtoutBR)) + geom_point(aes(row.names(filtoutBR), reads.in), color = "blue") + geom_point(aes(row.names(filtoutBR), reads.out), color = "orange") + ggtitle("Filter Trends for AMC BF2/BR2 Reverse Reads")




filttrend12sync <- ggplot(as.data.frame(filtout12sync)) + geom_point(aes(row.names(filtoutBR), reads.in), color = "blue") + geom_point(aes(row.names(filtoutBR), reads.out), color = "orange") + ggtitle("Filter Trends for AMC 12 Synced Reverse Reads")

filttrendBsync <- ggplot(as.data.frame(filtoutBsync)) + geom_point(aes(row.names(filtoutBF), reads.in), color = "blue") + geom_point(aes(row.names(filtoutBF), reads.out), color = "orange") + ggtitle("Filter Trends for AMC BF2/BR2 Synced Reads")

ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/FilterTrends_12Sync.png", filttrend12sync)
ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/FilterTrends_BSync.png", filttrendBsync)

ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/FilterTrends_12F.png", filttrend12F)
ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/FilterTrends_12R.png", filttrend12R)
ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/FilterTrends_BF.png", filttrendBF)
ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/FilterTrends_BR.png", filttrendBR)
```

just out of curiosity's sake, let's also explore some quality profiles again from the filtered reads

```{r plotfiltered}

#Compare post and pre-filter and trim

plotQualityProfile("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Filtered/12S Reverse/AMC22_MB01_MR2filt.fastq.gz")

plotQualityProfile("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Raw/12S/Read2/AMC22_MB01_MR2.fastq.gz")

# 

plotQualityProfile("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Filtered/12S Reverse/AMC23_SU03_MR2filt.fastq.gz")

plotQualityProfile("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Raw/12S/Read2/AMC23_SU03_MR2.fastq.gz")
```

# Error Rates and Dereplication

First we need to set a seed and make sure the paths are all still set.

```{r error setup}
set.seed(0743)

# Set the names for the filtered files we'll be using

# create a list of files in the path
filtnames12Finter <- list.files(path_filt12F, full.names = TRUE)

# Specify I only want the files with the .gz extension
filtnames12F <- filtnames12Finter[grepl('.gz$', filtnames12Finter)]

# Extract just the file name, not the path, and remove the .gz extension. This leaves on the .fastq extension
fastqfilt12F <- tools::file_path_sans_ext(basename(filtnames12F))

# Remove the .fastq extension
names12Ffilt <- tools::file_path_sans_ext(basename(fastqfilt12F))



# Repeat for the other folders

# 12Reverse
filtnames12Rinter <- list.files(path_filt12R, full.names = TRUE)
filtnames12R <- filtnames12Rinter[grepl('.gz$', filtnames12Rinter)]
fastqfilt12R <- tools::file_path_sans_ext(basename(filtnames12R))
names12Rfilt <- tools::file_path_sans_ext(basename(fastqfilt12R))

# BForward
filtnamesBFinter <- list.files(path_filtBF, full.names = TRUE)
filtnamesBF <- filtnamesBFinter[grepl('.gz$', filtnamesBFinter)]
fastqfiltBF <- tools::file_path_sans_ext(basename(filtnamesBF))
namesBFfilt <- tools::file_path_sans_ext(basename(fastqfiltBF))

# BReverse
filtnamesBRinter <- list.files(path_filtBR, full.names = TRUE)
filtnamesBR <- filtnamesBRinter[grepl('.gz$', filtnamesBRinter)]
fastqfiltBR <- tools::file_path_sans_ext(basename(filtnamesBR))
namesBRfilt <- tools::file_path_sans_ext(basename(fastqfiltBR))
```

## Dereplication

This step comes from the dada2 workflow (https://bioconductor.org/packages/devel/bioc/vignettes/dada2/inst/doc/dada2-intro.html#learn-the-error-rates), not Sue's code

```{r derep}
derep12F <- derepFastq(filtnames12F, verbose=TRUE)
derep12R <- derepFastq(filtnames12R, verbose=TRUE)
derepBF <- derepFastq(filtnamesBF, verbose = TRUE)
derepBR <- derepFastq(filtnamesBR, verbose = TRUE)

```

Now that the names are set up, we can run the simulations to learn and estimate the number of errors in each filtered folder. This step is in Sue's code (and is the source of the saveRDS and plotErrors lines), but using the default parameters from the dada2 workflow

## Error Rates

```{r errorrates}
err12F <- learnErrors(derep12F, multithread = FALSE, randomize = TRUE)
# The default nbases is 1e8. Using this default since Sue's 1e6 parameter filled up after only one sample

saveRDS(err12F, "C:/Users/bydav/Desktop/MathFilterTestAMC/12Forward_Error.rds")

ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/12Forward_ErrorPlot.png", plotErrors(err12F, nominalQ = TRUE)) 



err12R <- learnErrors(derep12R, multithread = FALSE, randomize = TRUE)

saveRDS(err12R, "C:/Users/bydav/Desktop/MathFilterTestAMC/12Reverse_Error.rds")

ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/12Reverse_ErrorPlot.png", plotErrors(err12R, nominalQ = TRUE)) 



errBF <- learnErrors(derepBF, multithread = FALSE, randomize = TRUE)

saveRDS(errBF, "C:/Users/bydav/Desktop/MathFilterTestAMC/BForward_Error.rds")

ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/BForward_ErrorPlot.png", plotErrors(errBF, nominalQ = TRUE)) 



errBR <- learnErrors(derepBR, multithread = FALSE, randomize = TRUE)

saveRDS(errBR, "C:/Users/bydav/Desktop/MathFilterTestAMC/BReverse_Error.rds")

ggsave("C:/Users/bydav/Desktop/MathFilterTestAMC/BReverse_ErrorPlot.png", plotErrors(errBR, nominalQ = TRUE)) 

# This takes a long time. If you have to do it again, include this:
beep(sound = "fanfare")
```

### Load error rates back in if needed:
```{r}
err12F <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/12Forward_Error.rds")

err12R <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/12Reverse_Error.rds")

errBF <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/BForward_Error.rds")

errBR <- readRDS("C:/Users/bydav/Desktop/MathFilterTestAMC/BReverse_Error.rds")
```

# Infer sample composition
```{r samplecomposition}
dada12F <- dada(derep12F, err = err12F, multithread = FALSE)
print("dada 12F finished")
beep(sound = "coin")

dada12R <- dada(derep12R, err = err12R, multithread = FALSE)
print("dada 12R finished")
beep(sound = "coin")

dadaBF <- dada(derepBF, err = errBF, multithread = FALSE)
print("dada BF finished")
beep(sound = "coin")

dadaBR <- dada(derepBR, err = errBR, multithread = FALSE)
print("dada BR finished")
beep(sound = "fanfare")

# Save all in case I need to reload the dada objects
saveRDS(dada12F, "C:/Users/bydav/Desktop/MathFilterTestAMC/12Forward_SampleComp.rds")
saveRDS(dada12R, "C:/Users/bydav/Desktop/MathFilterTestAMC/12Reverse_SampleComp.rds")
saveRDS(dadaBF, "C:/Users/bydav/Desktop/MathFilterTestAMC/BForward_SampleComp.rds")
saveRDS(dadaBR, "C:/Users/bydav/Desktop/MathFilterTestAMC/BReverse_SampleComp.rds")
```


Merge paired reads

* Apparently if you filter and trim separately, you cannot merge after that point, and must filterandTrim the forward and reverse reads together. (source: the DADA2 pipeline tutorial)
Restarting with that in mind
```{r}
mergers12 <- mergePairs(dada12F, path_filt12F, dada12R, path_filt12R, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers12[[1]])

mergersB <- mergePairs(dadaBF, derepBF, dadaBR, derepBR, verbose = TRUE)
head(mergersB[[1]])


saveRDS(mergers12, "C:/Users/bydav/Desktop/MathFilterTestAMC/12MergedSampleComp.rds")
saveRDS(mergersB, "C:/Users/bydav/Desktop/MathFilterTestAMC/BMergedSampleComp.rds")
```

Now make a sequence table (A for unmerged, B for merged)

A - Unmerged:

```{r seqtabA}
seqtab12F <- makeSequenceTable(dada12F)
dim(seqtab12F)
saveRDS(seqtab12F, "C:/Users/bydav/Desktop/MathFilterTestAMC/12F Sequence Table.rds")

seqtab12R <- makeSequenceTable(dada12R)
dim(seqtab12R)
saveRDS(seqtab12R, "C:/Users/bydav/Desktop/MathFilterTestAMC/12R Sequence Table.rds")

seqtabBF <- makeSequenceTable(dadaBF)
dim(seqtabBF)
saveRDS(seqtabBF, "C:/Users/bydav/Desktop/MathFilterTestAMC/BF Sequence Table.rds")

seqtabBR <- makeSequenceTable(dadaBR)
dim(seqtabBR)
saveRDS(seqtabBR, "C:/Users/bydav/Desktop/MathFilterTestAMC/BR Sequence Table.rds")

```

# Remove Chimeras A
```{r nochimA}
seqtab12F.nochim <- removeBimeraDenovo(seqtab12F, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtab12F.nochim)
saveRDS(seqtab12F.nochim, "C:/Users/bydav/Desktop/MathFilterTestAMC/12F Sequence No Chimera Table.rds")

seqtab12R.nochim <- removeBimeraDenovo(seqtab12R, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtab12R.nochim)
saveRDS(seqtab12R.nochim, "C:/Users/bydav/Desktop/MathFilterTestAMC/12R Sequence No Chimera Table.rds")

seqtabBF.nochim <- removeBimeraDenovo(seqtabBF, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtabBF.nochim)
saveRDS(seqtabBF.nochim, "C:/Users/bydav/Desktop/MathFilterTestAMC/BF Sequence No Chimera Table.rds")

seqtabBR.nochim <- removeBimeraDenovo(seqtabBR, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtabBR.nochim)
saveRDS(seqtabBR.nochim, "C:/Users/bydav/Desktop/MathFilterTestAMC/BR Sequence No Chimera Table.rds")

beep(sound = "coin")
```

B - Merged:

```{r seqtabB}
seqtab12 <- makeSequenceTable(mergers12)
dim(seqtab12)
saveRDS(seqtab12, "C:/Users/bydav/Desktop/MathFilterTestAMC/12Merged Sequence Table.rds")

seqtabB <- makeSequenceTable(mergersB)
dim(seqtabB)
saveRDS(seqtabB, "C:/Users/bydav/Desktop/MathFilterTestAMC/BMerged Sequence Table.rds")

dim(seqtab12)
table(nchar(getSequences(seqtab12)))

dim(seqtabB)
table(nchar(getSequences(seqtabB)))
```

# Remove Chimeras B
```{r nochimB}
seqtab12.nochim <- removeBimeraDenovo(seqtab12, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtab12.nochim)
saveRDS(seqtab12.nochim, "C:/Users/bydav/Desktop/MathFilterTestAMC/12Merged Sequence No Chimera Table.rds")

seqtabB.nochim <- removeBimeraDenovo(seqtabB, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtabB.nochim)
saveRDS(seqtabB.nochim, "C:/Users/bydav/Desktop/MathFilterTestAMC/BMerged Sequence No Chimera Table.rds")

beep(sound = "coin")

dim(seqtab12.nochim)
table(nchar(getSequences(seqtab12.nochim)))

dim(seqtabB.nochim)
table(nchar(getSequences(seqtabB.nochim)))
```


# Next

The process from here on is to download a reference sequence library in .fa.gz format, then use phyloseq to assignTaxonomy to the entire seqtab.nochim objects. Then identify which species are present in the negative controls, and subtract them from the samples (either all samples or batches). This is the decontam process in DNASeqAnalysis_3.rmd. We will subtract the control species from batches of samples, based on which controls were collected, filtered, and extracted with which 'real samples.' Refer to the field and lab notebooks to generate this list. This will probably get messy and might even cross projects. 


[] download reference libraries for the 12S primers and BF2/BR2 COI primers
[] AssignTaxonomy
[] Proceed with the decontam process
[] Remove any unwanted taxa
[] Continue with rarefaction analysis and then the rest of the diversity and abundance tests


# Workflow Verification

```{r workflow_verification, echo = FALSE}
seqtab12.nochim <- readRDS("G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Merged Sequence No Chimera Table.rds")

getN <- function(x) sum(getUniques(x))

track12 <- cbind(filtout12sync, sapply(dada12F, getN), sapply(dada12R, getN), sapply(mergers12, getN), rowSums(seqtab12.nochim))

colnames(track12) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
#rownames(track) <- sample.names
head(track12)


trackB <- cbind(filtoutBsync, sapply(dadaBF, getN), sapply(dadaBR, getN), sapply(mergersB, getN), rowSums(seqtabB.nochim))



colnames(trackB) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
#rownames(track) <- sample.names
head(trackB)
```

trackB is offset because of the files that were removed during filtering, but the same trend is present. Most common large removals happen at filtering and merging.

Filtering and merging lost the most number of reads across samples, but it isn't a constant enough effect to seem like a full error. May confer with Andy on if the filtering criteria are too strict

# Assign Taxonomy

```{r assign_taxonomy, echo = FALSE}
# Very slow and intensive
all.taxa <- assignTaxonomy(seqtab12.nochim, 'C:/Users/bydav/Desktop/a04_REFDBdada.fasta', tryRC = TRUE, verbose = TRUE)
beep(sound = "fanfare")
```



```{r taxonomy_results, echo = FALSE}
saveRDS(all.taxa, "G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12SAMC_AssignedTaxa.rds")
write.csv(all.taxa, 'G:/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12SAMC_AssignedTaxa.csv')
```

# Phyloseq

Load in a metadata file - make sure the sample names and order match between it and the seqtab
```{r name_check, include = FALSE}
meta <- read.csv("C:/Users/bydav/Desktop/AMCMeta.csv")
row.names(seqtab12.nochim)


# Convince R by force that they match - this is a flat replacement, not a match-up
row.names(meta) <- row.names(seqtab12.nochim) 
```

```{r ps_object, echo = FALSE}
# Create phyloseq object with all samples
EX_ps <- phyloseq(otu_table(seqtab12.nochim, taxa_are_rows=FALSE), sample_data(meta), tax_table(all.taxa))

EX_ps
```


# Decontam with Negative Controls (Ishaq Method)
Clean all batches and then merge together into a clean phyloseq object - make this into a loop after MSWC
```{r}
# batch 3
fbatch3 = subset_samples(EX_ps, Fbatch == "3")  
fbatch3_kit = subset_samples(fbatch3, Type == "NegCon")
fbatch3_kit <- prune_taxa(taxa_sums(fbatch3_kit) > 0, fbatch3_kit)
fb3_control_vec <- as.vector(taxa_names(fbatch3_kit)) 
fb3_vec <- as.vector(taxa_names(fbatch3)) 
fb3_keep <- setdiff(fb3_vec, fb3_control_vec)
fb3_clean <- prune_taxa(fb3_keep, fbatch3)

# batch 4
#fbatch4 = subset_samples(EX_ps, Fbatch == "4")  
#fbatch4_kit = subset_samples(fbatch4, Type == "NegCon")
#fbatch4_kit <- prune_taxa(taxa_sums(fbatch4_kit) > 0, fbatch4_kit)
#fb4_control_vec <- as.vector(taxa_names(fbatch4_kit)) 
#fb4_vec <- as.vector(taxa_names(fbatch4)) 
#fb4_keep <- setdiff(fb4_vec, fb4_control_vec)
#fb4_clean <- prune_taxa(fb4_keep, fbatch4)

# batch 5
fbatch5 = subset_samples(EX_ps, Fbatch == "5")  
fbatch5_kit = subset_samples(fbatch5, Type == "NegCon")
fbatch5_kit <- prune_taxa(taxa_sums(fbatch5_kit) > 0, fbatch5_kit)
fb5_control_vec <- as.vector(taxa_names(fbatch5_kit)) 
fb5_vec <- as.vector(taxa_names(fbatch5)) 
fb5_keep <- setdiff(fb5_vec, fb5_control_vec)
fb5_clean <- prune_taxa(fb5_keep, fbatch5)

# batch 6
fbatch6 = subset_samples(EX_ps, Fbatch == "6")  
fbatch6_kit = subset_samples(fbatch6, Type == "NegCon")
fbatch6_kit <- prune_taxa(taxa_sums(fbatch6_kit) > 0, fbatch6_kit)
fb6_control_vec <- as.vector(taxa_names(fbatch6_kit)) 
fb6_vec <- as.vector(taxa_names(fbatch6)) 
fb6_keep <- setdiff(fb6_vec, fb6_control_vec)
fb6_clean <- prune_taxa(fb6_keep, fbatch6)

# batch 7
fbatch7 = subset_samples(EX_ps, Fbatch == "7")  
fbatch7_kit = subset_samples(fbatch7, Type == "NegCon")
fbatch7_kit <- prune_taxa(taxa_sums(fbatch7_kit) > 0, fbatch7_kit)
fb7_control_vec <- as.vector(taxa_names(fbatch7_kit)) 
fb7_vec <- as.vector(taxa_names(fbatch7)) 
fb7_keep <- setdiff(fb7_vec, fb7_control_vec)
fb7_clean <- prune_taxa(fb7_keep, fbatch7)

# batch 8
fbatch8 = subset_samples(EX_ps, Fbatch == "8")  
fbatch8_kit = subset_samples(fbatch8, Type == "NegCon")
fbatch8_kit <- prune_taxa(taxa_sums(fbatch8_kit) > 0, fbatch8_kit)
fb8_control_vec <- as.vector(taxa_names(fbatch8_kit)) 
fb8_vec <- as.vector(taxa_names(fbatch8)) 
fb8_keep <- setdiff(fb8_vec, fb8_control_vec)
fb8_clean <- prune_taxa(fb8_keep, fbatch8)

# batch 9
fbatch9 = subset_samples(EX_ps, Fbatch == "9")  
fbatch9_kit = subset_samples(fbatch9, Type == "NegCon")
fbatch9_kit <- prune_taxa(taxa_sums(fbatch9_kit) > 0, fbatch9_kit)
fb9_control_vec <- as.vector(taxa_names(fbatch9_kit)) 
fb9_vec <- as.vector(taxa_names(fbatch9)) 
fb9_keep <- setdiff(fb9_vec, fb9_control_vec)
fb9_clean <- prune_taxa(fb9_keep, fbatch9)

# batch 10
fbatch10 = subset_samples(EX_ps, Fbatch == "10")  
fbatch10_kit = subset_samples(fbatch10, Type == "NegCon")
fbatch10_kit <- prune_taxa(taxa_sums(fbatch10_kit) > 0, fbatch10_kit)
fb10_control_vec <- as.vector(taxa_names(fbatch10_kit)) 
fb10_vec <- as.vector(taxa_names(fbatch10)) 
fb10_keep <- setdiff(fb10_vec, fb10_control_vec)
fb10_clean <- prune_taxa(fb10_keep, fbatch10)

# batch 11
fbatch11 = subset_samples(EX_ps, Fbatch == "11")  
fbatch11_kit = subset_samples(fbatch11, Type == "NegCon")
fbatch11_kit <- prune_taxa(taxa_sums(fbatch11_kit) > 0, fbatch11_kit)
fb11_control_vec <- as.vector(taxa_names(fbatch11_kit)) 
fb11_vec <- as.vector(taxa_names(fbatch11)) 
fb11_keep <- setdiff(fb11_vec, fb11_control_vec)
fb11_clean <- prune_taxa(fb11_keep, fbatch11)

# batch 12
#fbatch12 = subset_samples(EX_ps, Fbatch == "12")  
#fbatch12_kit = subset_samples(fbatch12, Type == "NegCon")
#fbatch12_kit <- prune_taxa(taxa_sums(fbatch12_kit) > 0, fbatch12_kit)
#fb12_control_vec <- as.vector(taxa_names(fbatch12_kit)) 
#fb12_vec <- as.vector(taxa_names(fbatch12)) 
#fb12_keep <- setdiff(fb12_vec, fb12_control_vec)
#fb12_clean <- prune_taxa(fb12_keep, fbatch12)

# batch 13
fbatch13 = subset_samples(EX_ps, Fbatch == "13")  
fbatch13_kit = subset_samples(fbatch13, Type == "NegCon")
fbatch13_kit <- prune_taxa(taxa_sums(fbatch13_kit) > 0, fbatch13_kit)
fb13_control_vec <- as.vector(taxa_names(fbatch13_kit)) 
fb13_vec <- as.vector(taxa_names(fbatch13)) 
fb13_keep <- setdiff(fb13_vec, fb13_control_vec)
fb13_clean <- prune_taxa(fb13_keep, fbatch13)

# batch 14
fbatch14 = subset_samples(EX_ps, Fbatch == "14")  
fbatch14_kit = subset_samples(fbatch14, Type == "NegCon")
fbatch14_kit <- prune_taxa(taxa_sums(fbatch14_kit) > 0, fbatch14_kit)
fb14_control_vec <- as.vector(taxa_names(fbatch14_kit)) 
fb14_vec <- as.vector(taxa_names(fbatch14)) 
fb14_keep <- setdiff(fb14_vec, fb14_control_vec)
fb14_clean <- prune_taxa(fb14_keep, fbatch14)

# batch 15
fbatch15 = subset_samples(EX_ps, Fbatch == "15")  
fbatch15_kit = subset_samples(fbatch15, Type == "NegCon")
fbatch15_kit <- prune_taxa(taxa_sums(fbatch15_kit) > 0, fbatch15_kit)
fb15_control_vec <- as.vector(taxa_names(fbatch15_kit)) 
fb15_vec <- as.vector(taxa_names(fbatch15)) 
fb15_keep <- setdiff(fb15_vec, fb15_control_vec)
fb15_clean <- prune_taxa(fb15_keep, fbatch15)

# batch 0

fbatch0 = subset_samples(EX_ps, Fbatch == "0")  
fbatch0_kit = subset_samples(fbatch0, Type == "NegCon")
fbatch0_kit <- prune_taxa(taxa_sums(fbatch0_kit) > 0, fbatch0_kit)
fb0_control_vec <- as.vector(taxa_names(fbatch0_kit)) 
fb0_vec <- as.vector(taxa_names(fbatch0)) 
fb0_keep <- setdiff(fb0_vec, fb0_control_vec)
fb0_clean <- prune_taxa(fb0_keep, fbatch0)

```

```{r}
# Merge the phyloseq objects back together, then remove any blank taxa or samples
EX_ps_NC_batch_clean <- merge_phyloseq(fb0_clean, fb2_clean, fb3_clean, fb5_clean, fb6_clean, fb7_clean, fb8_clean, fb9_clean, fb11_clean, fb14_clean, fb15_clean) 

# Clean out taxa/SV columns that are no longer present
EX_ps_NC_batch_clean <- prune_taxa(taxa_sums(EX_ps_NC_batch_clean) > 0, EX_ps_NC_batch_clean) 

EX_ps_NC_batch_clean <- prune_samples(sample_sums(EX_ps_NC_batch_clean) > 0, EX_ps_NC_batch_clean)
EX_ps_NC_batch_clean
```


```{r recheck_ordination, echo = FALSE}
# Check simple ordination again to see if decontam worked

EX_cleaner.ord <- ordinate(EX_ps_NC_batch_clean, method ="PCoA", "jaccard", binary = TRUE) 
 
plot_ordination(EX_ps_NC_batch_clean, EX_cleaner.ord, type="samples", color="Fbatch", title="Prelim 12S Jaccard Binary Ordination Post-Decontam")
```


# Remove Unwanted Taxa with Dplyr


```{r remove_taxa, echo = FALSE}
EX_ps_clean <- EX_ps_NC_batch_clean %>% 
  subset_taxa(Kingdom != "Bacteria" & Genus != "Homo") 

```

```{r clean_columns, echo = FALSE}
EX_ps_clean <- prune_taxa(taxa_sums(EX_ps_clean) > 0, EX_ps_clean) 
EX_ps_clean <- prune_samples(sample_sums(EX_ps_clean) > 0, EX_ps_clean)

EX_ps_clean
saveRDS(EX_ps_clean, 'C:/Users/bydav/Desktop/12SPrelim_EX_ps_clean.RDS')
```


# Taxonomic Information and Maps

```{r unique, echo = FALSE}
taxa_abundance_bars(
taxa_filter(EX_ps_clean.rar, frequency = 0.8),
classification = 'Phylum', treatment = c('Treatment'),
subset = c('Pre', 'Post'), transformation = 'none')

taxa_abundance_bars(
unique_taxa(EX_ps_clean, c('Week', 'Sheep_ID')),
classification = 'Phylum', transformation = 'none')
```

```{r heatmap, echo = FALSE}



EX_ps_clean.rar.glom = tax_glom(EX_ps_clean.rar, "Family")

plot_heatmap(EX_ps_clean.rar, fill="Family") + facet_grid(Diet~Week, space="free", scales="free") + theme(legend.position = "bottom") + theme(axis.text.x = element_blank())

plot_heatmap(EX_ps_clean.rar, fill="Family") + theme(legend.position = "bottom") + theme(axis.text.x = element_blank())

taxabund0 <- abundance_heatmap(EX_ps_clean.rar, classification = 'Family', treatment = c('Treatment'), subset = 'Pre')

taxabund1 <- abundance_heatmap(EX_ps_clean.rar, classification = 'Phylum', treatment = c('Week'), subset = '1')

taxabund2 <- abundance_heatmap(EX_ps_clean.rar, classification = 'Phylum', treatment = c('Week'), subset = '2')

taxabund0
taxabund1
taxabund2

ggarrange(taxabund0, taxabund1, taxabund2, nrow = 2, ncol = 2, labels = NULL, common.legend = TRUE)
```

```{r 100bar, echo = FALSE}
## Family

# All Taxa
EX_ps_clean.rar.stacked = transform_sample_counts(EX_ps_clean.rar, function(x) x / sum(x) )
plot_bar(EX_ps_clean.rar.stacked, fill="Family")

# Top 50 by abundance only
physeqhigh = prune_taxa(taxa_names(EX_ps_clean.rar)[1:50], EX_ps_clean.rar)
physeqhighbars = transform_sample_counts(physeqhigh, function(x) x / sum(x) )
plot_bar(physeqhighbars, fill="Family")

# Lowest 50 by abundance only
physeqlow = prune_taxa(taxa_names(EX_ps_clean.rar)[60:110], EX_ps_clean.rar)
physeqlowbars = transform_sample_counts(physeqlow, function(x) x / sum(x) )
plot_bar(physeqlowbars, fill="Family")

prelimLWA <- subset_samples(EX_ps_clean.rar, TreatType != "")

# Top 20 by prelim LWA treatment
topprelimLWA <- prune_taxa(taxa_names(prelimLWA)[1:20], prelimLWA)
topprelimLWAbars = transform_sample_counts(topprelimLWA, function(x) x / sum(x) )

# Facet grid (abundance)
plot_bar(topprelimLWAbars, "Family", fill="Family", facet_grid=~TreatType) + theme(legend.position = "bottom") + theme(axis.title.x = element_blank()) + theme(axis.ticks = element_blank()) + theme(axis.text.x = element_blank())

# Stacked plots (species proportions)
plot_bar(topprelimLWAbars, fill="Family") + theme(legend.position = "right") + theme(axis.title.x = element_blank()) + theme(axis.ticks = element_blank())

## 
# Low 20 by prelim LWA treatment
lowprelimLWA <- prune_taxa(taxa_names(prelimLWA)[90:110], prelimLWA)
lowprelimLWAbars = transform_sample_counts(lowprelimLWA, function(x) x / sum(x) )


# Facet grid (abundance)
plot_bar(lowprelimLWAbars, "Family", fill="Family", facet_grid=~TreatType) + theme(legend.position = "bottom") + theme(axis.title.x = element_blank()) + theme(axis.ticks = element_blank()) + theme(axis.text.x = element_blank())

# Stacked plots (species proportions)
plot_bar(lowprelimLWAbars, fill="Family") + theme(legend.position = "right") + theme(axis.title.x = element_blank()) + theme(axis.ticks = element_blank())


```


```{r taxa_dist}
get_taxa_unique(EX_ps_clean.rar, taxonomic.rank="Family", errorIfNULL=TRUE)

proportionstreat <- taxa_proportions(EX_ps_clean.rar, 'Phylum', treatment = c('Treatment'))

ggplot(proportions, aes(x = Treatment, y = Proportion, fill = Phylum)) + geom_col(position = "fill") + theme(axis.text.x = element_text(angle = 60, hjust = 1))

proportionslamb <- taxa_proportions(EX_ps_clean.rar, 'Phylum', treatment = c('Sheep_ID'))

ggplot(proportionslamb, aes(x = Sheep_ID, y = Proportion, fill = Phylum)) + geom_col(position = "fill") + theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

