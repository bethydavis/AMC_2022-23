---
title: "AMCSeqAnalysis"
author: "BYDavis"
date: "2024-02-29"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Markdown file to analyze AMC water eDNA samples, using Dr. Sue Ishaq's DNA Sequencing Analysis workflow (to start). 

Debugging statements are preceded with ###

# Install and load relevant packages
```{r packageinstall}
if(!require("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install("dada2")
install.packages("ggplot2")
install.packages("beepr")

```

```{r loadpackages}
library(dada2)
library(ggplot2)
library(beepr)
```

Prior to this Rmd, the raw fastq files have been renamed in the repo UM_FSM_Cleaning

# Set folder paths and file types
```{r raw_paths, echo = FALSE}
# For the 12S samples:

# Set the file paths for the raw fastqs
path_raw12F <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Raw/12S/Read1"
path_raw12R <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Raw/12S/Read2"

# Specify file name formats
fns12F <- list.files(path_raw12F)
fastqs12F <- fns12F[grepl('.gz$', fns12F)]

fns12R <- list.files(path_raw12R)
fastqs12R <- fns12R[grepl('.gz$', fns12R)]

###  print(fastqs12F)

# For the BF2 samples:

# Set the file paths for the raw fastqs
path_rawBF <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Raw/BF2BR2/Read1"
path_rawBR <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Raw/BF2BR2/Read2"

# Specify file name formats
fnsBF <- list.files(path_rawBF)
fastqsBF <- fnsBF[grepl('.gz$', fnsBF)]

fnsBR <- list.files(path_rawBR)
fastqsBR <- fnsBR[grepl('.gz$', fnsBR)]

# The file paths still include the desktop.ini file, so let's specify a file path to lead future code only to the fastq files in the folder

fns12Fisolate <- file.path(path_raw12F, fastqs12F)
fns12Risolate <- file.path(path_raw12R, fastqs12R)

fnsBFisolate <- file.path(path_rawBF, fastqsBF)
fnsBRisolate <- file.path(path_rawBR, fastqsBR)

# Set sample names to a vector
# Remove path and the .gz extension
names12Ffast <- tools::file_path_sans_ext(basename(fastqs12F))
# Repeat file_path_sans_ext to also remove the .fastq
names12F <- tools::file_path_sans_ext(names12Ffast)

names12Rfast <- tools::file_path_sans_ext(basename(fastqs12R))
names12R <- tools::file_path_sans_ext(names12Rfast)

namesBFfast <- tools::file_path_sans_ext(basename(fastqsBF))
namesBF <- tools::file_path_sans_ext(namesBFfast)

namesBRfast <- tools::file_path_sans_ext(basename(fastqsBR))
# Repeat file_path_sans_ext to also remove the .fastq
namesBR <- tools::file_path_sans_ext(namesBRfast)

# Set location to put output images and graphs
path_outputs <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs"
```

The sample names are already set how I want them, so I won't use/adapt Sue's file name cleaning chunk (Clean File Names for Initial Load)

# Check Sequence Quality

Run 4 randomly selected quality profile plots per primer per read type (forward or reverse). Number will correspond to the file order. Min is set to 2 to avoid the desktop.ini file that GoogleDrive desktop creates from being chosen

```{r random_selection}

# Numbers for 12S Forward reads:
sample(1:78, 6, replace = FALSE)
  # Results on 3/20/2024: 75 15 63 34 35 28

# Numbers for 12S Reverse reads:
sample(1:78, 6, replace = FALSE)
  # Results on 3/20/2024: 70 40 69 38  5 10

# Numbers for BF2 Forward reads:
sample(1:78, 6, replace = FALSE)
  # Results on 3/20/2024: 48 64 20 57 15 11

# Numbers for BF2 Reverse reads:
sample(1:78, 6, replace = FALSE)
  # Results on 3/20/2024: 15, 49, 56, 78, 11, 28

```


```{r qualityplotgeneration}
# 12S Forward:
plotQualityProfile(fns12Fisolate[75])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ForwardQuality_RNG75.png", plot = plotQualityProfile(fns12Fisolate[75]))

plotQualityProfile(fns12Fisolate[15])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ForwardQuality_RNG15.png", plot = plotQualityProfile(fns12Fisolate[15]))

plotQualityProfile(fns12Fisolate[63])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ForwardQuality_RNG63.png", plot = plotQualityProfile(fns12Fisolate[63]))

plotQualityProfile(fns12Fisolate[34])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ForwardQuality_RNG34.png", plot = plotQualityProfile(fns12Fisolate[34]))

plotQualityProfile(fns12Fisolate[35])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ForwardQuality_RNG35.png", plot = plotQualityProfile(fns12Fisolate[35]))

plotQualityProfile(fns12Fisolate[28])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ForwardQuality_RNG28.png", plot = plotQualityProfile(fns12Fisolate[28]))

beep(sound = "fanfare")

```

```{r}
# 12S Reverse:
plotQualityProfile(fns12Risolate[70])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ReverseQuality_RNG70.png", plot = plotQualityProfile(fns12Risolate[70]))

plotQualityProfile(fns12Risolate[40])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ReverseQuality_RNG40.png", plot = plotQualityProfile(fns12Risolate[40]))

plotQualityProfile(fns12Risolate[69])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ReverseQuality_RNG69.png", plot = plotQualityProfile(fns12Risolate[69]))

plotQualityProfile(fns12Risolate[38])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ReverseQuality_RNG38.png", plot = plotQualityProfile(fns12Risolate[38]))

plotQualityProfile(fns12Risolate[5])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ReverseQuality_RNG5.png", plot = plotQualityProfile(fns12Risolate[5]))

plotQualityProfile(fns12Risolate[10])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12ReverseQuality_RNG10.png", plot = plotQualityProfile(fns12Risolate[10]))

beep(sound = "fanfare")

```

```{r}
# B Forward:
plotQualityProfile(fnsBFisolate[48])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ForwardQuality_RNG48.png", plot = plotQualityProfile(fnsBFisolate[48]))

plotQualityProfile(fnsBFisolate[64]) 
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ForwardQuality_RNG64.png", plot = plotQualityProfile(fnsBFisolate[64]))

plotQualityProfile(fnsBFisolate[20])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ForwardQuality_RNG20.png", plot = plotQualityProfile(fnsBFisolate[20]))

plotQualityProfile(fnsBFisolate[57])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ForwardQuality_RNG57.png", plot = plotQualityProfile(fnsBFisolate[57]))

plotQualityProfile(fnsBFisolate[15])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ForwardQuality_RNG15.png", plot = plotQualityProfile(fnsBFisolate[15]))

plotQualityProfile(fnsBFisolate[11])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ForwardQuality_RNG11.png", plot = plotQualityProfile(fnsBFisolate[11]))

beep(sound = "fanfare")

```

```{r}
# B Reverse:
plotQualityProfile(fnsBRisolate[15])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ReverseQuality_RNG15.png", plot = plotQualityProfile(fnsBRisolate[15]))

plotQualityProfile(fnsBRisolate[49]) 
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ReverseQuality_RNG49.png", plot = plotQualityProfile(fnsBRisolate[49]))

plotQualityProfile(fnsBRisolate[56])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ReverseQuality_RNG56.png", plot = plotQualityProfile(fnsBRisolate[56]))

plotQualityProfile(fnsBRisolate[77])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ReverseQuality_RNG77.png", plot = plotQualityProfile(fnsBRisolate[77]))

plotQualityProfile(fnsBRisolate[11])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ReverseQuality_RNG11.png", plot = plotQualityProfile(fnsBRisolate[11]))

plotQualityProfile(fnsBRisolate[28])
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF2ReverseQuality_RNG28.png", plot = plotQualityProfile(fnsBRisolate[28]))

beep(sound = "fanfare")
```

The indexes and adapters have been removed from the sequence files, but the original primers are still attached. Need to account for their presence in the trim amounts and remove them.

12S Forward primer: 21 - GTCGGTAAAACTCGTGCCAGC

12S Reverse primer: 27 - CATAGTGGGGTATCTAATCCCAGTTTG

BF2 primer length: 20 -  GCHCCHGAYATRGCHTTYCC 

BR2 primer length: 20 -  TCDGGRTGNCCRAARAAYCA 


Quality notes:

12 Forward: cut after 220

12 Reverse: cut after 220

BF2 Forward: cut after 240, potentially from 0-50 also?

BR2 Reverse: these are a hot mess. Run more randoms. Post-randoms - there's a consistent bad low quality spike around 50-60. Trim from 60-200

# Have not localized code from here on

# Begin filtering process

First set directories for a new set of folders - the locations that the filtered files will go into

```{r filterlocale}
path_filt12F <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Filtered/12S Forward"
path_filt12R <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Filtered/12S Reverse"

path_filtBF <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Filtered/BF Forward"
path_filtBR <- "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Filtered/BF Reverse"
```


Going to do this one at a time cause I'm scared

Running the reads separate even when I do have pairs because the BF2/BR2 has a mismatch in number of files currently and I'd rather do them all the same. From my understanding, this may let some lower quality reads through since it'll allow individual reads pass when it wouldn't pass as a pair, but I figure the trim settings will already take care of most of the issues, and there are further cleaning steps too.

```{r filt_12F}
filtout12F <- filterAndTrim(file.path(path_raw12F, fastqs12F), file.path(path_filt12F, paste0(names12F, "filt.fastq.gz")), trimLeft = 21, truncLen = 220, maxEE = 2, maxN = 0, verbose = TRUE)

saveRDS(filtout12F, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12F_FilteredOutputs.rds")
```
AMC23_C721_MR1 did not have any reads left after the filter, file was not written. Image saved in output folder

```{r filt_12R}
filtout12R <- filterAndTrim(file.path(path_raw12R, fastqs12R), file.path(path_filt12R, paste0(names12R, "filt.fastq.gz")), trimLeft = 27, truncLen = 220, maxEE = 2, maxN = 0, verbose = TRUE)

saveRDS(filtout12R, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12R_FilteredOutputs.rds")
```
AMC23_C721_MR2 removed

```{r filt_BF}
filtoutBF <- filterAndTrim(file.path(path_rawBF, fastqsBF), file.path(path_filtBF, paste0(namesBF, "filt.fastq.gz")), trimLeft = 50, truncLen = 240, maxEE = 2, maxN = 0, verbose = TRUE)

saveRDS(filtoutBF, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF_FilteredOutputs.rds")
```
AMC22_AB01_BR1 removed
AMC23_AB04_BR1 removed
AMC23_MB09_BR1 removed


```{r filt_BR}
filtoutBR <- filterAndTrim(file.path(path_rawBR, fastqsBR), file.path(path_filtBR, paste0(namesBR, "filt.fastq.gz")), trimLeft = 60, truncLen = 200, maxEE = 2, maxN = 0, verbose = TRUE)

saveRDS(filtoutBR, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BR_FilteredOutputs.rds")
```
AMC22_AB01_BR2 removed
AMC23_AB04_BR2 removed
AMC23_MB09_BR2 removed

### Load filt outputs back in if needed:
```{r}
filtout12F <- readRDS("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12F_FilteredOutputs.rds")

filtout12R <- readRDS("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12R_FilteredOutputs.rds")

filtoutBF <- readRDS("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF_FilteredOutputs.rds")

filtoutBR <- readRDS("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BR_FilteredOutputs.rds")
```

# Explore filtered output 

Can look at the dimensions of each RDS output, and order by the filtered read amount, and/or look at the number of total raw reads compared to the filtered out reads

```{r assess filt_12F}
filtout12F
dim(filtout12F)

# Order by filtered read amount
filtout12F[order(filtout12F[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtout12F)
```


```{r assess filt_12R}
filtout12R
dim(filtout12R)

# Order by filtered read amount
filtout12R[order(filtout12R[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtout12R)
```


```{r assess filt_BF}
filtoutBF
dim(filtoutBF)

# Order by filtered read amount
filtoutBF[order(filtoutBF[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtoutBF)
```


```{r assess filt_BR}
filtoutBR
dim(filtoutBR)

# Order by filtered read amount
filtoutBR[order(filtoutBR[,2], decreasing=FALSE),]

# Compare total raw in and filtered out read amounts
colSums(filtoutBR)
```

# Look at filtered trends
```{r filtertrends}
filttrend12F <- ggplot(as.data.frame(filtout12F)) + geom_point(aes(row.names(filtout12F), reads.in), color = "blue") + geom_point(aes(row.names(filtout12F), reads.out), color = "orange") + ggtitle("Filter Trends for AMC 12S Forward Reads")

filttrend12R <- ggplot(as.data.frame(filtout12R)) + geom_point(aes(row.names(filtout12R), reads.in), color = "blue") + geom_point(aes(row.names(filtout12R), reads.out), color = "orange") + ggtitle("Filter Trends for AMC 12S Reverse Reads")

filttrendBF <- ggplot(as.data.frame(filtoutBF)) + geom_point(aes(row.names(filtoutBF), reads.in), color = "blue") + geom_point(aes(row.names(filtoutBF), reads.out), color = "orange") + ggtitle("Filter Trends for AMC BF2/BR2 Forward Reads")

filttrendBR <- ggplot(as.data.frame(filtoutBR)) + geom_point(aes(row.names(filtoutBR), reads.in), color = "blue") + geom_point(aes(row.names(filtoutBR), reads.out), color = "orange") + ggtitle("Filter Trends for AMC BF2/BR2 Reverse Reads")


ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/FilterTrends_12F.png", filttrend12F)
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/FilterTrends_12R.png", filttrend12R)
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/FilterTrends_BF.png", filttrendBF)
ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/FilterTrends_BR.png", filttrendBR)
```

just out of curiosity's sake, let's also explore some quality profiles again from the filtered reads

```{r plotfiltered}

#Compare post and pre-filter and trim

plotQualityProfile("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Filtered/12S Reverse/AMC22_MB01_MR2filt.fastq.gz")

plotQualityProfile("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Raw/12S/Read2/AMC22_MB01_MR2.fastq.gz")

# 

plotQualityProfile("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Filtered/12S Reverse/AMC23_SU03_MR2filt.fastq.gz")

plotQualityProfile("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/Raw/12S/Read2/AMC23_SU03_MR2.fastq.gz")
```

# Error Rates and Dereplication

First we need to set a seed and make sure the paths are all still set.

```{r error setup}
set.seed(0743)

# Set the names for the filtered files we'll be using

# create a list of files in the path
filtnames12Finter <- list.files(path_filt12F, full.names = TRUE)

# Specify I only want the files with the .gz extension
filtnames12F <- filtnames12Finter[grepl('.gz$', filtnames12Finter)]

# Extract just the file name, not the path, and remove the .gz extension. This leaves on the .fastq extension
fastqfilt12F <- tools::file_path_sans_ext(basename(filtnames12F))

# Remove the .fastq extension
names12Ffilt <- tools::file_path_sans_ext(basename(fastqfilt12F))



# Repeat for the other folders

# 12Reverse
filtnames12Rinter <- list.files(path_filt12R, full.names = TRUE)
filtnames12R <- filtnames12Rinter[grepl('.gz$', filtnames12Rinter)]
fastqfilt12R <- tools::file_path_sans_ext(basename(filtnames12R))
names12Rfilt <- tools::file_path_sans_ext(basename(fastqfilt12R))

# BForward
filtnamesBFinter <- list.files(path_filtBF, full.names = TRUE)
filtnamesBF <- filtnamesBFinter[grepl('.gz$', filtnamesBFinter)]
fastqfiltBF <- tools::file_path_sans_ext(basename(filtnamesBF))
namesBFfilt <- tools::file_path_sans_ext(basename(fastqfiltBF))

# BReverse
filtnamesBRinter <- list.files(path_filtBR, full.names = TRUE)
filtnamesBR <- filtnamesBRinter[grepl('.gz$', filtnamesBRinter)]
fastqfiltBR <- tools::file_path_sans_ext(basename(filtnamesBR))
namesBRfilt <- tools::file_path_sans_ext(basename(fastqfiltBR))
```

## Dereplication

This step comes from the dada2 workflow (https://bioconductor.org/packages/devel/bioc/vignettes/dada2/inst/doc/dada2-intro.html#learn-the-error-rates), not Sue's code

```{r derep}
derep12F <- derepFastq(filtnames12F, verbose=TRUE)
derep12R <- derepFastq(filtnames12R, verbose=TRUE)
derepBF <- derepFastq(filtnamesBF, verbose = TRUE)
derepBR <- derepFastq(filtnamesBR, verbose = TRUE)

```

Now that the names are set up, we can run the simulations to learn and estimate the number of errors in each filtered folder. This step is in Sue's code (and is the source of the saveRDS and plotErrors lines), but using the default parameters from the dada2 workflow

## Error Rates

```{r errorrates}
err12F <- learnErrors(derep12F, multithread = FALSE, randomize = TRUE)
# The default nbases is 1e8. Using this default since Sue's 1e6 parameter filled up after only one sample

saveRDS(err12F, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Forward_Error.rds")

ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Forward_ErrorPlot.png", plotErrors(err12F, nominalQ = TRUE)) 



err12R <- learnErrors(derep12R, multithread = FALSE, randomize = TRUE)

saveRDS(err12R, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Reverse_Error.rds")

ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Reverse_ErrorPlot.png", plotErrors(err12R, nominalQ = TRUE)) 



errBF <- learnErrors(derepBF, multithread = FALSE, randomize = TRUE)

saveRDS(errBF, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BForward_Error.rds")

ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BForward_ErrorPlot.png", plotErrors(errBF, nominalQ = TRUE)) 



errBR <- learnErrors(derepBR, multithread = FALSE, randomize = TRUE)

saveRDS(errBR, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BReverse_Error.rds")

ggsave("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BReverse_ErrorPlot.png", plotErrors(errBR, nominalQ = TRUE)) 

# This takes a long time. If you have to do it again, include this:
beep(sound = "fanfare")
```

### Load error rates back in if needed:
```{r}
err12F <- readRDS("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Forward_Error.rds")

err12R <- readRDS("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Reverse_Error.rds")

errBF <- readRDS("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BForward_Error.rds")

errBR <- readRDS("C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BReverse_Error.rds")
```

# Infer sample composition
```{r samplecomposition}
dada12F <- dada(derep12F, err = err12F, multithread = FALSE)
print("dada 12F finished")
beep(sound = "coin")

dada12R <- dada(derep12R, err = err12R, multithread = FALSE)
print("dada 12R finished")
beep(sound = "coin")

dadaBF <- dada(derepBF, err = errBF, multithread = FALSE)
print("dada BF finished")
beep(sound = "coin")

dadaBR <- dada(derepBR, err = errBR, multithread = FALSE)
print("dada BR finished")
beep(sound = "fanfare")

# Save all in case I need to reload the dada objects
saveRDS(dada12F, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Forward_SampleComp.rds")
saveRDS(dada12R, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Reverse_SampleComp.rds")
saveRDS(dadaBF, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BForward_SampleComp.rds")
saveRDS(dadaBR, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BReverse_SampleComp.rds")
```


Merge paired reads

* Apparently if you filter and trim separately, you cannot merge after that point, and must filterandTrim the forward and reverse reads together. (source: the DADA2 pipeline tutorial)
Restarting with that in mind
```{r}
mergers12 <- mergePairs(dada12F, path_filt12F, dada12R, path_filt12R, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers12[[1]])

mergersB <- mergePairs(dadaBF, derepBF, dadaBR, derepBR, verbose = TRUE)
head(mergersB[[1]])


saveRDS(mergers12, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12MergedSampleComp.rds")
saveRDS(mergersB, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BMergedSampleComp.rds")
```

Now make a sequence table (A for unmerged, B for merged)

A - Unmerged:

```{r seqtabA}
seqtab12F <- makeSequenceTable(dada12F)
dim(seqtab12F)
saveRDS(seqtab12F, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12F Sequence Table.rds")

seqtab12R <- makeSequenceTable(dada12R)
dim(seqtab12R)
saveRDS(seqtab12R, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12R Sequence Table.rds")

seqtabBF <- makeSequenceTable(dadaBF)
dim(seqtabBF)
saveRDS(seqtabBF, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF Sequence Table.rds")

seqtabBR <- makeSequenceTable(dadaBR)
dim(seqtabBR)
saveRDS(seqtabBR, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BR Sequence Table.rds")

```

# Remove Chimeras A
```{r nochimA}
seqtab12F.nochim <- removeBimeraDenovo(seqtab12F, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtab12F.nochim)
saveRDS(seqtab12F.nochim, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12F Sequence No Chimera Table.rds")

seqtab12R.nochim <- removeBimeraDenovo(seqtab12R, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtab12R.nochim)
saveRDS(seqtab12R.nochim, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12R Sequence No Chimera Table.rds")

seqtabBF.nochim <- removeBimeraDenovo(seqtabBF, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtabBF.nochim)
saveRDS(seqtabBF.nochim, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BF Sequence No Chimera Table.rds")

seqtabBR.nochim <- removeBimeraDenovo(seqtabBR, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtabBR.nochim)
saveRDS(seqtabBR.nochim, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BR Sequence No Chimera Table.rds")

beep(sound = "coin")
```

B - Merged:

```{r seqtabB}
seqtab12 <- makeSequenceTable(mergers12)
dim(seqtab12)
saveRDS(seqtab12, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Merged Sequence Table.rds")

seqtabB <- makeSequenceTable(mergersB)
dim(seqtabB)
saveRDS(seqtabB, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BMerged Sequence Table.rds")
```

# Remove Chimeras B
```{r nochimB}
seqtab12.nochim <- removeBimeraDenovo(seqtab12, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtab12.nochim)
saveRDS(seqtab12.nochim, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/12Merged Sequence No Chimera Table.rds")

seqtabB.nochim <- removeBimeraDenovo(seqtabB, method = "consensus", multithread = FALSE, verbose = TRUE)
dim(seqtabB.nochim)
saveRDS(seqtabB.nochim, "C:/Users/bydav/My Drive/2_UMaine FSM - Field Projects/AMC/Data/dataoutputs/BMerged Sequence No Chimera Table.rds")

beep(sound = "coin")
```


# Next

The process from here on is to download a reference sequence library in .fa.gz format, then use phyloseq to assignTaxonomy to the entire seqtab.nochim objects. Then identify which species are present in the negative controls, and subtract them from the samples (either all samples or batches). This is the decontam process in DNASeqAnalysis_3.rmd. We will subtract the control species from batches of samples, based on which controls were collected, filtered, and extracted with which 'real samples.' Refer to the field and lab notebooks to generate this list. This will probably get messy and might even cross projects. 


[] download reference libraries for the 12S primers and BF2/BR2 COI primers
[] AssignTaxonomy
[] Proceed with the decontam process
[] Remove any unwanted taxa
[] Continue with rarefaction analysis and then the rest of the diversity and abundance tests

# ################################




# Workflow Verification

```{r workflow_verification, echo = FALSE}
# Make sure number of samples match
dim(filtoutput)
dim(seqtab.nochim)
#dim(meta)
```

```{r table_binding, echo = FALSE}
# Combine filtered output, treatment factor, and no.chim information to a new variable
track <- cbind(filtoutput, rowSums(seqtab.nochim), meta$Sample_type) 
# CHANGE "Sample_type" to be the column name in your metadata file of the groups in your data

colnames(track) <- c("reads.in","filtered", "nonchimeras", "Sample_type") 
rownames(track) <- rownames(meta)
head(track)

# save when happy with it
saveRDS(track, 'C:/Users/bydav/Desktop/AVS 554/Output/Lab4_trackedseqs.rds') 
```

```{r plot_qa, echo = FALSE}
# Display options:

# plot of reads along QC workflow
plotData <- as.data.frame(track) %>% gather(type,totals, reads.in, filtered, nonchimeras)

# plot sample_type along the x axis
qc2 <- ggplot(plotData,aes(x=Sample_type,y=as.numeric(totals))) + geom_point(aes(color=type)) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + ylab("Sequences") + xlab("Sample_type")

# plot QA stage along x axis
qc3 <- ggplot(plotData,aes(x=type,y=as.numeric(totals))) + geom_point(aes(color=Sample_type)) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + ylab("Sequences") + xlab("QA stage")

qctitle <- "Forward Reads QC"
qctgrob <- text_grob(qctitle,size = 10)
# Draw the text
qctit <- as_ggplot(qctgrob) + theme(plot.margin = margin(0,1,0,0, "cm"))

ggarrange(qctit, NULL, qc2, qc3, nrow = 2, ncol = 2, labels = NULL, common.legend = TRUE, heights = c(1,5))
```


# Assign Taxonomy


Be sure to have a taxonomy database file downloaded from here: https://benjjneb.github.io/dada2/training.html.

The Silva file shown below is used for 16S rRNA (prokaryotes) and nicely formatted versions can be downloaded from the DADA2 website, which also contains some (18S) eukaryotic versions. 

```{r assign_taxonomy, echo = FALSE}
# Very slow and intensive
all.taxa <- assignTaxonomy(seqtab.nochim, 'C:/Users/bydav/Desktop/AVS 554/prov_silva_nr99_v138_train_set.fa.gz', tryRC = TRUE)


seqs <- getSequences(system.file("extdata", "example_seqs.fa", package="dada2"))
training_fasta <- system.file("extdata", "example_train_set.fa.gz", package="dada2")
taxa <- assignTaxonomy(seqs, training_fasta)
taxa80 <- assignTaxonomy(seqs, training_fasta, minBoot=80, multithread=2)

```

```{r taxonomy_results, echo = FALSE}
saveRDS(all.taxa, 'C:/Users/bydav/Desktop/AVS 554/Output/Lab4_taxa_silva_rds.rds')
write.csv(all.taxa, 'C:/Users/bydav/Desktop/AVS 554/Output/Lab4_taxa_silva_csv.csv')
```

# Phyloseq

Check that sample names match, if not change manually - don't rely on code to fix it
```{r name_check, include = FALSE}
row.names(meta) 
row.names(seqtab.nochim)

# Convince R by force that they match - this is a flat replacement, not a match-up
row.names(seqtab.nochim) <- row.names(meta) 
```

```{r ps_object, echo = FALSE}
# Create phyloseq object with all samples
EX_ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), sample_data(meta), tax_table(all.taxa.sp))

EX_ps
# Need to add a column that defines if a sample is a Sample or Control, and a column that lists which batch number (for decontam) each sample belongs to
```
# RESULT:
34 samples made it this far

######################################################
# Alpha Diversity Teaser
######################################################

```{r adiversity_preview, echo = FALSE}
plot_richness(EX_ps, x="Diet", measures=c("Observed", "Shannon", "Chao1", "Simpson"), color="Treatment") + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + title("Alpha Diversity Preview by Treatment")
```
Notes:

- Hopefully, diversity/richness is lower in the Negative controls than real samples
- 'measures' are some of the alpha diversity measures you can use
- You can change the color to any color of your choice

# RESULT: 
In both the Observed and Chao1 metrics, the negative controls and mock sample treatment labels reflect a consistent (clustered) low diversity compared to the other treatments, though the experimental treatments have quite varied (high spread) alpha diversity for this pre-richness peek. Shannon diversity metrics report a middling diversity for the negative controls,  however the Simpson metric reports a high negative control diversity (comparatively) around 0.95, with experimental treatment datapoints ranging from 0.21 to 1 for Pelleted Alfalfa and 0.31 to 1 for Loose Alfalfa

Assess negative control richness compared to real samples

```{r taxa_sums, echo = FALSE}
plot(sort(taxa_sums(EX_ps), TRUE), type="h", ylim=c(0, 20))
```
# ISSUE - How to interpret this graph?
This shows how populated each taxa group is

```{r simple_ordination, echo = FALSE}
EX.ord <- ordinate(EX_ps, 
                   #calculate similarities
                   method ="PCoA", 
                   #ordination type 
                   "bray", binary = FALSE) 
#similarity type. Jaccard is binary, Bray can be binary (unweighted) or not (weighted)

plot_ordination(EX_ps, EX.ord, type="samples", color="Treatment", title="Bray Non-Binary PCoA Simple Ordination")
```
Initial clustering by extraction/sequencing batch or confounding variables implies contamination issues (see next section). Horse-shoe patterns indicate underlying patterns to the data

# RESULT: 

Jaccard - No obvious clustering is seen for the negative, pelleted, or loose alfalfa overall, though the week-based results of pelleted alfalfa wk 2 and loose alfalfa week 1 are closer together than the other treatments. Slight horseshoe curve possible. Negative control outliers

Bray Binary - More obvious clustering of pelleted alfalfa wk 1 and 2 data and a clearer, more curved horseshoe shape. Negative control outliers

Bray Non-Binary - strong horseshow pattern, negative control outliers but much less clustering of data

######################################################
# Decontam with Negative Controls (Ishaq Method)
######################################################

Dr. Ishaq's method creates vectors out of the SV table data for negative controls, and subtracts those SVs from the sample data. Depending on the type of negative control, these are removed from the whole data set or from subsets of batches. Remove PCR and sampling materials negative control SVs fully from all samples, and remove extraction kit SVs fully from each dna_extraction_batch, respectively.

```{r decontam_full, echo = FALSE}


# Subset controls
EX_ps_controls = subset_samples(EX_ps, Sample_type == "NegCon_swab" | Sample_type == "NegCon_PCR" | Sample_type == "Mock") 
# CHANGE ME to the column name that holds the variables associated with being a negative control

EX_ps_controls <- prune_taxa(taxa_sums(EX_ps_controls) > 0, EX_ps_controls)

# To view columns in a phyloseq object:
sample_data(EX_ps)$Treatment

# subset each DNA extraction batch
batch1 = subset_samples(EX_ps_NCclean, DNA_extraction_batch == "1") #23 samples 
batch2 = subset_samples(EX_ps_NCclean, DNA_extraction_batch == "2") #22 samples

# subset controls and prune to only those taxa
batch1_kit = subset_samples(batch1, Sample_type == "NegCon_kit")
batch2_kit = subset_samples(batch2, Sample_type == "NegCon_kit")

batch1_kit <- prune_taxa(taxa_sums(batch1_kit) > 0, batch1_kit)
batch2_kit <- prune_taxa(taxa_sums(batch2_kit) > 0, batch2_kit)

# Make the taxa names into a vector so you can remove them, then use the keep vector for the prune taxa argument, because it wants the argument to be true (matching), and repeat for both batches
b1_control_vec <- as.vector(taxa_names(batch1_kit)) 
b1_vec <- as.vector(taxa_names(batch1)) 
b1_keep <- setdiff(b1_vec, b1_control_vec)
b1_clean <- prune_taxa(b1_keep, batch1)

b2_control_vec <- as.vector(taxa_names(batch2_kit)) 
b2_vec <- as.vector(taxa_names(batch2)) 
b2_keep <- setdiff(b2_vec, b2_control_vec)
b2_clean <- prune_taxa(b2_keep, batch2)

# Merge the phyloseq objects back together, then remove any blank taxa or samples
EX_ps_NC_batch_clean <- merge_phyloseq(b1_clean, b2_clean) 

# Clean out taxa/SV columns that are no longer present
EX_ps_NC_batch_clean <- prune_taxa(taxa_sums(EX_ps_NC_batch_clean) > 0, EX_ps_NC_batch_clean) 

EX_ps_NC_batch_clean <- prune_samples(sample_sums(EX_ps_NC_batch_clean) > 0, EX_ps_NC_batch_clean)
```

# NOTE - Removed Mock here

```{r recheck_ordination, echo = FALSE}
# Check simple ordination again to see if decontam worked

EX_cleaner.ord <- ordinate(EX_ps_NC_batch_clean, method ="PCoA", "jaccard", binary = TRUE) 
 
plot_ordination(EX_ps_NC_batch_clean, EX_cleaner.ord, type="samples", color="Treatment", title="Jaccard Binary Ordination Post-Decontam")
```

# ISSUE: Examine results. Explain what type of negative controls used and did you notice clustering by batch before and after removing contaminants?

######################################################
# Remove Unwanted Taxa from Chloroplast and Mitochondria with Dplyr
######################################################

Dplyer notes:
- '==' it means to keep that, or set as equal to it
- If you have '!=' it means not that (not matching)
- '&' adds more sections

```{r remove_taxa, echo = FALSE}
EX_ps_clean <- EX_ps_NC_batch_clean %>% 
  # CHANGE ME to reflect the phyloseq object name you have 
  subset_taxa(Kingdom == "Bacteria" & Family != "Mitochondria" & Order != "Chloroplast") 
# CHANGE ME to the things you want to remove

EX_ps_clean <- EX_ps_NC_batch_clean %>% 
  # CHANGE ME to reflect the phyloseq object name you have 
  subset_taxa(Kingdom == "Bacteria" & Family != "Mitochondria" & Order != "Chloroplast") 
# CHANGE ME to the things you want to remove
```

```{r clean_columns, echo = FALSE}
EX_ps_clean <- prune_taxa(taxa_sums(EX_ps_clean) > 0, EX_ps_clean) 
EX_ps_clean <- prune_samples(sample_sums(EX_ps_clean) > 0, EX_ps_clean)

EX_ps_clean
saveRDS(EX_ps_clean, 'C:/Users/bydav/Desktop/AVS 554/Output/EX_ps_clean_phyloseq_object.RDS')
```

# ####################

######################################################
# Rarefaction
######################################################

Rarefaction curves are an important precursor to alpha diversity examinations and examine if samples have enough coverage

```{r rarecurve, echo = FALSE}
rc <-as.data.frame(otu_table(EX_ps_clean))
test <- rarecurve(rc, step = 10, cex=0.5, label = TRUE)
```

```{r curve_results, echo = FALSE}
# look at rowsums (total sequences per sample)
rowSums(otu_table(EX_ps_clean))


col <- c("black", "darkred", "forestgreen", "orange", "blue", "yellow", "hotpink")
lty <- c("solid", "dashed", "longdash", "dotdash")

# rerun curve
test <- rarecurve(rc, step = 10, cex=0.5, col = col, lty = lty, label = FALSE)
```

```{r}
raremax <- max(rowSums(otu_table(EX_ps_clean)))
raremax
raremin <- min(rowSums(otu_table(EX_ps_clean)))
raremin
```


# RESULT:
There are 13 samples with fewer than 10,000 SVs in them. The largest number of SVs in a sample is 213655, and the smallest number of SVs in a sample is 3592

The Pelleted Alfalfa (particularly weeks 1 and 2) tended to have a much higher number of SVs than any of the Loose alfalfa lambs

Make a rareified phyloseq object
```{r rar_ps, echo = FALSE}
EX_ps_clean.rar <- rarefy_even_depth(EX_ps_clean, sample.size=5000, 
# CHANGE ME to the SVs/sample you want. 5-10k is a good amount, more is better 
      replace=FALSE, #sampling with or without replacement 
      trimOTUs=TRUE, #remove SVs left empty (called OTUs here but really they are SVs) 
      rngseed=711, verbose=TRUE)

saveRDS(EX_ps_clean.rar, 'C:/Users/bydav/Desktop/AVS 554/Output/EX_ps_clean_rarefied_phyloseq_object.RDS')

write.csv(otu_table(EX_ps_clean.rar), 'C:/Users/bydav/Desktop/AVS 554/Output/EX_ps_clean_rarefied_phyloseq_object.csv')
```
# RESULT: 
sample.size set to 5000

`set.seed(711)` was used to initialize repeatable random subsampling.
Please record this for your records so others can reproduce.
Try `set.seed(711); .Random.seed` for the full vector

6 samples removedbecause they contained fewer reads than `sample.size`.
Up to first five removed samples are: 

LooseAlfalfa_wk0_13LooseAlfalfa_wk1_81PelletedAlfalfa_wk0_29PelletedAlfalfa_wk0_93LooseAlfalfa_wk2_81
...
2699OTUs were removed because they are no longer 
present in any sample after random subsampling

######################################################
# Graphics and Tests
######################################################

Interested in the change in alpha, beta, and gamma diversity and taxonomic hierarchy of rumen microbes (with lambs or diet treatment as ecosystems) over the course of the diet period, separated by diet treatment

Potentially important variables and values:

Diet: PelletedAlfalfa, LooseAlfalfa, Mock (1)

Week: 0, 1, 2


Alpha Diversity - 

# Alpha Diversity Graphics


```{r}
# Clean variable names for neater graphing

sample_data(EX_ps_clean.rar)$Diet <- factor(sample_data(EX_ps_clean.rar)$Diet, levels=c("LooseAlfalfa", "PelletedAlfalfa", "Mock"), labels=c("Loose Alfalfa", "Pelleted Alfalfa", "Negative Control"))

sample_data(EX_ps_clean.rar)$Week <- factor(sample_data(EX_ps_clean.rar)$Week, levels=c("0", "1", "2"), labels=c("Week 0", "Week 1", "Week 2"))
```

```{r base_alpha_graph, echo = FALSE}
richobs <- plot_richness(EX_ps_clean.rar, (aes ="Diet"), title = "Observed Richness", measures= "Observed") + 
geom_violin(trim=TRUE, aes(fill="Diet")) + theme(axis.text.x=element_blank()) + facet_grid(.~Week, space="free") + theme(legend.position = "none") + ylab("Observed Bacterial Richness (SVs)") + xlab("Diet")

richsha <-  plot_richness(EX_ps_clean.rar, (aes ="Diet"), title = "Shannon Richness", measures= "Shannon") + 
geom_violin(trim=TRUE, aes(fill="Diet")) + theme(axis.text.x=element_blank()) + facet_grid(.~Week, space="free") + theme(legend.position = "none") +
  ylab("Observed Bacterial Richness (SVs)") + xlab("Diet")

richchao <-  plot_richness(EX_ps_clean.rar, (aes ="Diet"), title = "Chao1 Richness", measures= "Chao1") + 
geom_violin(trim=TRUE, aes(fill="Diet")) + facet_grid(.~Week, space="free") + theme(legend.position = "none") +
  ylab("Observed Bacterial Richness (SVs)") + xlab("Diet")

richsim <-  plot_richness(EX_ps_clean.rar, (aes ="Diet"), title = "Simpson Richness",
 measures= "Simpson") + 
geom_violin(trim=TRUE, aes(fill="Diet")) + facet_grid(.~Week, space="free") + theme(legend.position = "none") +
  ylab("Observed Bacterial Richness (SVs)") + xlab("Diet")
```

```{r rich_plots, echo = FALSE}
ggarrange(richsha + rremove("xlab"), richobs + rremove("xlab") + rremove("ylab"), richchao, richsim + rremove("ylab"), nrow = 2, ncol = 2, labels = NULL)
```

# Original Spot for Richness and Evenness

# Richness and Evenness Graphs
```{r plot richandeven, echo = FALSE}
everichobst <- ggplot(data=subset(EX_ps_clean.rar.rich.df, !is.na(Week)), aes(x=Week, y=Observed)) + theme_minimal() + geom_point(aes(color=Diet), size = 3) + geom_violin(trim=TRUE, aes(fill=factor(Diet))) + xlab("Week") + ylab("Observed Bacterial Richness (SVs)") + ggtitle("Observed")

everichshat <- ggplot(data=subset(EX_ps_clean.rar.rich.df, !is.na(Week)), aes(x=Week, y=Shannon)) + theme_minimal() + geom_point(aes(color=Diet), size = 3) + geom_violin(trim=TRUE, aes(fill=factor(Diet))) + xlab("Week") + ylab("Observed Bacterial Richness (SVs)") + ggtitle("Shannon")

everichchaot <- ggplot(data=subset(EX_ps_clean.rar.rich.df, !is.na(Week)), aes(x=Week, y=Chao1)) + theme_minimal() + geom_point(aes(color=Diet), size = 3) + geom_violin(trim=TRUE, aes(fill=factor(Diet))) + xlab("Week") + ylab("Observed Bacterial Richness (SVs)") + ggtitle("Chao1") 

everichsimt <- ggplot(data=subset(EX_ps_clean.rar.rich.df, !is.na(Week)), aes(x=Week, y=Simpson)) + theme_minimal() + geom_point(aes(color=Diet), size = 3) + geom_violin(trim=TRUE, aes(fill=factor(Diet))) + xlab("Week") + ylab("Observed Bacterial Richness (SVs)") + ggtitle("Simpson")
```

```{r everich_plots_week, echo = FALSE}
evenweekrichplts <- ggarrange(everichobst + rremove("xlab"), everichshat + rremove("xlab") + rremove("ylab"), everichchaot + rremove("xlab"), everichsimt +
rremove("xlab") +rremove("ylab"), nrow = 2, ncol = 2, labels = NULL, common.legend = TRUE)

evenweekrichplts
```


Histograms of diversity metrics and evenness after conducting richness
```{r evenness, echo = FALSE}

hist(EX_ps_clean.rar.rich.df$Observed)

hist(EX_ps_clean.rar.rich.df$Shannon)
hist(EX_ps_clean.rar.evenSha)

hist(EX_ps_clean.rar.rich.df$Chao1)
hist(EX_ps_clean.rar.evenChao)

hist(EX_ps_clean.rar.rich.df$Simpson)
hist(EX_ps_clean.rar.evenSim)
```

# Kurtosis
```{r kurtosis, echo = FALSE}

kurtosis(EX_ps_clean.rar.rich.df$Observed)
kurtosis(EX_ps_clean.rar.rich.df$Shannon)
kurtosis(EX_ps_clean.rar.rich.df$Chao1)
kurtosis(EX_ps_clean.rar.rich.df$Simpson)
```
# RESULT: 
Kurtosis results:
Observed: 0.8534116
Shannon: 0.548193
Chao1: 2.050403
Simpson: -1.59566

# Shapiro-Wilk Normality Tests
```{r shapiro_metrics, echo = FALSE}
shapiro.test(EX_ps_clean.rar.rich.df$Observed)
shapiro.test(EX_ps_clean.rar.rich.df$Shannon) 
shapiro.test(EX_ps_clean.rar.rich.df$Chao1) 
shapiro.test(EX_ps_clean.rar.rich.df$Simpson) 

shapiro.test(EX_ps_clean.rar.rich.df$EX_ps_clean.rar.even)
```
# RESULT:

Observed: W = 0.90901, p-value = 0.03357 - Not normal
Shannon: W = 0.75841, p-value = 6.624e-05 - Not normal
Chao1: W = 0.83478, p-value = 0.001155 - Not normal
Simpson: W = 0.52829, p-value = 1.074e-07 - Not normal
Evenness: W = 0.62296, p-value = 1.126e-06 - Not normal

A p-value > 0.05 means that the data distribution are not significantly different from being normal


```{r blood, echo = FALSE}
# Convert character variables of interest to numeric and verify
EX_ps_clean.rar.rich.df[,13] <- as.numeric(EX_ps_clean.rar.rich.df[,13])
EX_ps_clean.rar.rich.df[,15] <- as.numeric(EX_ps_clean.rar.rich.df[,15])
EX_ps_clean.rar.rich.df[,22] <- as.numeric(EX_ps_clean.rar.rich.df[,22])
EX_ps_clean.rar.rich.df[,24] <- as.numeric(EX_ps_clean.rar.rich.df[,24])
EX_ps_clean.rar.rich.df[,25] <- as.numeric(EX_ps_clean.rar.rich.df[,25])
EX_ps_clean.rar.rich.df[,30] <- as.numeric(EX_ps_clean.rar.rich.df[,30])
EX_ps_clean.rar.rich.df[,31] <- as.numeric(EX_ps_clean.rar.rich.df[,31])
EX_ps_clean.rar.rich.df[,32] <- as.numeric(EX_ps_clean.rar.rich.df[,32])
EX_ps_clean.rar.rich.df[,33] <- as.numeric(EX_ps_clean.rar.rich.df[,33])
EX_ps_clean.rar.rich.df[,34] <- as.numeric(EX_ps_clean.rar.rich.df[,34])

sapply(EX_ps_clean.rar.rich.df, class)
```

```{r shapiro_blood, echo = FALSE}
shapiro.test(EX_ps_clean.rar.rich.df$ALB)
shapiro.test(EX_ps_clean.rar.rich.df$CA)
shapiro.test(EX_ps_clean.rar.rich.df$PHOS)
shapiro.test(EX_ps_clean.rar.rich.df$NA.)
shapiro.test(EX_ps_clean.rar.rich.df$K)
shapiro.test(EX_ps_clean.rar.rich.df$CL)
shapiro.test(EX_ps_clean.rar.rich.df$MG)
shapiro.test(EX_ps_clean.rar.rich.df$TC02)
shapiro.test(EX_ps_clean.rar.rich.df$Weight_kg)
```

# RESULT:

ALB: W = 0.84649, p-value = 0.001883 - Not Normal
CA: W = 0.97122, p-value = 0.6972 - Normal
PHOS: W = 0.9618, p-value = 0.4756 - Not Normal
NA.: W = 0.75539, p-value = 5.977e-05 - Not Normal
K: W = 0.94152, p-value = 0.1764 - Not Normal
CL: W = 0.80073, p-value = 0.0003028 - Not Normal
MG: W = 0.92611, p-value = 0.07982 - Not Normal
TC02: W = 0.94715, p-value = 0.2348 - Not Normal
Weight_kg: W = 0.95948, p-value = 0.428 - Not Normal

A p-value > 0.05 means that the data distribution are not significantly different from being normal

# Kruskal-Wallis

What are the impacts of the diet, treatment, and week groups on lamb health metrics and the diversity measures?

Groups: Diet, Treatment, Week
Response Variables - Observed, Shannon, Chao1, Simpson, Weight_kg, ALB, CA (with anova), PHOS, NA., K, CL, MG, TC02

```{r anova_calcium, echo = FALSE}
# Calcium was the only normally distributed variable, so use the one-way ANOVA

# Metrics on Calcium by Week test
aovcad <- aov(CA ~ Diet, data = EX_ps_clean.rar.rich.df)
aovcaw <- aov(CA ~ Week, data = EX_ps_clean.rar.rich.df)
aovcat <- aov(CA ~ Treatment, data = EX_ps_clean.rar.rich.df)

summary(aovcad)
summary(aovcaw)
summary(aovcat)


aovcaresults <- matrix(c(AIC(aovcad), AIC(aovcaw), AIC(aovcat)), ncol = 3, byrow = TRUE)

colnames(aovcaresults) <- c('CA ~ Diet', ' CA ~ Week', 'CA ~ Treatment')
aovcaresults

write.table(aovcaresults, file = 'C:/Users/bydav/Desktop/AVS 554/Output/anovaresults.csv')
```
# RESULT: 
No significance
 CA ~ Diet  CA ~ Week CA ~ Treatment
[1,]   10.2749   11.04909       3.479816

```{r kw_tests, echo = FALSE}

# Impact of Diet, Treatment, and Week on the Diversity metrics
kwobsD <- kruskal.test(Observed ~ Diet, data = EX_ps_clean.rar.rich.df) 
kwobsT <- kruskal.test(Observed ~ Treatment, data = EX_ps_clean.rar.rich.df)
kwobsW <- kruskal.test(Observed ~ Week, data = EX_ps_clean.rar.rich.df)

kwshaD <- kruskal.test(Shannon ~ Diet, data = EX_ps_clean.rar.rich.df) 
kwshaT <- kruskal.test(Shannon ~ Treatment, data = EX_ps_clean.rar.rich.df)
kwshaW <- kruskal.test(Shannon ~ Week, data = EX_ps_clean.rar.rich.df)

kwchaD <- kruskal.test(Chao1 ~ Diet, data = EX_ps_clean.rar.rich.df) 
kwchaT <- kruskal.test(Chao1 ~ Treatment, data = EX_ps_clean.rar.rich.df)
kwchaW <- kruskal.test(Chao1 ~ Week, data = EX_ps_clean.rar.rich.df)

kwsimD <- kruskal.test(Simpson ~ Diet, data = EX_ps_clean.rar.rich.df) 
kwsimT <- kruskal.test(Simpson ~ Treatment, data = EX_ps_clean.rar.rich.df)
kwsimW <- kruskal.test(Simpson ~ Week, data = EX_ps_clean.rar.rich.df)

# Impact of Diet, Treatment, and Week on the Lamb Health metrics

# Weight
kwtD <- kruskal.test(Weight_kg ~ Diet, data = EX_ps_clean.rar.rich.df) 
kwtT <- kruskal.test(Weight_kg ~ Treatment, data = EX_ps_clean.rar.rich.df)
kwtW <- kruskal.test(Weight_kg ~ Week, data = EX_ps_clean.rar.rich.df)

# Albumin - ALB
kalD <- kruskal.test(ALB ~ Diet, data = EX_ps_clean.rar.rich.df) 
kalT <- kruskal.test(ALB ~ Treatment, data = EX_ps_clean.rar.rich.df)
kalW <- kruskal.test(ALB ~ Week, data = EX_ps_clean.rar.rich.df)

# Glucose - GLU
kglD <- kruskal.test(GLU ~ Diet, data = EX_ps_clean.rar.rich.df) 
kglT <- kruskal.test(GLU ~ Treatment, data = EX_ps_clean.rar.rich.df)
kglW <- kruskal.test(GLU ~ Week, data = EX_ps_clean.rar.rich.df)

# Phosphorus - PHOS
kphD <- kruskal.test(PHOS ~ Diet, data = EX_ps_clean.rar.rich.df) 
kphT <- kruskal.test(PHOS ~ Treatment, data = EX_ps_clean.rar.rich.df)
kphW <- kruskal.test(PHOS ~ Week, data = EX_ps_clean.rar.rich.df)

# Sodium - NA.
knaD <- kruskal.test(NA. ~ Diet, data = EX_ps_clean.rar.rich.df) 
knaT <- kruskal.test(NA. ~ Treatment, data = EX_ps_clean.rar.rich.df)
knaW <- kruskal.test(NA. ~ Week, data = EX_ps_clean.rar.rich.df)

# Potassium - K
kpoD <- kruskal.test(K ~ Diet, data = EX_ps_clean.rar.rich.df) 
kpoT <- kruskal.test(K ~ Treatment, data = EX_ps_clean.rar.rich.df)
kpoW <- kruskal.test(K ~ Week, data = EX_ps_clean.rar.rich.df)

# Chloride - CL
kclD <- kruskal.test(CL ~ Diet, data = EX_ps_clean.rar.rich.df) 
kclT <- kruskal.test(CL ~ Treatment, data = EX_ps_clean.rar.rich.df)
kclW <- kruskal.test(CL ~ Week, data = EX_ps_clean.rar.rich.df)

# Magnesium - MG
kmgD <- kruskal.test(MG ~ Diet, data = EX_ps_clean.rar.rich.df) 
kmgT <- kruskal.test(MG ~ Treatment, data = EX_ps_clean.rar.rich.df)
kmgW <- kruskal.test(MG ~ Week, data = EX_ps_clean.rar.rich.df)

# Total Carbon Dioxide - TC02
kcoD <- kruskal.test(TC02 ~ Diet, data = EX_ps_clean.rar.rich.df) 
kcoT <- kruskal.test(TC02 ~ Treatment, data = EX_ps_clean.rar.rich.df)
kcoW <- kruskal.test(TC02 ~ Week, data = EX_ps_clean.rar.rich.df)
```

```{r kw_matrix, echo = FALSE}
# Result Matrix
kwmatrix <- matrix(c(kalD$data.name,kalD$p.value,kalD$statistic,kalD$parameter,kalT$data.name,kalT$p.value,kalT$statistic,kalT$parameter,kalW$data.name,kalW$p.value,kalW$statistic,kalW$parameter,kclD$data.name,kclD$p.value,kclD$statistic,kclD$parameter,kclT$data.name,kclT$p.value,kclT$statistic,kclT$parameter,kclW$data.name,kclW$p.value,kclW$statistic,kclW$parameter,kcoD$data.name,kcoD$p.value,kcoD$statistic,kcoD$parameter,kcoT$data.name,kcoT$p.value,kcoT$statistic,kcoT$parameter,kcoW$data.name,kcoW$p.value,kcoW$statistic,kcoW$parameter,kglD$data.name,kglD$p.value,kglD$statistic,kglD$parameter,kglT$data.name,kglT$p.value,kglT$statistic,kglT$parameter,kglW$data.name,kglW$p.value,kglW$statistic,kglW$parameter,kmgD$data.name,kmgD$p.value,kmgD$statistic,kmgD$parameter,kmgT$data.name,kmgT$p.value,kmgT$statistic,kmgT$parameter,kmgW$data.name,kmgW$p.value,kmgW$statistic,kmgW$parameter,knaD$data.name,knaD$p.value,knaD$statistic,knaD$parameter,knaT$data.name,knaT$p.value,knaT$statistic,knaT$parameter,knaW$data.name,knaW$p.value,knaW$statistic,knaW$parameter,kphD$data.name,kphD$p.value,kphD$statistic,kphD$parameter,kphT$data.name,kphT$p.value,kphT$statistic,kphT$parameter,kphW$data.name,kphW$p.value,kphW$statistic,kphW$parameter,kpoD$data.name,kpoD$p.value,kpoD$statistic,kpoD$parameter,kpoT$data.name,kpoT$p.value,kpoT$statistic,kpoT$parameter,kpoW$data.name,kpoW$p.value,kpoW$statistic,kpoW$parameter,kwchaD$data.name,kwchaD$p.value,kwchaD$statistic,kwchaD$parameter,kwchaT$data.name,kwchaT$p.value,kwchaT$statistic,kwchaT$parameter,kwchaW$data.name,kwchaW$p.value,kwchaW$statistic,kwchaW$parameter,kwshaD$data.name,kwshaD$p.value,kwshaD$statistic,kwshaD$parameter,kwshaT$data.name,kwshaT$p.value,kwshaT$statistic,kwshaT$parameter,kwshaW$data.name,kwshaW$p.value,kwshaW$statistic,kwshaW$parameter,kwsimD$data.name,kwsimD$p.value,kwsimD$statistic,kwsimD$parameter,kwsimT$data.name,kwsimT$p.value,kwsimT$statistic,kwsimT$parameter,kwsimW$data.name,kwsimW$p.value,kwsimW$statistic,kwsimW$parameter,kwtD$data.name,kwtD$p.value,kwtD$statistic,kwtD$parameter,kwtT$data.name,kwtT$p.value,kwtT$statistic,kwtT$parameter,kwtW$data.name,kwtW$p.value,kwtW$statistic,kwtW$parameter), ncol = 4, byrow = TRUE)

colnames(kwmatrix) <- c('KW Test', 'p-Value', 'Chi-Squared', 'df')
kwmatrix

sortkwmatrix <- kwmatrix[order(kwmatrix[, 2]), ]

sortkwmatrix
write.table(sortkwmatrix, file = 'C:/Users/bydav/Desktop/AVS 554/Output/kwmatrixresults.csv')
```

# Conover Test
Run as a list of the significant KW test results

```{r conover, echo = FALSE}
sigkw <- list(EX_ps_clean.rar.rich.df$PHOS, EX_ps_clean.rar.rich.df$Chao1, EX_ps_clean.rar.rich.df$Simpson, EX_ps_clean.rar.rich.df$MG, EX_ps_clean.rar.rich.df$Shannon, EX_ps_clean.rar.rich.df$NA.)

siggrp <- list(EX_ps_clean.rar.rich.df$Week, EX_ps_clean.rar.rich.df$Treatment, EX_ps_clean.rar.rich.df$Diet)

con1 <- ConoverTest(sigkw, method = "holm", alternative = c("two.sided"), out.list = FALSE)

con2 <- conover.test(sigkw, method="holm", kw=TRUE, label=TRUE, 
      wrap=TRUE, table=TRUE, list=FALSE, rmc=FALSE, alpha=0.05, altp=FALSE)
```
# RESULT:
Reject the null hypothesis for all KW significant results - all conover test comparisons have a p-value < 0.05

# Non-Parametric Regression

What impact did the level of microbial diversity and Week have on lamb health metrics?

Dependent Variables: Weight_kg, ALB, CA (with anova), PHOS, NA., K, CL, MG, TC02

Independent Variables: Observed, Shannon, Chao1, Simpson, Week, Evenness

```{r OLS_residuals, echo = FALSE}
# Create basic OLS models for Weight vs each metric
OLSObserved <- lm(Weight_kg~Observed, data=EX_ps_clean.rar.rich.df)
summary(OLSObserved)

OLSShannon <- lm(Weight_kg~Shannon, data=EX_ps_clean.rar.rich.df)
summary(OLSShannon)

OLSChao1 <- lm(Weight_kg~Chao1, data=EX_ps_clean.rar.rich.df)
summary(OLSChao1)

OLSSimpson <- lm(Weight_kg~Simpson, data=EX_ps_clean.rar.rich.df)
summary(OLSSimpson)

OLSWeek <- lm(Weight_kg~Week, data=EX_ps_clean.rar.rich.df)
summary(OLSWeek)

OLSEven <- lm(Weight_kg~EX_ps_clean.rar.evenSha, data=EX_ps_clean.rar.rich.df)
summary(OLSEven)

# Explore residuals
resObs <- resid(OLSObserved)
resSha <- resid(OLSShannon)
resCha <- resid(OLSChao1)
resSim <- resid(OLSSimpson)
resWee <- resid(OLSWeek)
resEve <- resid(OLSEven)

plot(fitted(model), res)

respObs <- plot(fitted(OLSObserved), resObs) # high scattering
respSha <- plot(fitted(OLSShannon), resSha) # possible clustering on the right but broad
respCha <- plot(fitted(OLSChao1), resCha) # possible clustering on the left but broad
respSim <- plot(fitted(OLSSimpson), resSim) # vertical clustering along the right
respWee <- plot(fitted(OLSWeek), resWee) # vertical clustering at either end
respEve <- plot(fitted(OLSEven), resEve) # vertical clustering on the right

qqnorm(resObs)
qqline(resObs) # good and strong cohesion to the line

qqnorm(resSha)
qqline(resSha) # A few strays at the very bottom and near the top but generally close to the line

qqnorm(resCha)
qqline(resCha) # stray outliers along the bottom but close to the line

qqnorm(resSim)
qqline(resSim) # stray outliers along the bottom but close to the line

qqnorm(resWee)
qqline(resWee) # Wider outliers at the bottom and top but central points are on the line

qqnorm(resEve)
qqline(resEve) # stray outliers along the bottom but close to the line
```

Model Explanations:
Diversity Metric Alone (x5)
Diversity Metric + Week (x5)
Week Alone

```{r lme_models, echo = FALSE}
moddf <- EX_ps_clean.rar.rich.df
Eve <- EX_ps_clean.rar.evenSha

# Weight_kg
Wt_Week = lm(Weight_kg ~ Week, method = "ML", data=moddf)

Wt_Obs = lm(Weight_kg ~ Observed, method = "ML", data=moddf) 
Wt_Week_Obs = lm(Weight_kg ~ Observed + Week, method = "ML", data=moddf)

Wt_Sha = lm(Weight_kg ~ Shannon, method = "ML", data=moddf) 
Wt_Week_Sha = lm(Weight_kg ~ Shannon + Week, method = "ML", data=moddf)

Wt_Cha = lm(Weight_kg ~ Chao1, method = "ML", data=moddf) 
Wt_Week_Cha = lm(Weight_kg ~ Chao1 + Week, method = "ML", data=moddf)

Wt_Sim = lm(Weight_kg ~ Simpson, method = "ML", data=moddf) 
Wt_Week_Sim = lm(Weight_kg ~ Simpson + Week, method = "ML", data=moddf)

Wt_Eve = lm(Weight_kg ~ Eve, method = "ML", data=moddf) 
Wt_Week_Eve = lm(Weight_kg ~ Eve + Week, method = "ML", data=moddf)

# ALB
ALB_Week = lm(ALB ~ Week, method = "ML", data=moddf)

ALB_Obs = lm(ALB ~ Observed, method = "ML", data=moddf) 
ALB_Week_Obs = lm(ALB ~ Observed + Week, method = "ML", data=moddf)

ALB_Sha = lm(ALB ~ Shannon, method = "ML", data=moddf) 
ALB_Week_Sha = lm(ALB ~ Shannon + Week, method = "ML", data=moddf)

ALB_Cha = lm(ALB ~ Chao1, method = "ML", data=moddf) 
ALB_Week_Cha = lm(ALB ~ Chao1 + Week, method = "ML", data=moddf)

ALB_Sim = lm(ALB ~ Simpson, method = "ML", data=moddf) 
ALB_Week_Sim = lm(ALB ~ Simpson + Week, method = "ML", data=moddf)

ALB_Eve = lm(ALB ~ Eve, method = "ML", data=moddf) 
ALB_Week_Eve = lm(ALB ~ Eve + Week, method = "ML", data=moddf)

# CA

CA_Week = lm(CA ~ Week, method = "ML", data=moddf)

CA_Obs = lm(CA ~ Observed, method = "ML", data=moddf) 
CA_Week_Obs = lm(CA ~ Observed + Week, method = "ML", data=moddf)

CA_Sha = lm(CA ~ Shannon, method = "ML", data=moddf) 
CA_Week_Sha = lm(CA ~ Shannon + Week, method = "ML", data=moddf)

CA_Cha = lm(CA ~ Chao1, method = "ML", data=moddf) 
CA_Week_Cha = lm(CA ~ Chao1 + Week, method = "ML", data=moddf)

CA_Sim = lm(CA ~ Simpson, method = "ML", data=moddf) 
CA_Week_Sim = lm(CA ~ Simpson + Week, method = "ML", data=moddf)

CA_Eve = lm(CA ~ Eve, method = "ML", data=moddf) 
CA_Week_Eve = lm(CA ~ Eve + Week, method = "ML", data=moddf)

# PHOS

PHOS_Week = lm(PHOS ~ Week, method = "ML", data=moddf)

PHOS_Obs = lm(PHOS ~ Observed, method = "ML", data=moddf) 
PHOS_Week_Obs = lm(PHOS ~ Observed + Week, method = "ML", data=moddf)

PHOS_Sha = lm(PHOS ~ Shannon, method = "ML", data=moddf) 
PHOS_Week_Sha = lm(PHOS ~ Shannon + Week, method = "ML", data=moddf)

PHOS_Cha = lm(PHOS ~ Chao1, method = "ML", data=moddf) 
PHOS_Week_Cha = lm(PHOS ~ Chao1 + Week, method = "ML", data=moddf)

PHOS_Sim = lm(PHOS ~ Simpson, method = "ML", data=moddf) 
PHOS_Week_Sim = lm(PHOS ~ Simpson + Week, method = "ML", data=moddf)

PHOS_Eve = lm(PHOS ~ Eve, method = "ML", data=moddf) 
PHOS_Week_Eve = lm(PHOS ~ Eve + Week, method = "ML", data=moddf)

# NA.

NA._Week = lm(NA. ~ Week, method = "ML", data=moddf)

NA._Obs = lm(NA. ~ Observed, method = "ML", data=moddf) 
NA._Week_Obs = lm(NA. ~ Observed + Week, method = "ML", data=moddf)

NA._Sha = lm(NA. ~ Shannon, method = "ML", data=moddf) 
NA._Week_Sha = lm(NA. ~ Shannon + Week, method = "ML", data=moddf)

NA._Cha = lm(NA. ~ Chao1, method = "ML", data=moddf) 
NA._Week_Cha = lm(NA. ~ Chao1 + Week, method = "ML", data=moddf)

NA._Sim = lm(NA. ~ Simpson, method = "ML", data=moddf) 
NA._Week_Sim = lm(NA. ~ Simpson + Week, method = "ML", data=moddf)

NA._Eve = lm(NA. ~ Eve, method = "ML", data=moddf) 
NA._Week_Eve = lm(NA. ~ Eve + Week, method = "ML", data=moddf)

# K

K_Week = lm(K ~ Week, method = "ML", data=moddf)

K_Obs = lm(K ~ Observed, method = "ML", data=moddf) 
K_Week_Obs = lm(K ~ Observed + Week, method = "ML", data=moddf)

K_Sha = lm(K ~ Shannon, method = "ML", data=moddf) 
K_Week_Sha = lm(K ~ Shannon + Week, method = "ML", data=moddf)

K_Cha = lm(K ~ Chao1, method = "ML", data=moddf) 
K_Week_Cha = lm(K ~ Chao1 + Week, method = "ML", data=moddf)

K_Sim = lm(K ~ Simpson, method = "ML", data=moddf) 
K_Week_Sim = lm(K ~ Simpson + Week, method = "ML", data=moddf)

K_Eve = lm(K ~ Eve, method = "ML", data=moddf) 
K_Week_Eve = lm(K ~ Eve + Week, method = "ML", data=moddf)

# CL

CL_Week = lm(CL ~ Week, method = "ML", data=moddf)

CL_Obs = lm(CL ~ Observed, method = "ML", data=moddf) 
CL_Week_Obs = lm(CL ~ Observed + Week, method = "ML", data=moddf)

CL_Sha = lm(CL ~ Shannon, method = "ML", data=moddf) 
CL_Week_Sha = lm(CL ~ Shannon + Week, method = "ML", data=moddf)

CL_Cha = lm(CL ~ Chao1, method = "ML", data=moddf) 
CL_Week_Cha = lm(CL ~ Chao1 + Week, method = "ML", data=moddf)

CL_Sim = lm(CL ~ Simpson, method = "ML", data=moddf) 
CL_Week_Sim = lm(CL ~ Simpson + Week, method = "ML", data=moddf)

CL_Eve = lm(CL ~ Eve, method = "ML", data=moddf) 
CL_Week_Eve = lm(CL ~ Eve + Week, method = "ML", data=moddf)

# MG

MG_Week = lm(MG ~ Week, method = "ML", data=moddf)

MG_Obs = lm(MG ~ Observed, method = "ML", data=moddf) 
MG_Week_Obs = lm(MG ~ Observed + Week, method = "ML", data=moddf)

MG_Sha = lm(MG ~ Shannon, method = "ML", data=moddf) 
MG_Week_Sha = lm(MG ~ Shannon + Week, method = "ML", data=moddf)

MG_Cha = lm(MG ~ Chao1, method = "ML", data=moddf) 
MG_Week_Cha = lm(MG ~ Chao1 + Week, method = "ML", data=moddf)

MG_Sim = lm(MG ~ Simpson, method = "ML", data=moddf) 
MG_Week_Sim = lm(MG ~ Simpson + Week, method = "ML", data=moddf)

MG_Eve = lm(MG ~ Eve, method = "ML", data=moddf) 
MG_Week_Eve = lm(MG ~ Eve + Week, method = "ML", data=moddf)

# TC02

TC_Week = lm(TC02 ~ Week, method = "ML", data=moddf)

TC_Obs = lm(TC02 ~ Observed, method = "ML", data=moddf) 
TC_Week_Obs = lm(TC02 ~ Observed + Week, method = "ML", data=moddf)

TC_Sha = lm(TC02 ~ Shannon, method = "ML", data=moddf) 
TC_Week_Sha = lm(TC02 ~ Shannon + Week, method = "ML", data=moddf)

TC_Cha = lm(TC02 ~ Chao1, method = "ML", data=moddf) 
TC_Week_Cha = lm(TC02 ~ Chao1 + Week, method = "ML", data=moddf)

TC_Sim = lm(TC02 ~ Simpson, method = "ML", data=moddf) 
TC_Week_Sim = lm(TC02 ~ Simpson + Week, method = "ML", data=moddf)

TC_Eve = lm(TC02 ~ Eve, method = "ML", data=moddf) 
TC_Week_Eve = lm(TC02 ~ Eve + Week, method = "ML", data=moddf)

```

```{r lme_results, echo = FALSE}
lm_compare <- matrix(c(AIC(ALB_Cha),BIC(ALB_Cha),logLik(ALB_Cha),
AIC(ALB_Eve),BIC(ALB_Eve),logLik(ALB_Eve),
AIC(ALB_Obs),BIC(ALB_Obs),logLik(ALB_Obs),
AIC(ALB_Sha),BIC(ALB_Sha),logLik(ALB_Sha),
AIC(ALB_Sim),BIC(ALB_Sim),logLik(ALB_Sim),
AIC(ALB_Week),BIC(ALB_Week),logLik(ALB_Week),
AIC(ALB_Week_Cha),BIC(ALB_Week_Cha),logLik(ALB_Week_Cha),
AIC(ALB_Week_Eve),BIC(ALB_Week_Eve),logLik(ALB_Week_Eve),
AIC(ALB_Week_Obs),BIC(ALB_Week_Obs),logLik(ALB_Week_Obs),
AIC(ALB_Week_Sha),BIC(ALB_Week_Sha),logLik(ALB_Week_Sha),
AIC(ALB_Week_Sim),BIC(ALB_Week_Sim),logLik(ALB_Week_Sim),
AIC(CA_Cha),BIC(CA_Cha),logLik(CA_Cha),
AIC(CA_Eve),BIC(CA_Eve),logLik(CA_Eve),
AIC(CA_Obs),BIC(CA_Obs),logLik(CA_Obs),
AIC(CA_Sha),BIC(CA_Sha),logLik(CA_Sha),
AIC(CA_Sim),BIC(CA_Sim),logLik(CA_Sim),
AIC(CA_Week),BIC(CA_Week),logLik(CA_Week),
AIC(CA_Week_Cha),BIC(CA_Week_Cha),logLik(CA_Week_Cha),
AIC(CA_Week_Eve),BIC(CA_Week_Eve),logLik(CA_Week_Eve),
AIC(CA_Week_Obs),BIC(CA_Week_Obs),logLik(CA_Week_Obs),
AIC(CA_Week_Sha),BIC(CA_Week_Sha),logLik(CA_Week_Sha),
AIC(CA_Week_Sim),BIC(CA_Week_Sim),logLik(CA_Week_Sim),
AIC(CL_Cha),BIC(CL_Cha),logLik(CL_Cha),
AIC(CL_Eve),BIC(CL_Eve),logLik(CL_Eve),
AIC(CL_Obs),BIC(CL_Obs),logLik(CL_Obs),
AIC(CL_Sha),BIC(CL_Sha),logLik(CL_Sha),
AIC(CL_Sim),BIC(CL_Sim),logLik(CL_Sim),
AIC(CL_Week),BIC(CL_Week),logLik(CL_Week),
AIC(CL_Week_Cha),BIC(CL_Week_Cha),logLik(CL_Week_Cha),
AIC(CL_Week_Eve),BIC(CL_Week_Eve),logLik(CL_Week_Eve),
AIC(CL_Week_Obs),BIC(CL_Week_Obs),logLik(CL_Week_Obs),
AIC(CL_Week_Sha),BIC(CL_Week_Sha),logLik(CL_Week_Sha),
AIC(CL_Week_Sim),BIC(CL_Week_Sim),logLik(CL_Week_Sim),
AIC(K_Cha),BIC(K_Cha),logLik(K_Cha),
AIC(K_Eve),BIC(K_Eve),logLik(K_Eve),
AIC(K_Obs),BIC(K_Obs),logLik(K_Obs),
AIC(K_Sha),BIC(K_Sha),logLik(K_Sha),
AIC(K_Sim),BIC(K_Sim),logLik(K_Sim),
AIC(K_Week),BIC(K_Week),logLik(K_Week),
AIC(K_Week_Cha),BIC(K_Week_Cha),logLik(K_Week_Cha),
AIC(K_Week_Eve),BIC(K_Week_Eve),logLik(K_Week_Eve),
AIC(K_Week_Obs),BIC(K_Week_Obs),logLik(K_Week_Obs),
AIC(K_Week_Sha),BIC(K_Week_Sha),logLik(K_Week_Sha),
AIC(K_Week_Sim),BIC(K_Week_Sim),logLik(K_Week_Sim),
AIC(MG_Cha),BIC(MG_Cha),logLik(MG_Cha),
AIC(MG_Eve),BIC(MG_Eve),logLik(MG_Eve),
AIC(MG_Obs),BIC(MG_Obs),logLik(MG_Obs),
AIC(MG_Sha),BIC(MG_Sha),logLik(MG_Sha),
AIC(MG_Sim),BIC(MG_Sim),logLik(MG_Sim),
AIC(MG_Week),BIC(MG_Week),logLik(MG_Week),
AIC(MG_Week_Cha),BIC(MG_Week_Cha),logLik(MG_Week_Cha),
AIC(MG_Week_Eve),BIC(MG_Week_Eve),logLik(MG_Week_Eve),
AIC(MG_Week_Obs),BIC(MG_Week_Obs),logLik(MG_Week_Obs),
AIC(MG_Week_Sha),BIC(MG_Week_Sha),logLik(MG_Week_Sha),
AIC(MG_Week_Sim),BIC(MG_Week_Sim),logLik(MG_Week_Sim),
AIC(NA._Cha),BIC(NA._Cha),logLik(NA._Cha),
AIC(NA._Eve),BIC(NA._Eve),logLik(NA._Eve),
AIC(NA._Obs),BIC(NA._Obs),logLik(NA._Obs),
AIC(NA._Sha),BIC(NA._Sha),logLik(NA._Sha),
AIC(NA._Sim),BIC(NA._Sim),logLik(NA._Sim),
AIC(NA._Week),BIC(NA._Week),logLik(NA._Week),
AIC(NA._Week_Cha),BIC(NA._Week_Cha),logLik(NA._Week_Cha),
AIC(NA._Week_Eve),BIC(NA._Week_Eve),logLik(NA._Week_Eve),
AIC(NA._Week_Obs),BIC(NA._Week_Obs),logLik(NA._Week_Obs),
AIC(NA._Week_Sha),BIC(NA._Week_Sha),logLik(NA._Week_Sha),
AIC(NA._Week_Sim),BIC(NA._Week_Sim),logLik(NA._Week_Sim),
AIC(PHOS_Cha),BIC(PHOS_Cha),logLik(PHOS_Cha),
AIC(PHOS_Eve),BIC(PHOS_Eve),logLik(PHOS_Eve),
AIC(PHOS_Obs),BIC(PHOS_Obs),logLik(PHOS_Obs),
AIC(PHOS_Sha),BIC(PHOS_Sha),logLik(PHOS_Sha),
AIC(PHOS_Sim),BIC(PHOS_Sim),logLik(PHOS_Sim),
AIC(PHOS_Week),BIC(PHOS_Week),logLik(PHOS_Week),
AIC(PHOS_Week_Cha),BIC(PHOS_Week_Cha),logLik(PHOS_Week_Cha),
AIC(PHOS_Week_Eve),BIC(PHOS_Week_Eve),logLik(PHOS_Week_Eve),
AIC(PHOS_Week_Obs),BIC(PHOS_Week_Obs),logLik(PHOS_Week_Obs),
AIC(PHOS_Week_Sha),BIC(PHOS_Week_Sha),logLik(PHOS_Week_Sha),
AIC(PHOS_Week_Sim),BIC(PHOS_Week_Sim),logLik(PHOS_Week_Sim),
AIC(TC_Cha),BIC(TC_Cha),logLik(TC_Cha),
AIC(TC_Eve),BIC(TC_Eve),logLik(TC_Eve),
AIC(TC_Obs),BIC(TC_Obs),logLik(TC_Obs),
AIC(TC_Sha),BIC(TC_Sha),logLik(TC_Sha),
AIC(TC_Sim),BIC(TC_Sim),logLik(TC_Sim),
AIC(TC_Week),BIC(TC_Week),logLik(TC_Week),
AIC(TC_Week_Cha),BIC(TC_Week_Cha),logLik(TC_Week_Cha),
AIC(TC_Week_Eve),BIC(TC_Week_Eve),logLik(TC_Week_Eve),
AIC(TC_Week_Obs),BIC(TC_Week_Obs),logLik(TC_Week_Obs),
AIC(TC_Week_Sha),BIC(TC_Week_Sha),logLik(TC_Week_Sha),
AIC(TC_Week_Sim),BIC(TC_Week_Sim),logLik(TC_Week_Sim),
AIC(Wt_Cha),BIC(Wt_Cha),logLik(Wt_Cha),
AIC(Wt_Eve),BIC(Wt_Eve),logLik(Wt_Eve),
AIC(Wt_Obs),BIC(Wt_Obs),logLik(Wt_Obs),
AIC(Wt_Sha),BIC(Wt_Sha),logLik(Wt_Sha),
AIC(Wt_Sim),BIC(Wt_Sim),logLik(Wt_Sim),
AIC(Wt_Week),BIC(Wt_Week),logLik(Wt_Week),
AIC(Wt_Week_Cha),BIC(Wt_Week_Cha),logLik(Wt_Week_Cha),
AIC(Wt_Week_Eve),BIC(Wt_Week_Eve),logLik(Wt_Week_Eve),
AIC(Wt_Week_Obs),BIC(Wt_Week_Obs),logLik(Wt_Week_Obs),
AIC(Wt_Week_Sha),BIC(Wt_Week_Sha),logLik(Wt_Week_Sha),
AIC(Wt_Week_Sim),BIC(Wt_Week_Sim),logLik(Wt_Week_Sim)), ncol = 3, byrow = TRUE)

colnames(lm_compare) <- c("AIC", "BIC", "Log Likelihood")

rownames(lm_compare) <- c("ALB_Cha","ALB_Eve","ALB_Obs","ALB_Sha","ALB_Sim","ALB_Week","ALB_Week_Cha","ALB_Week_Eve","ALB_Week_Obs","ALB_Week_Sha","ALB_Week_Sim","CA_Cha","CA_Eve","CA_Obs","CA_Sha","CA_Sim","CA_Week","CA_Week_Cha","CA_Week_Eve","CA_Week_Obs","CA_Week_Sha","CA_Week_Sim","CL_Cha","CL_Eve","CL_Obs","CL_Sha","CL_Sim","CL_Week","CL_Week_Cha","CL_Week_Eve","CL_Week_Obs","CL_Week_Sha","CL_Week_Sim","K_Cha","K_Eve","K_Obs","K_Sha","K_Sim","K_Week","K_Week_Cha","K_Week_Eve","K_Week_Obs","K_Week_Sha","K_Week_Sim","MG_Cha","MG_Eve","MG_Obs","MG_Sha","MG_Sim","MG_Week","MG_Week_Cha","MG_Week_Eve","MG_Week_Obs","MG_Week_Sha","MG_Week_Sim","NA._Cha","NA._Eve","NA._Obs","NA._Sha","NA._Sim","NA._Week","NA._Week_Cha","NA._Week_Eve","NA._Week_Obs","NA._Week_Sha","NA._Week_Sim","PHOS_Cha","PHOS_Eve","PHOS_Obs","PHOS_Sha","PHOS_Sim","PHOS_Week","PHOS_Week_Cha","PHOS_Week_Eve","PHOS_Week_Obs","PHOS_Week_Sha","PHOS_Week_Sim","TC_Cha","TC_Eve","TC_Obs","TC_Sha","TC_Sim","TC_Week","TC_Week_Cha","TC_Week_Eve","TC_Week_Obs","TC_Week_Sha","TC_Week_Sim","Wt_Cha","Wt_Eve","Wt_Obs","Wt_Sha","Wt_Sim","Wt_Week","Wt_Week_Cha","Wt_Week_Eve","Wt_Week_Obs","Wt_Week_Sha","Wt_Week_Sim")

sort_lm_matrix <- lm_compare[order(lm_compare[, 1]), ]

sort_lm_matrix
write.table(sort_lm_matrix, file = 'C:/Users/bydav/Desktop/AVS 554/Output/lm_results.csv')

```

# Taxonomic Information and Maps

```{r unique, echo = FALSE}
taxa_abundance_bars(
taxa_filter(EX_ps_clean.rar, frequency = 0.8),
classification = 'Phylum', treatment = c('Week', 'Treatment'),
subset = c('0', '1', '2'), transformation = 'none')

taxa_abundance_bars(
unique_taxa(EX_ps_clean, c('Week', 'Sheep_ID')),
classification = 'Phylum', transformation = 'none')
```

```{r heatmap, echo = FALSE}
EX_ps_clean.rar.glom = tax_glom(EX_ps_clean.rar, "Family")

plot_heatmap(EX_ps_clean.rar, fill="Family") + facet_grid(Diet~Week, space="free", scales="free") + theme(legend.position = "bottom") + theme(axis.text.x = element_blank())

plot_heatmap(EX_ps_clean.rar, fill="Family") + theme(legend.position = "bottom") + theme(axis.text.x = element_blank())

taxabund0 <- abundance_heatmap(EX_ps_clean.rar, classification = 'Phylum', treatment = c('Week'), subset = '0')

taxabund1 <- abundance_heatmap(EX_ps_clean.rar, classification = 'Phylum', treatment = c('Week'), subset = '1')

taxabund2 <- abundance_heatmap(EX_ps_clean.rar, classification = 'Phylum', treatment = c('Week'), subset = '2')

taxabund0
taxabund1
taxabund2

ggarrange(taxabund0, taxabund1, taxabund2, nrow = 2, ncol = 2, labels = NULL, common.legend = TRUE)
```

```{r 100bar, echo = FALSE}
# All Taxa
EX_ps_clean.rar.stacked = transform_sample_counts(EX_ps_clean.rar, function(x) x / sum(x) )

plot_bar(EX_ps_clean.rar.stacked, fill="Phylum")

# Top 100 by abundance only
physeqhigh = prune_taxa(taxa_names(EX_ps_clean.rar)[1:100], EX_ps_clean.rar)

physeqhighbars = transform_sample_counts(physeqhigh, function(x) x / sum(x) )
plot_bar(physeqhighbars, fill="Phylum")

# Lowest 100 by abundance only
physeqlow = prune_taxa(taxa_names(EX_ps_clean.rar)[5968:6068], EX_ps_clean.rar)
physeqlowbars = transform_sample_counts(physeqlow, function(x) x / sum(x) )
plot_bar(physeqlowbars, fill="Phylum")
```

```{r dendro, echo = FALSE}
dendrogram_phyloseq(EX_ps_clean.rar, treatment = c('Treatment'), method = 'bray',colors = 'spectral')
```

```{r taxa_dist}
get_taxa_unique(EX_ps_clean.rar, taxonomic.rank="Family", errorIfNULL=TRUE)

proportionstreat <- taxa_proportions(EX_ps_clean.rar, 'Phylum', treatment = c('Treatment'))

ggplot(proportions, aes(x = Treatment, y = Proportion, fill = Phylum)) + geom_col(position = "fill") + theme(axis.text.x = element_text(angle = 60, hjust = 1))

proportionslamb <- taxa_proportions(EX_ps_clean.rar, 'Phylum', treatment = c('Sheep_ID'))

ggplot(proportionslamb, aes(x = Sheep_ID, y = Proportion, fill = Phylum)) + geom_col(position = "fill") + theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

```{r}
varcorrmapD <- variable_correlation_heatmap(EX_ps_clean.rar, treatment = c("Diet"), classification = 'Phylum', variables = c('Weight_kg', 'ALB', 'GLU', 'CA', 'PHOS', 'NA.', 'K', 'CL', 'MG', 'TC02'),
method = 'spearman', limits = c(-0.8, 0.8),
colors = 'default', significance_color = 'black', cores = 1)
varcorrmapD

corrtableD <- variable_correlation(EX_ps_clean.rar, variables = c('Weight_kg', 'ALB', 'GLU', 'CA', 'PHOS', 'NA.', 'K', 'CL', 'MG', 'TC02'), treatment = c("Diet"), classification = 'Phylum', method = 'spearman', cores = 1)
corrtableD

sigcorrD <- subset(corrtableD, p < 0.05)
sigcorrD

varcorrmapW <- variable_correlation_heatmap(EX_ps_clean.rar, treatment = c("Week"), classification = 'Phylum', variables = c('Weight_kg', 'ALB', 'GLU', 'CA', 'PHOS', 'NA.', 'K', 'CL', 'MG', 'TC02'),
method = 'spearman', limits = c(-0.8, 0.8),
colors = 'default', significance_color = 'black', cores = 1)
varcorrmapW

corrtableW <- variable_correlation(EX_ps_clean.rar, variables = c('Weight_kg', 'ALB', 'GLU', 'CA', 'PHOS', 'NA.', 'K', 'CL', 'MG', 'TC02'), treatment = c("Week"), classification = 'Phylum', method = 'spearman', cores = 1)
corrtableW
sigcorrW <- subset(corrtableW, p < 0.05)

sigcorrW
```

## DESeq

```{r deseq_changes, echo = FALSE}
EX_ps_clean_week0 = subset_samples(EX_ps_clean, Week == "0") 
EX_ps_clean_week0 <- prune_samples(sample_sums(EX_ps_clean_week0) > 0, EX_ps_clean_week0)
EX_ps_clean_week0 <- prune_taxa(taxa_sums(EX_ps_clean_week0) > 0, EX_ps_clean_week0)

EX_ps_clean_week1 = subset_samples(EX_ps_clean, Week == "1") 
EX_ps_clean_week1 <- prune_samples(sample_sums(EX_ps_clean_week1) > 0, EX_ps_clean_week1)
EX_ps_clean_week1 <- prune_taxa(taxa_sums(EX_ps_clean_week1) > 0, EX_ps_clean_week1)

EX_ps_clean_week2 = subset_samples(EX_ps_clean, Week == "2") 
EX_ps_clean_week2 <- prune_samples(sample_sums(EX_ps_clean_week2) > 0, EX_ps_clean_week2)
EX_ps_clean_week2 <- prune_taxa(taxa_sums(EX_ps_clean_week2) > 0, EX_ps_clean_week2)

diagdds_wk0 = phyloseq_to_deseq2(EX_ps_clean_week0, ~ Diet) 
diagdds_wk1 = phyloseq_to_deseq2(EX_ps_clean_week1, ~ Diet) 
diagdds_wk2 = phyloseq_to_deseq2(EX_ps_clean_week2, ~ Diet) 
```

```{r deseq_calc_0, echo = FALSE}
# Week 0
gm_mean0 = function(x, na.rm=TRUE){exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x)) } 
geoMeans0 = apply(counts(diagdds_wk0), 1, gm_mean0) 
diagdds_wk0 = estimateSizeFactors(diagdds_wk0, geoMeans = geoMeans0)

diagdds_wk0 = DESeq(diagdds_wk0, fitType="local")

# significance of ^ abundance calc
res0 = results(diagdds_wk0) 
res0 = res0[order(res0$padj, na.last=NA), ]
alpha0 = 0.01

res0 %>% data.frame() %>% View()
```

# RESULT: res0 padj are all very insignificant (>0.7) and using alpha as filter resulted in a sigtab with 0 rows. With the insignificant values kept, there were 731 SVs detected


```{r deseq_calc_1, echo = FALSE}
gm_mean1 = function(x, na.rm=TRUE){exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x)) } 
geoMeans1 = apply(counts(diagdds_wk1), 1, gm_mean1) 
diagdds_wk1 = estimateSizeFactors(diagdds_wk1, geoMeans = geoMeans1)

diagdds_wk1 = DESeq(diagdds_wk1, fitType="local", full = ~ Diet)

# significance of ^ abundance calc
res1 = results(diagdds_wk1) 
res1 = res1[order(res1$padj, na.last=NA), ]
alpha1 = 0.01 

res1 %>% data.frame() %>% View()
```

```{r deseq_calcsig_1, echo = FALSE}
sigtab1 = res1[(res1$padj < alpha1), ] 

sigtab1 = cbind(as(sigtab1, "data.frame"), as(tax_table(EX_ps_clean_week1)[rownames(sigtab1), ], "matrix")) 
dim(sigtab1)
```

```{r deseq_prep_1, echo = FALSE}
sigtab1 = sigtab1[sigtab1[, "log2FoldChange"] > 0, ]

sigtab1 = sigtab1[, c("baseMean", "log2FoldChange", "lfcSE", "padj", "Phylum", "Class", "Family")]

x1 = tapply(sigtab1$log2FoldChange, sigtab1$Phylum, function(x1) max(x1)) 
x1 = sort(x1, TRUE) 
sigtab1$Phylum = factor(as.character(sigtab1$Phylum), levels=names(x1))
```

```{r deseq_plot_1, echo = FALSE}
ggplot(sigtab1, aes(y=Family, x=log2FoldChange, color=Phylum)) + #play with aesthetics to make graph informative 
  geom_vline(xintercept = 0.0, color = "gray", size = 0.5) + geom_point(aes(size=baseMean)) + 
  #scale size by mean relative abundance 
  theme(axis.text.x = element_text(hjust = 0, vjust=0.5, size=10), axis.text.y = element_text(size=10)) + xlab("log2 Fold Change") + labs(size = "Mean Sequence Abundance") + theme_minimal() + ggtitle("Week 1")
```

```{r deseq_calc_2, echo = FALSE}
gm_mean2 = function(x, na.rm=TRUE){exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x)) } 
geoMeans2 = apply(counts(diagdds_wk2), 1, gm_mean2) 
diagdds_wk2 = estimateSizeFactors(diagdds_wk2, geoMeans = geoMeans2)

diagdds_wk2 = DESeq(diagdds_wk2, fitType="local", full = ~ Diet)

# significance of ^ abundance calc
res2 = results(diagdds_wk2) 
res2 = res2[order(res2$padj, na.last=NA), ]
alpha2 = 0.01 

res2 %>% data.frame() %>% View()
```

```{r deseq_calcsig_1, echo = FALSE}
sigtab2 = res2[(res2$padj < alpha2), ] 

sigtab2 = cbind(as(sigtab2, "data.frame"), as(tax_table(EX_ps_clean_week2)[rownames(sigtab2), ], "matrix")) 
dim(sigtab2)
```

# RESULT:
There are 191 unique SVs/taxa found

```{r deseq_prep_1, echo = FALSE}
sigtab2 = sigtab2[sigtab2[, "log2FoldChange"] > 0, ]

sigtab2 = sigtab2[, c("baseMean", "log2FoldChange", "lfcSE", "padj", "Phylum", "Class", "Family")]

x2 = tapply(sigtab2$log2FoldChange, sigtab2$Phylum, function(x2) max(x2)) 
x2 = sort(x2, TRUE) 
sigtab2$Phylum = factor(as.character(sigtab2$Phylum), levels=names(x2))
```

```{r deseq_plot_1, echo = FALSE}
ggplot(sigtab2, aes(y=Family, x=log2FoldChange, color=Phylum)) + #play with aesthetics to make graph informative 
  geom_vline(xintercept = 0.0, color = "gray", size = 0.5) + geom_point(aes(size=baseMean)) + 
  #scale size by mean relative abundance 
  theme(axis.text.x = element_text(hjust = 0, vjust=0.5, size=10), axis.text.y = element_text(size=10)) + xlab("log2 Fold Change") + labs(size = "Mean Sequence Abundance") + theme_minimal() + ggtitle("Week 2")
```

###########################################################################


# Alpha diversity metrics statistics (Lab 8)--------------
Phyloseq can measure and visualize alpha diversity: https://joey711.github.io/phyloseq/plot_richness-examples.html
phyloseq doesn't do stats or more complex graphs

#library(phyloseq)

Use phyloseq to measure alpha diversity
```{r}
EX_ps_clean.rar.rich <- estimate_richness(EX_ps_clean.rar, measure=c("Observed", "Shannon")) #change to whatever measures you want

# use phyloseq ro calculate Faith's Diversity metric (optional), https://rdrr.io/github/twbattaglia/btools/man/estimate_pd.html
EX_faith <- estimate_pd(EX_ps_clean.rar)

# OPTIONAL measure evenness for each SV individually
library(asbio)
library(microbiome)
EX_ps_clean.rar.even_SV <- evenness(otu_table(EX_ps_clean.rar))

# measure evenness for each sample
EX_ps_clean.rar.even <- EX_ps_clean.rar.rich$Shannon/log(EX_ps_clean.rar.rich$Observed)


# Coerce to data.frame and add the metadata for these samples
EX_ps_clean.rar.sd = as(sample_data(EX_ps_clean.rar), "matrix")
EX_ps_clean.rar.sd = as.data.frame(EX_ps_clean.rar.sd)
EX_ps_clean.rar.rich.df <- cbind(EX_ps_clean.rar.rich, EX_ps_clean.rar.even, EX_ps_clean.rar.sd)

# make a histogram to look at the shape of the data (bell curve? skew?). You can save this graph for your own benefit if you want.
hist(EX_ps_clean.rar.rich.df$Observed)

# Want to know how much skew there is? Measure Kurtosis (a.k.a. tailedness). You can save this graph for your own benefit if you want.
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
kurtosis(EX_ps_clean.rar.rich.df$Observed)
# example value output is 0.0006177001, looking for whether +/- and if a large/small number



head(EX_ps_clean.rar.rich.df)

#check the distribution of your data, which will change your approach
shapiro.test(EX_ps_clean.rar.rich.df$Shannon)
# W = 0.67622, p-value = 6.987e-06: my shannon diversity metric is not normally distributed (p-value > 0.05 reveals that the distribution of the data are not significantly different from normal distribution)

shapiro.test(EX_ps_clean.rar.rich.df$Observed)
# W = 0.94763, p-value = 0.2612, my observed data are normal. : p-value > 0.05 reveals that the distribution of the data are not significantly different from normal distribution

shapiro.test(EX_ps_clean.rar.rich.df$EX_ps_clean.rar.even)
# W = 0.58972, p-value = 6.973e-07, not normally distributed. : p-value > 0.05 reveals that the distribution of the data are not significantly different from normal distribution
```


If your alpha diversity is normally distributed (parametric)-----------------
```{r}
library(vegan)

# pick your anova model based on your study design: https://www.statmethods.net/stats/anova.html. Replace "y" with the alpha diversity measure you want, such as observed, evvenness, shannon, etc.
# Replace "A and B" with the factors you want from your own data, and and replace x with numerical data

    ### Linear model for numeric factors -----
# Interpret linear model in R https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R 
# https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r/#two-way-repeated-measures-anova
# https://boostedml.com/2019/06/linear-regression-in-r-interpreting-summarylm.html

library(lme4)
library(lmerTest)
library(emmeans)

# you might run each factor separately to find out if they are significant on their own. 
summary(lm(Observed ~ Diet, data= EX_ps_clean.rar.rich.df))

# if you have multiple groups, this will give you the pairwise comparisons
emmeans(lm,pairwise ~ A) 
# paste the output here

# what if you want to add log transformation to a numerical variable in your metadata? Check out the syntax here: https://rpubs.com/marvinlemos/log-transformation


    ### Wilcoxon (a.k.a. Mann-Whitney) for pairwise numeric or categorical factors -------
# Wilcoxon in R (a.k.a. Mann-Whitney): https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/
# Two data samples are independent if they come from distinct populations and the samples do not affect each other. Using the Mann-Whitney-Wilcoxon Test, we can decide whether the population distributions are identical while assuming them to follow the normal distribution. We can use this on non-normal data if we have many more than 30 samples per group and large variance between groups. Can only do pairwise comparisons (your factor can only have two variables in it)

# Replace "y" with the alpha diversity measure you want, "A" with the factor you want (can only do one as a time),
wilcox.test(y ~ A, data=EX_ps_clean.rar.rich.df)
```


ANOVA for multiple categorical factors -------
ANOVA is used when your factor has 2+ variables in it. If you have 3+ variables, you should run a post-hoc test to correct the p-value for multiple group comparisons

One Way Anova (Completely Randomized Design)
fit <- aov(y ~ A, data=EX_ps_clean.rar.rich.df)

Randomized Block Design (B is the blocking factor)
fit <- aov(y ~ A + B, data=EX_ps_clean.rar.rich.df)

Two Way Factorial Design
fit <- aov(y ~ A*B, data=EX_ps_clean.rar.rich.df) 

Analysis of Covariance
fit <- aov(y ~ A + x, data=EX_ps_clean.rar.rich.df) # x is a series of numerical values, like pH)


example of one way anova from class
example1 <- lm(Observed ~ Diet, data=EX_ps_clean.rar.rich.df)
example2 <- lm(Observed ~ Week, data=EX_ps_clean.rar.rich.df)

#which is model better? Find the AIC value
anova(example1,example2)




#example of two way from class
summary(aov(Observed ~ Diet * Week, data=EX_ps_clean.rar.rich.df))

#For within subjects designs, the data frame has to be rearranged so that each measurement on a subject is a separate observation. See R and Analysis of Variance.

# One Within Factor
fit <- aov(y~A+Error(Subject/A),data=EX_ps_clean.rar.rich.df) #Subject would be a Factor in your metadata, y is the value of interest, A is the Factor of interest

# Two Within Factors W1 W2, Two Between Factors B1 B2
fit <- aov(y~(W1*W2*B1*B2)+Error(Subject/(W1*W2))+(B1*B2),
           data=EX_ps_clean.rar.rich.df)


# once you have chosen a model, add a correction if you have multiple group comparisons (multiple factor levels)
# Tukey Honestly Significant Differences
TukeyHSD(fit) # where fit comes from aov()




  ## If your alpha diversity is not normally distributed (non-parametric) -----------------
    ### Kruskal-Wallis Test -----
# K-W is the non-parametric version of ANOVA: http://www.sthda.com/english/wiki/kruskal-wallis-test-in-r
kruskal.test(Observed ~ FactorA, data = EX_ps_clean.rar.rich.df)


# Follow it up with a Conover Test if you have multiple comparisons
# https://rdrr.io/cran/DescTools/man/ConoverTest.html 

ConoverTest(Observed, FactorA,
            method = c("holm", "hochberg", "hommel", "bonferroni", "BH", "BY", "fdr", "none"), # method of correction, OK to pick just one
            alternative = c("two.sided", "less", "greater"),
            data = EX_ps_clean.rar.rich.df)


    ### Linear model for numeric factors -----
# Interpret linear model in R https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R 
# https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r/#two-way-repeated-measures-anova
# https://boostedml.com/2019/06/linear-regression-in-r-interpreting-summarylm.html 



library(lme4)
library(lmerTest)
library(emmeans)

# you might run each factor separately to find out if they are significant on their own. 
summary(lm(Observed ~ Diet, data= EX_ps_clean.rar.rich.df))


# you might run each factor separately on just a subset of the data.
summary(lm(Observed ~ Diet, data=subset(EX_ps_clean.rar.rich.df, Week=="2")))

# what if you want to add log transformation to a numerical variable in your metadata? Check out the syntax here: https://rpubs.com/marvinlemos/log-transformation


    ### Linear mixed effects models for complicated experimental designs -----
# If you have more complicated experimental designs, you might need lmer or glmer models to accommodate that
# does not have a normal distribution, so compare means using glmer and poisson distribution (number of events within a time interval): https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459

lm <- (glmer(Observed ~ A + C + (1|Year_collected), family=poisson, data=EX_ps_clean.rar.rich.df))

summary(lm)
# paste the output here


# if you have multiple groups, this will give you the pairwise comparisons
emmeans(lm,pairwise ~ A) 
# paste the output here

emmeans(lm,pairwise ~ B) 
# paste the output here




# More ways to graph alpha diversity (Lab 9) -------
  ## Heatmaps --------------
# base plot
plot_heatmap(EX_ps_clean.rar)

# follow the tutorial to make prettier versions in phyloseq: https://joey711.github.io/phyloseq/plot_heatmap-examples.html

# example from class
EX_ps_clean.rar.glom = tax_glom(EX_ps_clean.rar, "Family")

plot_heatmap(EX_ps_clean.rar, fill="Family") +   facet_grid(~Week, space="free", scales="free") + theme(legend.position = "bottom") #, axis.text.x = element_blank()) 

#example from class, but looks like trash without a lot of fancying up
heatmap(otu_table(EX_ps_clean.rar))


  ## Correlogram --------------
# This makes a correlation matrix plot: https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html

library(RColorBrewer)
library(corrplot)


# corrplot requires one dataframe of metadata and/or seqtab data.  Any columns in the dataframe will be correlated against all others.  Too many columns makes the graph unreadable, try to keep it to <50.

# select to top 15 most abundant SVs from your phyloseq object and extract to a dataframe
#take out top 100 taxa based on abundance
topNOTUs = names(sort(taxa_sums(EX_ps_clean.rar), TRUE)[1:15])
top15 = prune_taxa(topNOTUs, EX_ps_clean.rar)

# combine with your metadata and create one dataframe object. you can include other info that you created for a previous dataframe, as long as those objects are still in your R environment. Reminder, you can only use numeric data in a correlation matrix, so you will have to drop certain columns or make them numeric instead.
# Coerce to data.frame and add the metadata for these samples
top15.sd = as(otu_table(top15), "matrix")
top15.sd = as.data.frame(top15.sd)

# add Genus names in place of the full sequence name that is in the SV columns
top15.tax <- as.data.frame(tax_table(top15))
## if the Genus is empty, replace with the Family
top15.tax$Genus[is.na(top15.tax$Genus)] <- top15.tax$Family[is.na(top15.tax$Genus)]
colnames(top15.sd) = top15.tax$Genus

# paste all the components together
EX_ps_clean.rar.corr.df <- cbind(top15.sd, EX_ps_clean.rar.rich, EX_ps_clean.rar.even, EX_ps_clean.rar.sd)

# check header to make sure it looks good
head(EX_ps_clean.rar.corr.df)

# change any column factor names to make them prettier
names(EX_ps_clean.rar.corr.df)[names(EX_ps_clean.rar.corr.df) == "EX_ps_clean.rar.even"] <- "Evenness"

# check header to make sure it looks good
head(EX_ps_clean.rar.corr.df)

# for example, drop a column with factorial data
EX_ps_clean.rar.corr.df <- subset(EX_ps_clean.rar.corr.df, select = -c(Treatment, Diet, Sheep_ID, ICTERIC_INDEX, LIPEMIC_INDEX))

# check header to make sure it looks good
head(EX_ps_clean.rar.corr.df)

# check that all remaining columns are numeric instead of factor or character
str(EX_ps_clean.rar.corr.df)

# clean up any columns which are not registering as numeric
EX_ps_clean.rar.corr.df <- sapply(EX_ps_clean.rar.corr.df, as.numeric)



# alternatively, load a premade dataframe containing your chosen SVs from your sequence table (otu)table in phyloseq) and the metadata you want to include
# EX_ps_clean.rar.corr.df <- read.csv("example_correlogram_dataframe.csv", check.names = FALSE, header=T, row.names=1)



# run correlations
ex_corr_calc <- cor(EX_ps_clean.rar.corr.df, 
                    use="complete.obs", # use=Specifies the handling of missing data. Options are all.obs (assumes no missing data - missing data will produce an error), complete.obs (listwise deletion), and pairwise.complete.obs (pairwise deletion)
                    method="spearman") # correlation method=pearson, spearman, or kendall

### Note, if you have too few samples, you may receive an error about too few observations. You may ignore the error message, or remove than column



### test significance of correlations, copy and paste this whole chunk together
cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
  diag(p.mat) <- 0
  diag(lowCI.mat) <- diag(uppCI.mat) <- 1
  for(i in 1:(n-1)){
    for(j in (i+1):n){
      tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
      p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
      lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
      uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
    }
  }
  return(list(p.mat, lowCI.mat, uppCI.mat))
}

res1 <- cor.mtest(EX_ps_clean.rar.corr.df,0.95)
res2 <- cor.mtest(EX_ps_clean.rar.corr.df,0.99)
# Note: if you come up with a warning message, it means that one or more of your columns generated correlations of value 0, which makes it angry.  Visualize the plot, and "?" will come up in those columns.  To fix, remove the column or add a data transformation, to the dataframe you created to make this.  Set the corr.mtest function and run corr.mtest again.


## plot only those correlations with a significant p-value <0.05, using hierarchical clustering
corrplot(ex_corr_calc, 
         type="lower", #shape of the plot itself: full, upper, lower
         method="circle", #shape of the data: circle, square, ellipse, number, shade, color, pie 
         order="hclust", #how to cluster samples: AOE, hclust, FPC, alphabet, or leave blank
         p.mat = res1[[1]], #which correlations to use
         sig.level=0.05, #sets significance cutoff
         insig="blank", #leaves p > 0.05 blank
         tl.col="black", # text color
         tl.cex=.9, #text size
         col=brewer.pal(n=10, name="RdYlBu")) #specify a color palette and number of shades needed





  ## Barplots------------------
# can add ggplot components to make it pretty
plot_bar(EX_ps_clean.rar, fill="Phylum") #don't recommend using genus here, it make crash R


# agglomerate SVs by taxa level to get rid of all the blacklines on the graph. can be done for any level of taxonomy
EX_ps_clean.rar.glom = tax_glom(EX_ps_clean.rar, "Genus")

# WARNING: if you group along the x-axis phyloseq just stacks seqence counts, it DOES NOT calculate show average for that group
plot_bar(EX_ps_clean.rar.glom, fill="Genus")

# example from class
EX_ps_clean.rar.glom = tax_glom(EX_ps_clean.rar, "Family")
plot_bar(EX_ps_clean.rar, fill="Family") +   facet_grid(~Diet, space="free", scales="free") + theme(legend.position = "bottom", axis.text.x = element_blank()) 


# make a stacked 100% bar chart in phyloseq

EX_ps_clean.rar.stacked = transform_sample_counts(EX_ps_clean.rar, function(x) x / sum(x) )

plot_bar(EX_ps_clean.rar.stacked, fill="Phylum") 

# to filter by abundance and pool low abundance groups: https://github.com/joey711/phyloseq/issues/901


  ## Core community members -----------
#code from this website: https://microbiome.github.io/tutorials/CoremicrobiotaAmplicon.html

#load libraries as needed
library(dplyr)
library(microbiome)

#calculate compositional version of the data (relative abundances)
core_biomeW.rel <- microbiome::transform(EX_ps_clean.rar, "compositional") #CHANGE ME to your phyloseq object

#This returns the taxa that exceed the given prevalence and minimum abundance detection thresholds. Set your preferred thresholds.
core_taxa_standardW <- core_members(core_biomeW.rel, detection = 1/1000, prevalence = 70/100) #CHANGE ME

#A full phyloseq object of the core microbiota at those limits is obtained as follows
phylo.coreW <- core(core_biomeW.rel, detection = 1/1000, prevalence = .7)

###retrieving the associated taxa names from the phyloseq object and add it to what you just made.
core.taxaW <- taxa(phylo.coreW)
class(core.taxaW)

# get the taxonomy data and assign it to what you just made.
tax.matW <- tax_table(phylo.coreW)
tax.dfW <- as.data.frame(tax.matW)

# add the SVs to last column of what you just made.
tax.dfW$SV <- rownames(tax.dfW)

## CHANGE ME, write down how many taxa are shared between these samples.

# select taxonomy of only those OTUs that are core members based on the thresholds that were used.
core.taxa.classW <- dplyr::filter(tax.dfW, rownames(tax.dfW) %in% core.taxaW)
knitr::kable(head(core.taxa.classW))

# save the list so you can access later, can report just the list
write.csv(core.taxa.classW, "~/core.taxa.example.csv")


# graph the abundance of those shared taxa, here are some example: https://microbiome.github.io/tutorials/Core.html

#aggregate the genera so we don't get a lot of lines separating all the SVs
plot.gen <- aggregate_taxa(core_biomeW.rel, "Genus")

# load libraries as needed
library(ggplot)
library(RColorBrewer)
library(viridis)

prevalences <- seq(.05, 1, .05)
detections <- 10^seq(log10(1e-4), log10(.2), length = 10)

plot_core(plot.gen, 
          plot.type = "heatmap", 
          prevalences = prevalences, 
          detections = detections, min.prevalence = .5) + #CHANGE min prevalence
  xlab("Detection Threshold (Relative Abundance (%))") + ylab("Bacterial SVs") +
  theme_minimal() + scale_fill_viridis()


# Comparing changes in taxonomy (Lab 10) ---------------------------------

  ## Differential Abundance with DESeq ---------------------------------
# DESeq only does pairwise comparisons. To make a multifactorial comparison and graph, use "DESeq_and_ternary_plot_example.R".  You can also subset your data
library("DESeq2")
packageVersion("DESeq2")

# OPTIONAL if you need to subset 
EX_ps_clean_subsetA = subset_samples(EX_ps_clean, FACTOR == "A") #CHANGE ME 
EX_ps_clean_subsetA <- prune_samples(sample_sums(EX_ps_clean_subsetA) > 0, EX_ps_clean_subsetA)
EX_ps_clean_subsetA <- prune_taxa(taxa_sums(EX_ps_clean_subsetA) > 0, EX_ps_clean_subsetA)


# grab phyloseq data for use in deseq
diagdds = phyloseq_to_deseq2(EX_ps_clean_subsetA, ~ FACTOR) #CHANGE ME to an factor with only 2 levels

# calculate differential abundance
gm_mean =  function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")

# calculate significance for those abundance calculations
res = results(diagdds)
res = res[order(res$padj, na.last=NA), ]
alpha = 0.01
sigtab = res[(res$padj < alpha), ]
sigtab = cbind(as(sigtab, "data.frame"), as(tax_table(EX_ps_clean_subsetA)[rownames(sigtab), ], "matrix")) #CHANGE ME if you didn't subset your data

head(sigtab)

dim(sigtab)
#CHANGE ME number of SVs that were different between them


# calculate log changes and set
# sigtab = sigtab[sigtab[, "log2FoldChange"] < 0, ] #CHANGE ME. use this to select only positive (or negative) log changes
sigtab = sigtab[, c("baseMean", "log2FoldChange", "lfcSE", "padj", "Phylum", "Class", "Family", "Genus")] #CHANGE ME add Order or Species - if you have it

# Phylum order
x = tapply(sigtab$log2FoldChange, sigtab$Phylum, function(x) max(x))
x = sort(x, TRUE)
sigtab$Phylum = factor(as.character(sigtab$Phylum), levels=names(x))

# Genus order
x = tapply(sigtab$log2FoldChange, sigtab$Genus, function(x) max(x))
x = sort(x, TRUE)
sigtab$Genus = factor(as.character(sigtab$Genus), levels=names(x))


## if the Genus is empty, replace with the Family NOTE: this used to work but the syntax is broken for unknown reasons
# sigtab.df$Genus[is.na(sigtab.df$Genus)] <- sigtab.df$Family[is.na(sigtab.df$Genus)]

# if the Genus is empty, replace with the Family NOTE: works as of Feb 2020
sigtab$Genus = ifelse(is.na(sigtab$Genus), paste(sigtab$Family),paste(sigtab$Genus));sigtab

# OPTIONAL bind Genus and Species together - only works if you had species-level taxonomy AND you added it a few steps prior
# sigtab$Genus.species <- paste(sigtab$Genus, sigtab$Species)


library("ggplot2")

## graph differential abundance
ggplot(sigtab, aes(y=Genus, x=log2FoldChange, color=Phylum)) + #play with aesthetics to make graph informative
  geom_vline(xintercept = 0.0, color = "gray", size = 0.5) +
  geom_point(aes(size=baseMean)) + #scale size by mean relative abundance
  theme(axis.text.x = element_text(hjust = 0, vjust=0.5, size=10), axis.text.y = element_text(size=10)) + xlab("log2 Fold Change") + labs(size = "Mean Sequence Abundance") + theme_minimal()



  ## Feature Prediction (Differential Abundance) with Random Forest ---------------------------------
#  https://rpubs.com/michberr/randomforestmicrobe
# https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm
# https://www.rdocumentation.org/packages/randomForest/versions/4.6-12/topics/randomForest

library(phyloseq)
library(vegan)
library(plyr)
library(dplyr)
require(magrittr)
require(scales)
require(grid)
require(reshape2)
require(knitr)
library(randomForest)
library(rfPermute)
library(ggplot2)
library(RColorBrewer)

# Make a dataframe of training data with OTUs as column and samples as rows, which is the phyloseq OTU table
predictors <- otu_table(EX_ps_clean)

dim(predictors)
#CHANGE ME how many samples and SVs includes

# output phyloseq tax table as a dataframe to make it manipulable
tax.df <- data.frame(tax_table(EX_ps_clean), stringsAsFactors=FALSE)

## if the Genus is empty, replace with the Family
tax.df$Genus[is.na(tax.df$Genus)] <- tax.df$Family[is.na(tax.df$Genus)]

# bind Genus and Species together
tax.df$Genus.species <- paste(tax.df$Genus, tax.df$Species)


# set column of combined genus and species names as the column names for the predictors, replacing the full SV
colnames(predictors) <- tax.df$Genus.species

# clean up some of the other taxa info
colnames(predictors) = gsub("_unclassified", "", colnames(predictors))
colnames(predictors) = gsub("_Intercertae_Sedis", "", colnames(predictors))

### start here when choosing factors, can reuse the above lines as needed. one example for factorial data, and one for numeric data is provided. select as needed.

# Make one column for our outcome/response variable. Choose which one applies to the thing you want to test, and then follow factorial or numeric through the rest of the code section.
response <- as.factor(sample_data(EX_ps_clean)$Factorial_Data) #CHANGE ME
response <- as.numeric(sample_data(EX_ps_clean)$Numeric_Data) #CHANGE ME

# Combine response and SVs into data frame
rf.data <- data.frame(response, predictors)


# set seed for random number generation reproducibility
set.seed(2)

# classify for factorial data
response.pf <- rfPermute(response ~. , data = rf.data, na.action = na.omit, ntree= 500, nrep = 100) #na.omit ignores NAs in data (not tolerated). ntrees is how many forests to build, nreps generates p-value

# or this way for numeric data
response.pf <- rfPermute(as.numeric(response) ~. , data = rf.data, na.action = na.omit, ntree= 500, nrep = 100)

print(response.pf)
# paste the print out here, especially the OOB error. 1-(Out-of-the-box error) = accuracy of your model


# grab which features were labeled "important"
imp <- importance(response.pf, scale = TRUE)

# Make a data frame with predictor names and their importance
imp.df <- data.frame(predictors = rownames(imp), imp) 


# For factorial data, grab only those features with p-value < 0.05
imp.sig <- subset(imp.df, MeanDecreaseAccuracy.pval <= 0.05) 
print(dim(imp.sig))
# CHANGE ME, how many SVs (sig features) were identified

# For numeric data, grab only those features with p-value < 0.05
imp.sig <- subset(imp.df, IncNodePurity.pval <= 0.05)
print(dim(imp.sig))
# CHANGE ME, how many SVs (sig features) were identified


# For Factorial data, sort by importance amount
imp.sort <- arrange(imp.sig, desc(MeanDecreaseAccuracy))

# For numeric data, sort by importance amount
imp.sort <- arrange(imp.sig, desc(IncNodePurity))



#create levels to the factor based on SV table
imp.sort$predictors <- factor(imp.sort$predictors, levels = imp.sort$predictors)

# Select the top so many predictors (more than 50 is a crowded graph)
imp.top <- imp.sort[1:50, ]

# figure out what they are and name them, otherwise will just be named the full SV
otunames <- imp.top$predictors


# need to grab the abundance data out of the otu_table and make a new data.frame, and add the taxa names back in
# grab the column names from the otu table that match those in the forest set
pred.abun.colnum <- which(colnames(rf.data) %in% otunames)

# when you find a match, grad the abudnance data
pred.abun <- rf.data[,sort(c(pred.abun.colnum))]

# make this into a dataframe for manipulation
pred.abun.df <- data.frame(pred.abun, stringsAsFactors=FALSE)

# use the row.names (sample names) from the phyloseq object to name the samples in your forest
row.names(pred.abun.df) <- row.names(sample_data(EX_ps_clean))


# set color palette  
col.bro <- (rainbow(6))

# add white to that color list
col.bro <- append(col.bro, "#ffffff", after = 6)

# add some factors that you can use to make your graph pretty, as many as you want
pred.abun.df$Sample <- row.names(sample_data(EX_ps_clean)) #always grab the sample names
pred.abun.df$FactorA <- sample_data(EX_ps_clean)$FactorA #CHANGE ME
pred.abun.df$FactorB <- sample_data(EX_ps_clean)$FactorB #CHANGE ME


# optional, if you want factors to graph in a specific order, you can set that manually, and relabel them so they are more readable in the graph
pred.abun.df$Date <- factor(pred.abun.df$Date, levels=c("21_Apr", "12_May", "01_Jun", "22_Jun", "25_Jul"), labels=c("21 Apr", "12 May", "01 Jun", "22 Jun", "25 Jul")) #CHANGE ME

# reload these packages in this order, because sometimes the ddply function breaks
 library(plyr)
 library(dplyr)

# melt and transform the data using ALL the factors you added
m <- melt(pred.abun.df, id.vars=c("Sample", "FactorA", "FactorB"))
m <- ddply(m, .(variable), transform, rescale = log(1 + value)) #NOTE: may need to change it to as.numeric(value) if it tells you an error about non-binary operators. if you go to graph it and it doesn't recognize 'rescale', it means this piece failed.


# adjusted plot for factorial data, recommend using sample ID as 'factor A'
ggplot(m, aes(as.factor(FactorA), variable)) + 
  theme_minimal() + 
  facet_grid(.~FactorA, space="free", scale="free") + #set up graph facets to separate out levels of FactorA
  geom_tile(aes(fill = rescale), color="gray") + #add the heatmap coloring 
  scale_fill_gradientn(colors = rev(col.bro), na.value = 'white') + #use the preset color palette
  labs(fill="Log abundance") + #rename legend heading
  theme(legend.position = 'bottom',
        axis.text.x = element_text(angle = 0, size = 8),
        axis.ticks.x = element_line(size = 2),
        axis.text.y = element_text(size = 6)) +
  ylab('Predictor Taxa') +
  xlab('Factor A') 


# adjusted plot for numeric data, recommend using sample ID as 'factor A'
for_moist <-ggplot(m, aes(FactorA, variable)) + #can use as.factor(FactorA) if don't have consistent values across gradient and will cut out white space on x-axis
  theme_minimal() + 
  geom_tile(aes(fill = rescale), color="gray") + 
  scale_fill_gradientn(colors = rev(col.bro), na.value = 'white') + 
  labs(fill="Log abundance") +
  theme(legend.position = 'bottom', # could be none, top, right, left
        axis.text.x = element_text(angle = 0, size = 10),
        #axis.ticks.x = element_line(size = 2),
        axis.text.y = element_text(size = 10))+
 # scale_x_discrete(breaks=c(0,24.4, 50.3, 74.8, 98.3), labels=c("0" = "0", "24.4" ="25", "50.3" = "50", "74.8" = "75", "98.3" ="100")) + #optional, may want to add x-axis scale breaks
  ylab('Predictor Taxa') +
  xlab('Factor A') 



  ## LEFSe ---------------------------------
# can be done within R using the lefse package

# can transform phyloseq data to lefse-ready format with the command that you create, phyloseq_to_lefse
# uses code by seahorse001x: https://github.com/seashore001x/Rrumen/blob/master/phyloseq2lefse.R

require(dplyr)
require(tibble)

# this script defines a function to convert phyloseq object into lefse recognized file format. no need to change anything in this bit.
phyloseq_to_lefs <- function(physeq){
  # aggregate to genus level
  ps_lefse <- physeq %>% tax_glom(taxrank = 'Genus', NArm = F)
  
  # extract taxonomic data from phyloseq object and then stored in a vector called lefse_tax
  lefse_tax <- ps_lefse %>% tax_table %>% data.frame(stringsAsFactors=FALSE)
  lefse_tax <- replace(lefse_tax, is.na(lefse_tax), 'Unclassified')
  lefse_tax <- lefse_tax %>% group_by(Kingdom, Phylum, Class, Order, Family, Genus) %>% mutate(id = paste(Kingdom, Phylum, Class, Order, Family, Genus, sep = "|")) %>% ungroup %>% pull(id)
  
  # extract otu abundance matrix from phyloeq object and annotated with tax information
  lefse_matrix <- otu_table(ps_lefse) %>% data.frame(stringsAsFactors = F) %>% t %>% data.frame
  colnames(lefse_matrix) <- lefse_tax 
  
  # extract sample metadata and order sample same in lefse_matrix
  lefse_sample <- sample_data(ps_lefse)
  
  # convert factor in lefse_sample into character in order to combine sample and abundance data
  lefse_sample_isfactor <- sapply(lefse_sample, is.factor)
  lefse_sample[,lefse_sample_isfactor] <- lefse_sample[,lefse_sample_isfactor] %>% lapply(as.character)
  lefse_sample <- lefse_sample %>% data.frame
  
  lefse_table <- full_join(rownames_to_column(lefse_sample), rownames_to_column(lefse_matrix), by = ("rowname" = "rowname")) %>% t
  
  return(lefse_table)
}


EX_clean_for_lefse <- phyloseq_to_lefse(EX_ps_clean)

# save this output and upload it to the LEFSe web version on Galaxy: http://huttenhower.sph.harvard.edu/galaxy/
```

# ########################

# Recap

## Set-up: Loading Packages and Data

### Packages
```{r load_packages, include = FALSE}
library(tinytex)
library(dada2); packageVersion('dada2')
library(GenomeInfoDbData)
library(ShortRead)
library(phyloseq); packageVersion("phyloseq")
require(tidyr) 
library(ggplot2)
require(dplyr)
library(vegan)
library(RColorBrewer)
library(ggpubr)
library(ggsignif)
require(grid)
library(PerformanceAnalytics) 
library(vegan)
library(lme4) 
library(nlme)
library(lmerTest) 
library(emmeans)
library(viridis)
library(SiMRiv)
library(microbiome)
library("DESeq2")
packageVersion("DESeq2")
library(DescTools)
library(conover.test)
install.packages("devtools")
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
library(DEGreport)
library(ggfortify)
install.packages("remotes")
remotes::install_github("schuyler-smith/phyloschuyler")
library(phylosmith)
library(ggdendro)

```

### Load Data
```{r load_data, include = FALSE}
path_raw_F <- 'C:/Users/bydav/Desktop/AVS 554/Forward'
path_raw_R <- 'C:/Users/bydav/Desktop/AVS 554/Reverse'

# Specify file name format for forward and reverse data
fnsF <- list.files(path_raw_F)
fastqsF <- fnsF[grepl('.gz$', fnsF)]

fnsR <- list.files(path_raw_R)
fastqsR <- fnsR[grepl('.gz$', fnsR)]

# load metadata file
meta <- read.csv('C:/Users/bydav/Desktop/AVS 554/Provided/prov_Particle_Size_metadata.csv', header = TRUE, row.names = 1) 
```

### Load Outputs for Pick-Up Points
```{r reload, include = FALSE}
errF <- readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab3_errorF.RDS')
path_filt_F <- 'C:/Users/bydav/Desktop/AVS 554/Lab2_Filtered_Forward'
seqtab <- readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab3_seqtab.rds')
filtoutput <- readRDS("C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab2_FiltOutput.rds")
seqtab.nochim <- readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab3_seqtabnochimtable.rds')
all.taxa.sp <- readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab4_taxa_silva_rds.rds')
EX_ps_clean <-readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/EX_ps_clean_phyloseq_object.RDS')
EX_ps_clean.rar <-readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/EX_ps_clean_rarefied_phyloseq_object.RDS')

# May need to reload file path and names
# fnsF <- list.files(path_filt_F, full.names = TRUE)
# filtsF <- fnsF[grepl('.gz$', fnsF)] 
# sample.namesF <- sapply(strsplit(basename(filtsF), '_filt'), `[`, 1) 
# names(filtsF) <- sample.namesF 
```



# Beta Diversity

```{r beta_diversity_PCoA, echo = FALSE}
EX.ord.PCoA <- ordinate(EX_ps_clean.rar, #calculate similarities 
                   method ="PCoA", #ordination type 
                   "jaccard", binary = TRUE)

simplePCoA <- plot_ordination(EX_ps_clean.rar, EX.ord.PCoA, type="samples", color="Diet") #CHANGE ME but leave type as "samples"
simplePCoA

fancyPCoA <- plot_ordination(EX_ps_clean.rar, EX.ord.PCoA, type="samples") + geom_point(aes(size = as.factor(Week), color=as.factor(Diet), shape=as.factor(Week), alpha = as.numeric(Weight_kg))) + theme_minimal()+ scale_color_viridis(discrete = TRUE, option="A", begin = 0, end = 0.75) + stat_ellipse(aes(group=as.factor(Week), linetype=as.factor(Week))) + labs(color = "Diet", size = "Week", shape = "Week", alpha = "Weight (kg)", linetype="Week")
fancyPCoA
```

```{r beta_nmds_ordination, echo = FALSE}
EX.ordNMDSj <- ordinate(EX_ps_clean.rar, method ="NMDS", "jaccard", binary = TRUE)

EX.ordNMDSb <- ordinate(EX_ps_clean.rar, method ="NMDS", "bray", binary = TRUE)

Ord_NMDSj <- plot_ordination(EX_ps_clean.rar, EX.ordNMDSj, type="samples", shape="Diet") + geom_point(aes(size = as.numeric(Weight_kg), color=as.factor(Week), alpha = as.factor(Week))) + scale_color_viridis(discrete = TRUE, option="A", begin = 0, end = 0.75) + stat_ellipse(aes(group=Diet, linetype=Diet)) + labs(color = "Weeks 0 to 2", size = "Weight in kg", shape = "Diet", linetype="Diet", alpha= "Week") + ggtitle("Jaccard NMDS")

Ord_NMDSb <- plot_ordination(EX_ps_clean.rar, EX.ordNMDSb, type="samples", shape="Diet") + geom_point(aes(size = as.numeric(Weight_kg), color=as.factor(Week), alpha = as.factor(Week))) + scale_color_viridis(discrete = TRUE, option="A", begin = 0, end = 0.75) + stat_ellipse(aes(group=Diet, linetype=Diet)) + ggtitle("Bray NMDS")

ggarrange(Ord_NMDSj, Ord_NMDSb, ncol = 2, common.legend = TRUE, labels = NULL)
```
# RESULT:

Jaccard:
Run 20 stress 0.05081642 
*** Best solution repeated 1 times
    
Bray:
Run 20 stress 0.05085451 
*** Best solution was not repeated -- monoMDS stopping criteria:
    19: stress ratio > sratmax
     1: scale factor of the gradient < sfgrmin

```{r vegan, echo = FALSE}
veganotu = function(physeq) { 
  require("vegan") 
  OTU = otu_table(physeq) 
  if (taxa_are_rows(OTU)) { 
    OTU = t(OTU) 
    } 
return(as(OTU, "matrix")) 
  } # export data from phyloseq to vegan-compatible object 
EX_ps_clean_vegan <- veganotu(EX_ps_clean.rar)
```

```{r vegan_beta, echo = FALSE}
EXsampledf <- data.frame(sample_data(EX_ps_clean.rar)) 
EX_clean_BCj <- vegdist(wisconsin(sqrt(EX_ps_clean_vegan)), method = "jaccard", binary=TRUE) 

# Bray
EXsampledf <- data.frame(sample_data(EX_ps_clean.rar)) 
EX_clean_BCb <- vegdist(wisconsin(sqrt(EX_ps_clean_vegan)), method = "bray", binary=TRUE) 
```

```{r betadisper}
# Jaccard
betadisp_EX_cleanj <- betadisper(EX_clean_BCj, EXsampledf$Treatment)
betadisp_EX_cleanj

scores(betadisp_EX_cleanj, display = c("sites", "centroids"), choices = c(1,2))
eigenvals(betadisp_EX_cleanj)
betaj <- boxplot(betadisp_EX_cleanj, ylab = "Distance to Centroid", main = "Betadisper Jaccard")
anova(betadisp_EX_cleanj)
permutest(betadisp_EX_cleanj)
TukeyHSD(betadisp_EX_cleanj)

betadisp_EX_cleanb <- betadisper(EX_clean_BCb, EXsampledf$Treatment)
betadisp_EX_cleanb

scores(betadisp_EX_cleanb, display = c("sites", "centroids"), choices = c(1,2))
eigenvals(betadisp_EX_cleanb)
betab <- boxplot(betadisp_EX_cleanb, ylab = "Distance to Centroid", main = "Betadisper Bray's")
anova(betadisp_EX_cleanb)
permutest(betadisp_EX_cleanb)
TukeyHSD(betadisp_EX_cleanb)


```

```{r adonis2}
JaccardAdonis <- adonis2(EX_clean_BCj ~ Diet * Week, as(sample_data(EX_ps_clean.rar), "data.frame"), permutations=9999, method = "jaccard", na.action=na.omit)

BrayAdonis <- adonis2(EX_clean_BCb ~ Diet * Week, as(sample_data(EX_ps_clean.rar), "data.frame"), permutations=9999, method = "bray", na.action=na.omit)
```

###########################################################################



#  Beta diversity ordinations (Lab 11) ---------------------------------
# ordinations in phyloseq: https://joey711.github.io/phyloseq/plot_ordination-examples.html

  ## PCA  -----------------
# currently, phyloseq doesn't run a PCA, but you can manually perform one using this tutorial: https://www.datacamp.com/community/tutorials/pca-analysis-r

install.packages("devtools")
library(devtools)
install_github("vqv/ggbiplot") #takes a few minutes
library(ggbiplot)

# calculate the components
EX_clean.rar.pca <- prcomp(otu_table(EX_ps_clean.rar), center = TRUE, scale = TRUE) #CHANGE ME

# take a look
summary(EX_clean.rar.pca) #CHANGE ME

# graph it. add ggplot2 text to make it pretty
ggbiplot(EX_clean.rar.pca)




  ## PCoA in phyloseq -----------------

# use phyloseq to calculate ordination for use in the plot
EX.ord <- ordinate(EX_ps_clean.rar, #calculate similarities
                   method ="PCoA", #ordination type
                   "jaccard", binary = TRUE) #similarity type. Jaccard is binary, Bray can be binary (unweighted) or not (weighted) but is usually run as binary=FALSE.

# simple ordination
plot_ordination(EX_ps_clean.rar, EX.ord, type="samples", color="Diet") #CHANGE ME but leave type as "samples"

# fancy ordination
install.packages("viridis")
library(viridis) #add a cool color palette

plot_ordination(EX_ps_clean.rar, EX.ord, type="samples") + #CHANGE ME
  geom_point(aes(size = as.factor(Week), color=as.factor(Diet), shape=as.factor(Week), alpha = as.numeric(Weight_kg))) + # resize the points based on a numeric factor, and make light/dark based on another factor
  theme_minimal()+
  scale_color_viridis(discrete = TRUE, option="A", begin = 0, end = 0.75) + #begin/end indicates where on the color scale to use, A refers to 'magma' color palette
  stat_ellipse(aes(group=as.factor(Week), linetype=as.factor(Week))) + #add circles around a particular grouping, and make the circle lines different
  labs(color = "Diet", size = "Week", shape = "Week", alpha = "Weight (kg)", linetype="Week") #rename the headers in the legend



# BOOKMARK
# function and tutorial from https://rpubs.com/DKCH2020/587758
veganotu = function(physeq) {
  require("vegan")
  OTU = otu_table(physeq)
  if (taxa_are_rows(OTU)) {
    OTU = t(OTU)
  }
  return(as(OTU, "matrix"))
}
# export data from phyloseq to vegan-compatible object
Vibrio_vegan <- veganotu(Vibrio_clean.rar)

# make a data frame that can be used in vegan from the sample_data in the
# phyloseq object
sampledf <- data.frame(sample_data(Vibrio_clean.rar))
Vibrio_BC <- vegdist(wisconsin(sqrt(Vibrio_vegan)), method = "jaccard", binary=TRUE)
betadisp_Vibrio <- betadisper(Vibrio_BC, sampledf$Sample_type)

betadisp_Vibrio

anova(betadisp_Vibrio)

# Analysis of Variance Table
# 
# Response: Distances
# Df   Sum Sq   Mean Sq F value    Pr(>F)    
# Groups      2 0.043240 0.0216200  94.901 < 2.2e-16 ***
#   Residuals 109 0.024832 0.0002278                      
# ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

permutest(betadisp_Vibrio)

# Permutation test for homogeneity of multivariate dispersions
# Permutation: free
# Number of permutations: 999
# 
# Response: Distances
# Df   Sum Sq   Mean Sq      F N.Perm Pr(>F)    
# Groups      2 0.043240 0.0216200 94.901    999  0.001 ***
#   Residuals 109 0.024832 0.0002278                         
# ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

TukeyHSD(betadisp_Vibrio)
# Tukey multiple comparisons of means
# 95% family-wise confidence level
# 
# Fit: aov(formula = distances ~ group, data = df)
# 
# $group
# diff          lwr       upr     p adj
# wild veligers-veligers 0.099436111  0.079792204 0.1190800 0.0000000
# tank-veligers          0.106213720  0.087878344 0.1245491 0.0000000
# tank-wild veligers     0.006777609 -0.002106682 0.0156619 0.1702206














# use that  ordination calculation style in adonis, basic permanova test using Adonis in vegan
adonis(EX.ord_jac ~ Diet * Week, as(sample_data(EX_ps_clean.rar), "data.frame"), permutations=9999, na.action=na.omit)

# save the table it spits out at you, like this:
#             Df  SumsOfSqs MeanSqs F.Model R2      Pr(>F)   
#   Diet       1    0.6479 0.64792  1.7560 0.07303 0.0047 **
#   Week       1    0.7136 0.71361  1.9340 0.08044 0.0019 **
#   Diet:Week  1    0.4997 0.49967  1.3542 0.05632 0.0495 * 
#   Residuals 19    7.0106 0.36898         0.79021          
#   Total     22    8.8718                 1.00000          
# ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

# interpretation: The F-model/F statistic is a measure of importance of the 



# more complicated permanova test using Adonis in vegan

# if want to add repeated sampling use this line
ctrl <- with(as.data.frame(sample_data(EX_ps_clean.rar)), how(blocks = Plot, nperm = 999))

adonis(EX.ord ~ FactorA * as.numeric(FactorB) * as.numeric(FactorC) * FactorD, data = data.frame(sample_data(EX_ps_clean.rar)), 
       permutations = ctrl, #for repeated measures
       strata = as.data.frame(sample_data(EX_ps_clean.rar))$Blocking_factor) #for replicates/blocks

### NOTE: if adonis is not working and says your data is of the wrong type:
# 1) check to make sure you don't have NAs in what you are trying use
# 2) update the data.table package, then reload the vegan package
# 3) Repeat the ordination calculation by piping phyloseq directly to vegan, because they haven't been playing nicely since 2020 (who has, really?)
  #Ex.ord_jac = phyloseq::distance(EX_ps_clean.rar, type="samples", "jaccard", binary=T) 
# use that second ordination calculation style in adonis, basic permanova test using Adonis in vegan
  #adonis(EX.ord_jac ~ Diet * Week, as(sample_data(EX_ps_clean.rar), "data.frame"), permutations=9999, na.action=na.omit)

  ## NMDS in phyloseq -----------------

# use phyloseq to calculate ordination for use in the plot
EX.ord2 <- ordinate(EX_ps_clean.rar, #calculate similarities
                   method ="NMDS", #ordination type
                   "jaccard", binary = TRUE) #similarity type. Jaccard is binary, Bray can be binary (unweighted) or not (weighted) but is usually run as binary=FALSE.

# report the amount of stress at the end of the NMDS calculation, as it is typical to report that in your manuscript. >0.2 is not great, and >0.3 plot is meaningless
# Run 20 stress 9.04618e-05 #CHANGE ME

# simple ordination
plot_ordination(EX_ps_clean.rar, EX.ord2, type="samples", color="Diet") #CHANGE ME but leave type as "samples"


# fancy ordination
install.packages("viridis")
library(viridis) #add a cool color palette

plot_ordination(EX_ps_clean.rar, EX.ord2, type="samples", shape="Diet") + #CHANGE ME
  geom_point(aes(size = as.numeric(Weight_kg), color=as.factor(Week), alpha = as.factor(Week))) + # resize the points based on a numeric factor, and make light/dark based on another factor
  scale_color_viridis(discrete = TRUE, option="A", begin = 0, end = 0.75) + #begin/end indicates where on the color scale to use, A refers to 'magma' color palette
  stat_ellipse(aes(group=Diet, linetype=Diet)) + #add circles around a particular grouping, and make the circle lines different
  labs(color = "Weeks 0 to 2", size = "Weight in kg", shape = "Diet", linetype="Diet", alpha= "Week") #rename the headers in the legend


# use that  ordination calculation style in adonis, basic permanova test using Adonis in vegan
adonis(EX.ord2 ~ Diet * Week, as(sample_data(EX_ps_clean.rar), "data.frame"), permutations=9999, na.action=na.omit)



### NOTE: if adonis is not working and says your data is of the wrong type:
# 1) check to make sure you don't have NAs in what you are trying use
# 2) update the data.table package, then reload the vegan package
# 3) Repeat the ordination calculation by piping phyloseq directly to vegan, because they haven't been playing nicely since 2020 (who has, really?)
#Ex.ord_jac = phyloseq::distance(EX_ps_clean.rar, type="samples", "jaccard", binary=T) 
# use that second ordination calculation style in adonis, basic permanova test using Adonis in vegan
#adonis(EX.ord_jac ~ Diet * Week, as(sample_data(EX_ps_clean.rar), "data.frame"), permutations=9999, na.action=na.omit)


# save the table it spits out at you, like this:
#             Df  SumsOfSqs MeanSqs F.Model R2      Pr(>F)   
#   Diet       1    0.6479 0.64792  1.7560 0.07303 0.0047 **
#   Week       1    0.7136 0.71361  1.9340 0.08044 0.0019 **
#   Diet:Week  1    0.4997 0.49967  1.3542 0.05632 0.0495 * 
#   Residuals 19    7.0106 0.36898         0.79021          
#   Total     22    8.8718                 1.00000          
# ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

# interpretation: The F-model/F statistic is a measure of importance of the 



# more complicated permanova test using Adonis in vegan

# if want to add repeated sampling use this line
ctrl <- with(as.data.frame(sample_data(EX_ps_clean.rar)), how(blocks = Plot, nperm = 999))

adonis(EX.ord ~ FactorA * as.numeric(FactorB) * as.numeric(FactorC) * FactorD, data = as.data.frame(sample_data(EX_ps_clean.rar)), 
       permutations = ctrl, #for repeated measures
       strata = as.data.frame(sample_data(EX_ps_clean.rar))$Blocking_factor) #for replicates/blocks



  ## 3D ordination --------

# Use the ordiplot package and any ordination data to graph it in 3D
# https://rdrr.io/cran/vegan3d/man/ordiplot3d.html

# use phyloseq to calculate ordination for use in the plot
EX.ord <- ordinate(EX_ps_clean.rar, #calculate similarities
                   method ="PCoA", #ordination type
                   "jaccard", binary = TRUE) #similarity type. Jaccard is binary, Bray can be binary (unweighted) or not (weighted) but is usually run as binary=FALSE.

install.packages("vegan3d")
library(vegan3d)

# BOOKMARK, can't find scores, ordination might need to be created with vegan instead of phyloseq
ordiplot3d(EX.ord, display = "sites", choices = 1:3, col = "black",
           ax.col = "red", arr.len = 0.1, arr.col = "blue", envfit,
           )


# Beta diversity components (Lab 12) ---------------------------------
  ## CCA in phyloseq -------------------------------------------
# correspondence analysis, or optionally, constrained correspondence analysis (a.k.a. canonical correspondence analysis). hypothesis-based testing does not need normally distributed data, but if you have a lot of outliers you may want to consider adding a data transformation.

# first create a distance ordination. BE SURE THERE ARE NO NAs in your factorial data!!
bray_not_na <- phyloseq::distance(physeq = EX_ps_clean.rar, method = "bray") #CHANGE ME

cca_ord <- ordinate(
  physeq = EX_ps_clean.rar, #CHANGE ME
  method = "CCA",
  distance = bray_not_na,
  formula = ~ Diet + Week  # CHANGE ME add factors
    #Condition(SAMPLE_ID) #CHANGE ME use for repeated measures
)

cca_ord
# paste the output here

# Call: cca(formula = OTU ~ Diet + Week, data = data)
# 
# Inertia Proportion Rank
# Total          7.2413     1.0000     
# Constrained    0.9964     0.1376    2
# Unconstrained  6.2450     0.8624   20
# Inertia is scaled Chi-square 
# 
# Eigenvalues for constrained axes:
#   CCA1   CCA2 
# 0.6864 0.3100 
# 
# Eigenvalues for unconstrained axes:
#   CA1    CA2    CA3    CA4    CA5    CA6    CA7    CA8 
# 0.9639 0.6452 0.5080 0.4325 0.4097 0.3276 0.3207 0.3114 
# (Showing 8 of 20 unconstrained eigenvalues)

# anova of whole model
anova(cca_ord, permu=1000)
# paste the output here    
# Permutation test for cca under reduced model
# Permutation: free
# Number of permutations: 999
# 
# Model: cca(formula = OTU ~ Diet + Week, data = data)
# Df ChiSquare      F Pr(>F)    
# Model     2    0.9964 1.5955  0.001 ***
#   Residual 20    6.2450                  
# ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


# anova of the factors (terms) you specified
anova(cca_ord, by="terms", permu=1000)
# Permutation test for cca under reduced model
# Terms added sequentially (first to last)
# Permutation: free
# Number of permutations: 999
# 
# Model: cca(formula = OTU ~ Diet + Week, data = data)
# Df ChiSquare      F Pr(>F)    
# Diet      1    0.4152 1.3297  0.058 .  
# Week      1    0.5812 1.8613  0.001 ***
#   Residual 20    6.2450                  
# ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1             


# cca plot with FACTORA in the model
cca_plot <- plot_ordination(
  physeq = EX_ps_clean.rar, #CHANGE ME
  ordination = cca_ord, 
  color = "Diet", #CHANGE ME
  axes = c(1,2)) + 
  theme_minimal() +
  aes(shape = as.factor(Week)) + #CHANGE ME
  geom_point(aes(colour = Diet), size = 4) + #CHANGE ME
  geom_point(colour = "grey90", size = 1.5) 



# Now add the environmental variables as arrows
arrowmat <- vegan::scores(cca_ord, display = "bp")

# Add labels, make a data.frame
arrowdf <- data.frame(labels = rownames(arrowmat), arrowmat)

# Define the arrow aesthetic mapping
arrow_map <- aes(xend = CCA1, 
                 yend = CCA2, 
                 x = 0, 
                 y = 0, 
                 shape = NULL, 
                 color = NULL, 
                 label = labels)

label_map <- aes(x = 1.3 * CCA1, 
                 y = 1.3 * CCA2, 
                 shape = NULL, 
                 color = NULL, 
                 label = labels)

arrowhead = arrow(length = unit(0.02, "npc"))

# Make a new graphic
cca_plot + 
  geom_segment(
    mapping = arrow_map, 
    size = .5, 
    data = arrowdf, 
    color = "gray", 
    arrow = arrowhead
  ) + 
  geom_text(
    mapping = label_map, 
    size = 4,  
    data = arrowdf, 
    show.legend = FALSE
  ) 





  ## RDA in phyloseq -------------------------------------------
# Redundancy analysis

# first create a distance ordination. BE SURE THERE ARE NO NAs in your factorial data!!
bray_not_na <- phyloseq::distance(physeq = EX_ps_clean.rar, method = "bray")

rda_ord <- ordinate(
  physeq = EX_ps_clean.rar, #CHANGEME
  method = "RDA",
  distance = bray_not_na,
  formula = ~ Diet * Week + Condition(SAMPLE_ID) # CHANGE ME add factors
   #  #CHANGE ME use for repeated measures
)

rda_ord
# paste the output here

# anova of whole model
anova(rda_ord, permu=1000)
# paste the output here    

# anova of the factors (terms) you specified
anova(rda_ord, by="terms", permu=1000)
# paste the output here              


# rda plot with FACTORA in the model
rda_plot <- plot_ordination(
  physeq = EX_ps_clean.rar, #CHANGE ME
  ordination = rda_ord, 
  color = "Diet", #CHANGE ME
  axes = c(1,2)) + 
  theme_minimal() +
  aes(shape = as.factor(Week)) + #CHANGE ME
  geom_point(aes(colour = Diet), size = 4) + #CHANGE ME
  geom_point(colour = "grey90", size = 1.5) 



# Now add the environmental variables as arrows
arrowmat <- vegan::scores(rda_ord, display = "bp")

# Add labels, make a data.frame
arrowdf <- data.frame(labels = rownames(arrowmat), arrowmat)

# Define the arrow aesthetic mapping
arrow_map <- aes(xend = RDA1, 
                 yend = RDA2, 
                 x = 0, 
                 y = 0, 
                 shape = NULL, 
                 color = NULL, 
                 label = labels)

label_map <- aes(x = 1.3 * RDA1, 
                 #y = 1.3 * RDA2, 
                 shape = NULL, 
                 color = NULL, 
                 label = labels)

arrowhead = arrow(length = unit(0.02, "npc"))

# Make a new graphic
rda_plot + 
  geom_segment(
    mapping = arrow_map, 
    size = .5, 
    data = arrowdf, 
    color = "gray", 
    arrow = arrowhead
  ) + 
  geom_text(
    mapping = label_map, 
    size = 4,  
    data = arrowdf, 
    show.legend = FALSE
  ) 



  ## dbRDA in phyloseq -------------------------------------------
# Constrained Analysis of Principal Coordinates or distance-based RDA,

# first create a distance ordination. BE SURE THERE ARE NO NAs in your factorial data!!
bray_not_na <- phyloseq::distance(physeq = EX_ps_clean.rar, method = "bray")

cap_ord <- ordinate(
  physeq = EX_ps_clean.rar, #CHANGEME
  method = "CAP",
  distance = bray_not_na,
  formula = ~ FACTORA + # CHANGE ME add factors
    Condition(SAMPLE_ID) #CHANGE ME use for repeated measures
)

cap_ord
# paste the output here

# anova of whole model
anova(cap_ord, permu=1000)
# paste the output here    

# anova of the factors (terms) you specified
anova(cap_ord, by="terms", permu=1000)
# paste the output here              


# CAP plot with FACTORA in the model
cap_plot <- plot_ordination(
  physeq = EX_ps_clean.rar, #CHANGE ME
  ordination = cap_ord, 
  color = "FACTORA", #CHANGE ME
  axes = c(1,2)) + 
  theme_minimal() +
  aes(shape = as.factor(FactorA)) + #CHANGE ME
  geom_point(aes(colour = FACTORB), size = 4) + #CHANGE ME
  geom_point(colour = "grey90", size = 1.5) 



# Now add the environmental variables as arrows
arrowmat <- vegan::scores(cap_ord, display = "bp")

# Add labels, make a data.frame
arrowdf <- data.frame(labels = rownames(arrowmat), arrowmat)

# Define the arrow aesthetic mapping
arrow_map <- aes(xend = CAP1, 
                 yend = CAP2, 
                 x = 0, 
                 y = 0, 
                 shape = NULL, 
                 color = NULL, 
                 label = labels)

label_map <- aes(x = 1.3 * CAP1, 
                 y = 1.3 * CAP2, 
                 shape = NULL, 
                 color = NULL, 
                 label = labels)

arrowhead = arrow(length = unit(0.02, "npc"))

# Make a new graphic
cap_plot + 
  geom_segment(
    mapping = arrow_map, 
    size = .5, 
    data = arrowdf, 
    color = "gray", 
    arrow = arrowhead
  ) + 
  geom_text(
    mapping = label_map, 
    size = 4,  
    data = arrowdf, 
    show.legend = FALSE
  ) 



  ## variance partitioning on CCA--------------
# How much did any one factor contribute to the sample clustering?
# http://www.hiercourse.com/docs/variationPartitioning.html 

library(vegan)

# create a dataframe from your SV table
EX.df <- as.data.frame(otu_table(EX_ps_clean.rar))

# extract your sample data from the phyloseq object
env.df <- as.data.frame(sample_data(EX_ps_clean.rar))


# calculate Principal Coordinates Of Neighborhood Matrix, diversity distance data transformed into rectangular format
EX.pcnm <- pcnm(dist(bray_not_na))

# environmental variables as predictors of community similarity
cap.env <- capscale(EX.df ~ ., data=env.df, distance='bray')

# calculate CCA 
cap.pcnm <- capscale(EX.df ~ ., data=as.data.frame(scores(EX.pcnm)), distance='bray')



# make a model using SV ordination and sample data
mod0.env <- capscale(EX.df ~ 1, data=env.df, distance='bray') # add + Condition(SAMPLE_ID) for repeated measures

# make a model using SV ordination and the component scores
mod0.pcnm <- capscale(EX.df ~ 1, data=as.data.frame(scores(EX.pcnm)), distance='bray') # add + Condition(SAMPLE_ID) for repeated measures

# select variables in each predictor table
step.env <- ordistep(mod0.env, scope=formula(cap.env))

# check variance inflation factors, higher number = data are redundant to another factor, 1= data are unique.  If factors are redundant (conflated) to each other (for example they basically report the same thing, go back and remove one and mention they were redundant/conflated in your manuscript Methods)
vif.cca(step.env)


step.pcnm <- ordistep(mod0.pcnm, scope=formula(cap.pcnm))

step.env$anova
# paste output here

step.pcnm$anova
# paste output here

EX.pcnm.sub <- scores(EX.pcnm, 
                            choices=c(1,3:16)) #CHANGE ME to include the significant pcnm from previous command

# partition variation among four predictor tables:
EX.var <- varpart(EX.df, 
                        ~ FACTORA, #CHANGE ME
                        ~ FactorB, #CHANGE ME
                        EX.pcnm.sub, data=env.df)

#plot 
par(mfrow=c(1, 2))
showvarparts(4)
plot(EX.var)



EX.var
# paste output here

anova(rda(EX.df  ~ FACTORA + Condition(EX.pcnm.sub), data=env.df)) # add + Condition(env.df$SAMPLE_ID) for repeated measures
# paste output here


# ####################################
# Recap

## Set-up: Loading Packages and Data

### Packages
```{r load_packages, include = FALSE}
library(tinytex)
library(dada2); packageVersion('dada2')
library(GenomeInfoDbData)
library(ShortRead)
library(phyloseq); packageVersion("phyloseq")
require(tidyr) 
library(ggplot2)
require(dplyr)
library(vegan)
library(RColorBrewer)
library(ggpubr)
library(ggsignif)
require(grid)
library(PerformanceAnalytics) 
library(vegan)
library(lme4) 
library(nlme)
library(lmerTest) 
library(emmeans)
library(viridis)
library(SiMRiv)
library(microbiome)
library("DESeq2")
packageVersion("DESeq2")
library(DescTools)
library(conover.test)
install.packages("devtools")
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
library(DEGreport)
library(ggfortify)
install.packages("remotes")
remotes::install_github("schuyler-smith/phyloschuyler")
library(phylosmith)
library(ggdendro)

```

### Load Data
```{r load_data, include = FALSE}
path_raw_F <- 'C:/Users/bydav/Desktop/AVS 554/Forward'
path_raw_R <- 'C:/Users/bydav/Desktop/AVS 554/Reverse'

# Specify file name format for forward and reverse data
fnsF <- list.files(path_raw_F)
fastqsF <- fnsF[grepl('.gz$', fnsF)]

fnsR <- list.files(path_raw_R)
fastqsR <- fnsR[grepl('.gz$', fnsR)]

# load metadata file
meta <- read.csv('C:/Users/bydav/Desktop/AVS 554/Provided/prov_Particle_Size_metadata.csv', header = TRUE, row.names = 1) 
```

### Load Outputs for Pick-Up Points
```{r reload, include = FALSE}
errF <- readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab3_errorF.RDS')
path_filt_F <- 'C:/Users/bydav/Desktop/AVS 554/Lab2_Filtered_Forward'
seqtab <- readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab3_seqtab.rds')
filtoutput <- readRDS("C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab2_FiltOutput.rds")
seqtab.nochim <- readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab3_seqtabnochimtable.rds')
all.taxa.sp <- readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/Lab4_taxa_silva_rds.rds')
EX_ps_clean <-readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/EX_ps_clean_phyloseq_object.RDS')
EX_ps_clean.rar <-readRDS('C:/Users/bydav/Desktop/AVS 554/Davis_Supplemental/Output_and_Images/Done/EX_ps_clean_rarefied_phyloseq_object.RDS')

# May need to reload file path and names
# fnsF <- list.files(path_filt_F, full.names = TRUE)
# filtsF <- fnsF[grepl('.gz$', fnsF)] 
# sample.namesF <- sapply(strsplit(basename(filtsF), '_filt'), `[`, 1) 
# names(filtsF) <- sample.namesF 
```



# Niche/neutral models (Lab 14)-----
# can create functions around the L-K math, or try this package: https://github.com/AlmaasLab/micInt

require('infotheo')
require('matrixStats')
require('magrittr')
require('deSolve')
require('igraph')
library(dplyr)
require('rlang')
require('glue')
require('viridis') # is a color palette so not critical to have
require('RhpcBLASctl')
library(ggplot2)
require('ggfortify')

install.packages("devtools")
library(devtools)
install_github("AlmaasLab/micInt")


# Source tracking (Lab 14) -----
# Source Tracker was developed for QIIME, and can identify samples which could be the source of the microbes in another community, called the sink

# Script and other files must be downloaded first:https://github.com/danknights/sourcetracker 

# Then follow this walkthrough to go from phyloseq to sourcetracker in R: https://mgaley-004.github.io/MiSeq-Analysis/Tutorials/SourceSink.html 


# PiCRUST (Lab 15) -------

# Tutorial and information here: https://github.com/picrust/picrust2/wiki
# Can be run online or command line using python
# working on an R workflow



# Alternatively, BugBase can do broad phenotype predictors but must be run online: https://bugbase.cs.umn.edu/


# Dendogram (Lab 16) ---------------------------------------
## Note: I typically use the UPGMA algorothm for Dendograms when I make them as stand-alone.  Some other graph functions will intergrate dendograms themselves.

#calculate dissimilarity based on your sequence data in base R
dist.object <- dist(as.data.frame(otu_table(EX_ps_clean.rar)), method = "euclidean", diag = FALSE, upper = FALSE, p = 2) # method = "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski"

# cluster the dissimiliary info with base R: https://www.rdocumentation.org/packages/stats/versions/3.2.1/topics/hclust
EX_ps.clust <- hclust(dist.object, method = "average", members = NULL) #method = average for UPGMA

# visualization in phyloseq: https://bioconductor.org/packages/devel/bioc/vignettes/phyloseq/inst/doc/phyloseq-analysis.html#hierarchical-clustering
plot(EX_ps.clust)


# Phylogenetic Trees (Lab 16) ---------------------------------------
# Tutorial to generate tree: https://f1000research.com/articles/5-1492/v2
# Tutorial to view tree: https://web.stanford.edu/class/bios221/labs/phyloseq/lab_phyloseq.html

library(dada2); packageVersion("dada2")
library(phyloseq)
library(DECIPHER)
library(phangorn) #to build a tree
library(ggplot2)

# pull apart your phyloseq object. Note 03/2020: there can be formatting issues associated with this step, so online tutorial information may be out of date.  Adjust code as needed.

seqtab <- as.data.frame(otu_table(EX_ps_clean.rar))
taxtab <- as.data.frame(tax_table(EX_ps_clean.rar))
meta <- as.data.frame(sample_data(EX_ps_clean.rar))

# grab the SV data and name it
# seqs <- getSequences(seqtab, collapse = FALSE, silence = TRUE) # NOTE: this code is from the tutorial and should pull the sequence data, but is currently broken for formatting issues. Instead use:
seqs <- names(seqtab)
names(seqs) <- seqs # This propagates to the tip labels of the tree

# align the SVs
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)

# construct a neighbor-joing tree as the basis
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phang.align)

## negative edges length changed to 0!

fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)

# combine your tree with your current phyloseq object
EX_ps_clean.rar <- merge_phyloseq(EX_ps_clean.rar, phy_tree(fitGTR$tree))

# OPTIONAL: recombine and load your phyloseq object
# EX_ps_clean.rar <- phyloseq(otu_table(seqtab, taxa_are_rows=FALSE), sample_data(meta), tax_table(taxtable), phy_tree(fitGTR$tree))

# view your tree with phyloseq. add other ggplot2 components to make it pretty
plot_tree(EX_ps_clean.rar, color="Genus", shape="FACTORA", size="FACTORB")

plot_tree(EX_ps_clean.rar, color="SampleType", shape="Phylum", size="abundance")


